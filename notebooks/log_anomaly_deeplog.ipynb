{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzangdol1029/RADAR/blob/main/notebooks/log_anomaly_deeplog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ë¡œê·¸íƒì§€ëª¨ë¸(DeepLog)"
      ],
      "metadata": {
        "id": "LCscjHjcSXMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==================== Google Drive ë§ˆìš´íŠ¸ ====================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# ==================== ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ìˆ˜ì •ë¨) ====================\n",
        "def extract_and_load_log_data(dataset_path, zip_file_name='logFile.zip', extract_path='./extracted_logs'):\n",
        "    \"\"\"\n",
        "    ì••ì¶• íŒŒì¼ í•´ì œ ë° ë¡œê·¸ ë°ì´í„° ë¡œë“œ\n",
        "\n",
        "    Args:\n",
        "        dataset_path: Google Driveì˜ ë°ì´í„°ì…‹ ê²½ë¡œ\n",
        "        zip_file_name: ì••ì¶• íŒŒì¼ ì´ë¦„\n",
        "        extract_path: ì••ì¶• í•´ì œ ìœ„ì¹˜\n",
        "\n",
        "    Returns:\n",
        "        sessions: ë¡œê·¸ ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ì••ì¶• íŒŒì¼ í•´ì œ ë° ë°ì´í„° ë¡œë“œ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. ì••ì¶• íŒŒì¼ ê²½ë¡œ\n",
        "    zip_file_path = os.path.join(dataset_path, zip_file_name)\n",
        "    print(f\"ì••ì¶• íŒŒì¼: {zip_file_path}\")\n",
        "\n",
        "    # 2. ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # 3. ì••ì¶• í•´ì œ\n",
        "    print(f\"ì••ì¶• í•´ì œ ì¤‘... â†’ {extract_path}\")\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # 4. í•´ì œëœ íŒŒì¼ ëª©ë¡ í™•ì¸\n",
        "    all_files = glob.glob(os.path.join(extract_path, '**/*'), recursive=True)\n",
        "    json_files = [f for f in all_files if f.endswith('.json')]\n",
        "\n",
        "\n",
        "    if json_files:\n",
        "        print(\"\\nJSON íŒŒì¼ ëª©ë¡:\")\n",
        "        for f in json_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "\n",
        "    # 5. ë°ì´í„° ë¡œë“œ\n",
        "    sessions = []\n",
        "\n",
        "    # JSON íŒŒì¼ ë¡œë“œ (ë°°ì—´ í˜•íƒœ)\n",
        "    for json_file in json_files:\n",
        "        print(f\"\\në¡œë“œ ì¤‘: {json_file}\")\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
        "                if isinstance(data, list):\n",
        "                    sessions.extend(data)\n",
        "                    print(f\"  âœ… {len(data)}ê°œ ì„¸ì…˜ ë¡œë“œ\")\n",
        "                # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš°\n",
        "                else:\n",
        "                    sessions.append(data)\n",
        "                    print(f\"  âœ… 1ê°œ ì„¸ì…˜ ë¡œë“œ\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ ì—ëŸ¬: {e}\")\n",
        "\n",
        "    print(f\"\\nì´ ë¡œë“œëœ ì„¸ì…˜ ìˆ˜: {len(sessions)}\")\n",
        "\n",
        "    # 6. ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
        "    if sessions:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸ ì„¸ì…˜)\")\n",
        "        print(\"=\" * 50)\n",
        "        sample = sessions[0]\n",
        "        for key, value in sample.items():\n",
        "            if isinstance(value, list) and len(value) > 10:\n",
        "                print(f\"{key}: [{value[:5]}... ] (ê¸¸ì´: {len(value)})\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "    return sessions\n",
        "\n",
        "# ==================== DeepLog ëª¨ë¸ ====================\n",
        "class DeepLog(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=32, hidden_size=64, num_layers=2):\n",
        "        super(DeepLog, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=0.3 if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(embedded, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ==================== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ====================\n",
        "class LogDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.LongTensor(sequences)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# ==================== ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ====================\n",
        "def prepare_deeplog_data(sessions, window_size=10, stride=1, min_seq_length=5):\n",
        "    \"\"\"\n",
        "    DeepLog í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„\n",
        "\n",
        "    Args:\n",
        "        sessions: ë¡œê·¸ ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸\n",
        "        window_size: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸°\n",
        "        stride: ìœˆë„ìš° ì´ë™ í¬ê¸°\n",
        "        min_seq_length: ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ìœ¼ë©´ ì œì™¸)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    all_event_ids = set()\n",
        "\n",
        "    # ì„¸ì…˜ë³„ í†µê³„\n",
        "    total_sessions = len(sessions)\n",
        "    valid_sessions = 0\n",
        "    skipped_sessions = 0\n",
        "\n",
        "    for session in sessions:\n",
        "        seq = session.get('event_sequence', [])\n",
        "\n",
        "        # ë¹ˆ ì‹œí€€ìŠ¤ ë˜ëŠ” ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ ìŠ¤í‚µ\n",
        "        if len(seq) < min_seq_length:\n",
        "            skipped_sessions += 1\n",
        "            continue\n",
        "\n",
        "        valid_sessions += 1\n",
        "        all_event_ids.update(seq)\n",
        "\n",
        "        # ìœˆë„ìš°ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìœ¼ë©´ ìŠ¤í‚µ\n",
        "        if len(seq) <= window_size:\n",
        "            continue\n",
        "\n",
        "        # Sliding Window ìƒì„±\n",
        "        for i in range(0, len(seq) - window_size, stride):\n",
        "            window = seq[i : i + window_size]\n",
        "            target = seq[i + window_size]\n",
        "\n",
        "            sequences.append(window)\n",
        "            labels.append(target)\n",
        "\n",
        "    print(f\"\\nì„¸ì…˜ í†µê³„:\")\n",
        "    print(f\"  - ì „ì²´ ì„¸ì…˜: {total_sessions}ê°œ\")\n",
        "    print(f\"  - ìœ íš¨ ì„¸ì…˜: {valid_sessions}ê°œ\")\n",
        "    print(f\"  - ìŠ¤í‚µëœ ì„¸ì…˜: {skipped_sessions}ê°œ\")\n",
        "\n",
        "    # ì´ë²¤íŠ¸ ID ì¬ë§¤í•‘\n",
        "    event_id_map = {eid: idx+1 for idx, eid in enumerate(sorted(all_event_ids))}\n",
        "    event_id_map[0] = 0  # íŒ¨ë”©ìš©\n",
        "    num_classes = len(event_id_map)\n",
        "\n",
        "    # ì¬ë§¤í•‘ ì ìš©\n",
        "    sequences_remapped = [[event_id_map.get(eid, 0) for eid in seq] for seq in sequences]\n",
        "    labels_remapped = [event_id_map.get(label, 0) for label in labels]\n",
        "\n",
        "    print(f\"\\në°ì´í„° í†µê³„:\")\n",
        "    print(f\"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(sequences):,}ê°œ\")\n",
        "    print(f\"  - ìœˆë„ìš° í¬ê¸°: {window_size}\")\n",
        "    print(f\"  - ì´ë²¤íŠ¸ í´ë˜ìŠ¤ ìˆ˜: {num_classes}\")\n",
        "    print(f\"  - ì´ë²¤íŠ¸ ID ë²”ìœ„: {min(all_event_ids)} ~ {max(all_event_ids)}\")\n",
        "\n",
        "    return np.array(sequences_remapped), np.array(labels_remapped), num_classes, event_id_map\n",
        "\n",
        "# ==================== í•™ìŠµ í•¨ìˆ˜ ====================\n",
        "def train_deeplog(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sequences, labels in train_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_deeplog(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in test_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ==================== ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ====================\n",
        "if __name__ == \"__main__\":\n",
        "    # ========== 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ==========\n",
        "    dataset_path = '/content/drive/MyDrive/ë¡œê·¸ì´ìƒíƒì§€/dataset/'\n",
        "    sessions = extract_and_load_log_data(\n",
        "        dataset_path=dataset_path,\n",
        "        zip_file_name='logFile.zip',\n",
        "        extract_path='./extracted_logs'\n",
        "    )\n",
        "\n",
        "    if not sessions:\n",
        "        print(\"âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "        exit(1)\n",
        "\n",
        "    # ========== 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ==========\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    window_size = 2\n",
        "    sequences, labels, num_classes, event_id_map = prepare_deeplog_data(\n",
        "        sessions,\n",
        "        window_size=window_size,\n",
        "        stride=1,\n",
        "        min_seq_length=3\n",
        "    )\n",
        "\n",
        "    if len(sequences) == 0:\n",
        "        print(\"âŒ ì „ì²˜ë¦¬ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤! window_sizeë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.\")\n",
        "        exit(1)\n",
        "\n",
        "    # ========== 3ë‹¨ê³„: Train/Test ë¶„í•  ==========\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        sequences, labels, test_size=0.2, random_state=42, shuffle=True\n",
        "    )\n",
        "\n",
        "    train_dataset = LogDataset(X_train, y_train)\n",
        "    test_dataset = LogDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    print(f\"\\ní•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ\")\n",
        "    print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset):,}ê°œ\")\n",
        "\n",
        "    # ========== 4ë‹¨ê³„: ëª¨ë¸ ì´ˆê¸°í™” ==========\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"3ë‹¨ê³„: ëª¨ë¸ ì´ˆê¸°í™”\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    model = DeepLog(\n",
        "        num_classes=num_classes,\n",
        "        embedding_dim=32,\n",
        "        hidden_size=64,\n",
        "        num_layers=2\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(model)\n",
        "    print(f\"\\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # ========== 5ë‹¨ê³„: í•™ìŠµ ==========\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    num_epochs = 50\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_deeplog(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = evaluate_deeplog(model, test_loader, criterion, device)\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'num_classes': num_classes,\n",
        "                'event_id_map': event_id_map,\n",
        "                'window_size': window_size,\n",
        "                'best_acc': best_acc\n",
        "            }, 'best_deeplog_model.pth')\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "            print(f\"  Best Test Acc: {best_acc:.2f}%\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_acc:.2f}%\")\n",
        "    print(f\"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: best_deeplog_model.pth\")\n",
        "\n",
        "    # ========== 6ë‹¨ê³„: ì´ë²¤íŠ¸ ë§¤í•‘ ì €ì¥ ==========\n",
        "    with open('event_id_mapping.json', 'w') as f:\n",
        "        json.dump(event_id_map, f, indent=2)\n",
        "    print(f\"ğŸ“ ì´ë²¤íŠ¸ ë§¤í•‘ ì €ì¥: event_id_mapping.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSS0Rubi8hxn",
        "outputId": "9855f2bd-5c8e-4e53-dc3e-2f1578181f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
            "==================================================\n",
            "ì••ì¶• íŒŒì¼ í•´ì œ ë° ë°ì´í„° ë¡œë“œ\n",
            "==================================================\n",
            "ì••ì¶• íŒŒì¼: /content/drive/MyDrive/ë¡œê·¸ì´ìƒíƒì§€/dataset/logFile.zip\n",
            "ì••ì¶• í•´ì œ ì¤‘... â†’ ./extracted_logs\n",
            "\n",
            "JSON íŒŒì¼ ëª©ë¡:\n",
            "  - ./extracted_logs/logFile/preprocessed_logs_2025-05-04.json\n",
            "  - ./extracted_logs/logFile/preprocessed_logs_2025-05-03.json\n",
            "  - ./extracted_logs/logFile/preprocessed_logs_2025-05-01.json\n",
            "  - ./extracted_logs/logFile/preprocessed_logs_2025-05-02.json\n",
            "\n",
            "ë¡œë“œ ì¤‘: ./extracted_logs/logFile/preprocessed_logs_2025-05-04.json\n",
            "  âœ… 115391ê°œ ì„¸ì…˜ ë¡œë“œ\n",
            "\n",
            "ë¡œë“œ ì¤‘: ./extracted_logs/logFile/preprocessed_logs_2025-05-03.json\n",
            "  âœ… 115332ê°œ ì„¸ì…˜ ë¡œë“œ\n",
            "\n",
            "ë¡œë“œ ì¤‘: ./extracted_logs/logFile/preprocessed_logs_2025-05-01.json\n",
            "  âœ… 115299ê°œ ì„¸ì…˜ ë¡œë“œ\n",
            "\n",
            "ë¡œë“œ ì¤‘: ./extracted_logs/logFile/preprocessed_logs_2025-05-02.json\n",
            "  âœ… 116306ê°œ ì„¸ì…˜ ë¡œë“œ\n",
            "\n",
            "ì´ ë¡œë“œëœ ì„¸ì…˜ ìˆ˜: 462328\n",
            "\n",
            "==================================================\n",
            "ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸ ì„¸ì…˜)\n",
            "==================================================\n",
            "event_sequence: [1]\n",
            "has_error: False\n",
            "has_warn: False\n",
            "service_name: user\n",
            "original_logs: ['2025-05-04 00:00:00.876 DEBUG 2044940 --- [   scheduling-1] o.h.SQL                                  :']\n",
            "simplified_text: [user] 2025-05-04 00:00:00.876 DEBUG 2044940 --- [ scheduling-1] o.h.SQL :\n",
            "time_window: 2025-05-04_00_00\n",
            "related_services: ['user', 'eureka', 'code', 'research', 'fs', 'smeta']\n",
            "correlation_id: 2025-05-04_00_00_user\n",
            "token_ids: [[101, 1, 102, 0, 0]... ] (ê¸¸ì´: 256)\n",
            "event_ids: [1]\n",
            "attention_mask: [[1, 1, 1, 0, 0]... ] (ê¸¸ì´: 256)\n",
            "session_id: 0\n",
            "\n",
            "==================================================\n",
            "2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬\n",
            "==================================================\n",
            "\n",
            "ì„¸ì…˜ í†µê³„:\n",
            "  - ì „ì²´ ì„¸ì…˜: 462328ê°œ\n",
            "  - ìœ íš¨ ì„¸ì…˜: 38148ê°œ\n",
            "  - ìŠ¤í‚µëœ ì„¸ì…˜: 424180ê°œ\n",
            "\n",
            "ë°ì´í„° í†µê³„:\n",
            "  - ì´ ìƒ˜í”Œ ìˆ˜: 119,747ê°œ\n",
            "  - ìœˆë„ìš° í¬ê¸°: 2\n",
            "  - ì´ë²¤íŠ¸ í´ë˜ìŠ¤ ìˆ˜: 113\n",
            "  - ì´ë²¤íŠ¸ ID ë²”ìœ„: 1 ~ 132\n",
            "\n",
            "í•™ìŠµ ë°ì´í„°: 95,797ê°œ\n",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 23,950ê°œ\n",
            "\n",
            "==================================================\n",
            "3ë‹¨ê³„: ëª¨ë¸ ì´ˆê¸°í™”\n",
            "==================================================\n",
            "DeepLog(\n",
            "  (embedding): Embedding(113, 32, padding_idx=0)\n",
            "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (fc): Linear(in_features=64, out_features=113, bias=True)\n",
            ")\n",
            "\n",
            "ì´ íŒŒë¼ë¯¸í„° ìˆ˜: 69,329\n",
            "\n",
            "==================================================\n",
            "4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ\n",
            "==================================================\n",
            "Epoch [5/50]\n",
            "  Train Loss: 0.3628, Train Acc: 87.22%\n",
            "  Test Loss: 0.3580, Test Acc: 87.52%\n",
            "  Best Test Acc: 87.52%\n",
            "--------------------------------------------------\n",
            "Epoch [10/50]\n",
            "  Train Loss: 0.3548, Train Acc: 87.25%\n",
            "  Test Loss: 0.3531, Test Acc: 87.49%\n",
            "  Best Test Acc: 87.52%\n",
            "--------------------------------------------------\n",
            "Epoch [15/50]\n",
            "  Train Loss: 0.3521, Train Acc: 87.37%\n",
            "  Test Loss: 0.3547, Test Acc: 87.22%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [20/50]\n",
            "  Train Loss: 0.3509, Train Acc: 87.40%\n",
            "  Test Loss: 0.3538, Test Acc: 87.23%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [25/50]\n",
            "  Train Loss: 0.3504, Train Acc: 87.35%\n",
            "  Test Loss: 0.3539, Test Acc: 87.48%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [30/50]\n",
            "  Train Loss: 0.3496, Train Acc: 87.38%\n",
            "  Test Loss: 0.3531, Test Acc: 87.55%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [35/50]\n",
            "  Train Loss: 0.3499, Train Acc: 87.42%\n",
            "  Test Loss: 0.3547, Test Acc: 86.99%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [40/50]\n",
            "  Train Loss: 0.3493, Train Acc: 87.33%\n",
            "  Test Loss: 0.3533, Test Acc: 87.52%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [45/50]\n",
            "  Train Loss: 0.3492, Train Acc: 87.39%\n",
            "  Test Loss: 0.3590, Test Acc: 87.46%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "Epoch [50/50]\n",
            "  Train Loss: 0.3493, Train Acc: 87.40%\n",
            "  Test Loss: 0.3540, Test Acc: 87.49%\n",
            "  Best Test Acc: 87.55%\n",
            "--------------------------------------------------\n",
            "\n",
            "âœ… í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: 87.55%\n",
            "ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: best_deeplog_model.pth\n",
            "ğŸ“ ì´ë²¤íŠ¸ ë§¤í•‘ ì €ì¥: event_id_mapping.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë° íë¦„ ë¶„ì„\n",
        "ì´ ì½”ë“œëŠ” DeepLog ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ì—¬ ë¡œê·¸ì˜ ì´ë²¤íŠ¸ ë°œìƒ ìˆœì„œë¥¼ í•™ìŠµí•˜ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "* ë°ì´í„° ë¡œë“œ (extract_and_load_log_data)\n",
        "\n",
        "    êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ logFile.zip ì••ì¶•ì„ í’‰ë‹ˆë‹¤.\n",
        "\n",
        "    í´ë” ë‚´ì˜ ëª¨ë“  .json íŒŒì¼ì„ ì°¾ì•„ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì…ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "* ë°ì´í„° ì „ì²˜ë¦¬ (prepare_deeplog_data) - ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„\n",
        "\n",
        "    Windowing: DeepLogëŠ” \"ì•ì„  Nê°œì˜ ë¡œê·¸ë¥¼ ë³´ê³  ë‹¤ìŒ ë¡œê·¸ë¥¼ ë§ì¶”ëŠ”\" ë°©ì‹ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ window_size=10ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. (ì§§ì€ ë¡œê·¸ë¡œ ë°ì´í„°ëŸ‰ì´ ì ì–´ 2ë¡œ ìˆ˜ì •)\n",
        "\n",
        "    Filtering: ë„ˆë¬´ ì§§ì€ ì„¸ì…˜ì€ í•™ìŠµì— ë„ì›€ì´ ì•ˆ ë˜ë¯€ë¡œ ì œì™¸í•©ë‹ˆë‹¤. (min_seq_length=5)\n",
        "\n",
        "    Mapping: ë¡œê·¸ì˜ ì´ë²¤íŠ¸ ID(110, 200 ë“±)ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤(0, 1, 2...)ë¡œ ë‹¤ì‹œ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
        "\n",
        "* ëª¨ë¸ ì„¤ê³„ (DeepLog í´ë˜ìŠ¤)\n",
        "\n",
        "    Embedding Layer: ì•ì„œ ì„¤ëª…í•´ ë“œë¦° ëŒ€ë¡œ One-hot Encoding ëŒ€ì‹  ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. (embedding_dim=32)\n",
        "\n",
        "    LSTM: ì‹œí€€ìŠ¤(ìˆœì„œ)ì˜ íë¦„ì„ í•™ìŠµí•˜ëŠ” í•µì‹¬ ì¸µì…ë‹ˆë‹¤.\n",
        "\n",
        "    Linear: ë‹¤ìŒì— ì˜¬ ì´ë²¤íŠ¸ê°€ ë¬´ì—‡ì¼ì§€ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ì¶œë ¥ì¸µì…ë‹ˆë‹¤.\n",
        "\n",
        "2. ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ (ë°ì´í„° ìƒíƒœ ì§„ë‹¨)\n",
        "ì‹¤í–‰ ë¡œê·¸ë¥¼ ë³´ë©´ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ìˆ˜ì •í•´ì•¼ í•  ì ì´ ëª…í™•íˆ ë³´ì…ë‹ˆë‹¤.\n",
        "\n",
        "    â‘  ì—„ì²­ë‚œ ë°ì´í„° ì†ì‹¤ ë°œìƒ (Critical)\n",
        "* í˜„ìƒ:\n",
        "\n",
        "    ì „ì²´ ë¡œë“œëœ ì„¸ì…˜: 462,328ê°œ (ë§¤ìš° ë§ìŒ, ì¢‹ìŒ)\n",
        "\n",
        "    ìŠ¤í‚µëœ ì„¸ì…˜: 427,287ê°œ (ì•½ 92%ê°€ ë²„ë ¤ì§)\n",
        "\n",
        "    ìµœì¢… í•™ìŠµ ë°ì´í„°: 724ê°œ (ë”¥ëŸ¬ë‹ì„ í•˜ê¸°ì—” í„±ì—†ì´ ë¶€ì¡±í•¨)\n",
        "\n",
        "* ì›ì¸:\n",
        "\n",
        "    ì½”ë“œ ì„¤ì •: window_size=10, min_seq_length=5 ( ì´ë¶€ë¶„ë„ ë°ì´í„° ì ì–´ì„œ 2ë¡œ ìˆ˜ì •)\n",
        "\n",
        "    ë°ì´í„° ìƒ˜í”Œì„ ë³´ë©´ event_sequence: [1] ì²˜ëŸ¼ ê¸¸ì´ê°€ 1ì¸ ì„¸ì…˜ì´ ë§ìŠµë‹ˆë‹¤.\n",
        "\n",
        "    DeepLogëŠ” ê³¼ê±° 10ê°œë¥¼ ë´ì•¼ í•˜ëŠ”ë°, ë°ì´í„° ê¸¸ì´ê°€ 1~5ë°–ì— ì•ˆ ë˜ë‹ˆ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ ìˆ˜ê°€ ì—†ì–´ì„œ ë‹¤ ë²„ë ¤ì§„ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "    â‘¡ ì´ë²¤íŠ¸ ì¢…ë¥˜\n",
        "* ê²°ê³¼: ì´ë²¤íŠ¸ í´ë˜ìŠ¤ ìˆ˜: 109, ì´ë²¤íŠ¸ ID ë²”ìœ„: 1 ~ 132\n",
        "\n"
      ],
      "metadata": {
        "id": "2oL6xorJQHN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    precision_recall_fscore_support, accuracy_score,\n",
        "    roc_curve, auc, precision_recall_curve\n",
        ")\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "# ==================== ì„±ê³¼í‰ê°€ í´ë˜ìŠ¤ ====================\n",
        "class DeepLogEvaluator:\n",
        "    \"\"\"DeepLog ëª¨ë¸ ì„±ê³¼í‰ê°€ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self, model, test_loader, device, num_classes, event_id_map):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.event_id_map = event_id_map\n",
        "        self.reverse_map = {v: k for k, v in event_id_map.items()}\n",
        "\n",
        "        # í‰ê°€ ê²°ê³¼ ì €ì¥\n",
        "        self.predictions = []\n",
        "        self.true_labels = []\n",
        "        self.probabilities = []\n",
        "        self.top_k_predictions = {k: [] for k in [1, 3, 5, 10]}\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"ëª¨ë¸ í‰ê°€ ì‹¤í–‰\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        print(\"=\" * 50)\n",
        "        print(\"ëª¨ë¸ í‰ê°€ ì‹œì‘\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in self.test_loader:\n",
        "                sequences = sequences.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # ì˜ˆì¸¡\n",
        "                outputs = self.model(sequences)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                # Top-1 ì˜ˆì¸¡\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                # ì €ì¥\n",
        "                self.predictions.extend(predicted.cpu().numpy())\n",
        "                self.true_labels.extend(labels.cpu().numpy())\n",
        "                self.probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "                # Top-K ì˜ˆì¸¡\n",
        "                for k in [1, 3, 5, 10]:\n",
        "                    if k <= self.num_classes:\n",
        "                        _, top_k_pred = probs.topk(k, dim=1)\n",
        "                        self.top_k_predictions[k].extend(top_k_pred.cpu().numpy())\n",
        "\n",
        "        self.predictions = np.array(self.predictions)\n",
        "        self.true_labels = np.array(self.true_labels)\n",
        "        self.probabilities = np.array(self.probabilities)\n",
        "\n",
        "        print(f\"âœ… í‰ê°€ ì™„ë£Œ! ì´ {len(self.predictions):,}ê°œ ìƒ˜í”Œ\")\n",
        "\n",
        "    def calculate_top_k_accuracy(self):\n",
        "        \"\"\"Top-K Accuracy ê³„ì‚°\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for k, preds in self.top_k_predictions.items():\n",
        "            if k > self.num_classes:\n",
        "                continue\n",
        "\n",
        "            preds = np.array(preds)\n",
        "            correct = 0\n",
        "\n",
        "            for i, true_label in enumerate(self.true_labels):\n",
        "                if true_label in preds[i]:\n",
        "                    correct += 1\n",
        "\n",
        "            accuracy = 100.0 * correct / len(self.true_labels)\n",
        "            results[f'Top-{k}'] = accuracy\n",
        "\n",
        "        return results\n",
        "\n",
        "    def calculate_basic_metrics(self):\n",
        "        \"\"\"ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
        "        accuracy = accuracy_score(self.true_labels, self.predictions) * 100\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            self.true_labels, self.predictions, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision * 100,\n",
        "            'Recall': recall * 100,\n",
        "            'F1-Score': f1 * 100\n",
        "        }\n",
        "\n",
        "    def plot_confusion_matrix(self, save_path='confusion_matrix.png', max_classes=20):\n",
        "        \"\"\"í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\"\"\"\n",
        "        # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë§Œ í‘œì‹œ\n",
        "        unique_labels = np.unique(np.concatenate([self.true_labels, self.predictions]))\n",
        "\n",
        "        if len(unique_labels) > max_classes:\n",
        "            # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ ì„ íƒ\n",
        "            label_counts = pd.Series(self.true_labels).value_counts()\n",
        "            top_labels = label_counts.head(max_classes).index.tolist()\n",
        "\n",
        "            # í•„í„°ë§\n",
        "            mask = np.isin(self.true_labels, top_labels)\n",
        "            filtered_true = self.true_labels[mask]\n",
        "            filtered_pred = self.predictions[mask]\n",
        "\n",
        "            cm = confusion_matrix(filtered_true, filtered_pred, labels=top_labels)\n",
        "            labels = [self.reverse_map.get(l, l) for l in top_labels]\n",
        "        else:\n",
        "            cm = confusion_matrix(self.true_labels, self.predictions)\n",
        "            labels = [self.reverse_map.get(l, l) for l in unique_labels]\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=labels, yticklabels=labels)\n",
        "        plt.title('Confusion Matrix (Top {} Classes)'.format(min(max_classes, len(unique_labels))))\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… Confusion Matrix ì €ì¥: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_top_k_accuracy(self, save_path='top_k_accuracy.png'):\n",
        "        \"\"\"Top-K Accuracy ì‹œê°í™”\"\"\"\n",
        "        top_k_results = self.calculate_top_k_accuracy()\n",
        "\n",
        "        k_values = [int(k.split('-')[1]) for k in top_k_results.keys()]\n",
        "        accuracies = list(top_k_results.values())\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(k_values, accuracies, marker='o', linewidth=2, markersize=8)\n",
        "        plt.xlabel('K', fontsize=12)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "        plt.title('Top-K Accuracy', fontsize=14)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # ê°’ í‘œì‹œ\n",
        "        for k, acc in zip(k_values, accuracies):\n",
        "            plt.text(k, acc + 1, f'{acc:.2f}%', ha='center', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… Top-K Accuracy ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "        return top_k_results\n",
        "\n",
        "    def plot_class_distribution(self, save_path='class_distribution.png'):\n",
        "        \"\"\"í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\"\"\"\n",
        "        true_counts = pd.Series(self.true_labels).value_counts().sort_index()\n",
        "        pred_counts = pd.Series(self.predictions).value_counts().sort_index()\n",
        "\n",
        "        # ìƒìœ„ 20ê°œ í´ë˜ìŠ¤ë§Œ\n",
        "        top_classes = true_counts.head(20).index\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # ì‹¤ì œ ë¶„í¬\n",
        "        axes[0].bar(range(len(top_classes)), true_counts[top_classes].values)\n",
        "        axes[0].set_xlabel('Event ID')\n",
        "        axes[0].set_ylabel('Count')\n",
        "        axes[0].set_title('True Label Distribution (Top 20)')\n",
        "        axes[0].set_xticks(range(len(top_classes)))\n",
        "        axes[0].set_xticklabels([self.reverse_map.get(c, c) for c in top_classes], rotation=45)\n",
        "\n",
        "        # ì˜ˆì¸¡ ë¶„í¬\n",
        "        axes[1].bar(range(len(top_classes)), pred_counts.reindex(top_classes, fill_value=0).values)\n",
        "        axes[1].set_xlabel('Event ID')\n",
        "        axes[1].set_ylabel('Count')\n",
        "        axes[1].set_title('Predicted Label Distribution (Top 20)')\n",
        "        axes[1].set_xticks(range(len(top_classes)))\n",
        "        axes[1].set_xticklabels([self.reverse_map.get(c, c) for c in top_classes], rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… í´ë˜ìŠ¤ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def plot_probability_distribution(self, save_path='probability_distribution.png'):\n",
        "        \"\"\"ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì‹œê°í™”\"\"\"\n",
        "        max_probs = np.max(self.probabilities, axis=1)\n",
        "\n",
        "        # ì •ë‹µ/ì˜¤ë‹µ êµ¬ë¶„\n",
        "        correct_mask = self.predictions == self.true_labels\n",
        "        correct_probs = max_probs[correct_mask]\n",
        "        wrong_probs = max_probs[~correct_mask]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.hist(correct_probs, bins=50, alpha=0.6, label='Correct', color='green')\n",
        "        plt.hist(wrong_probs, bins=50, alpha=0.6, label='Wrong', color='red')\n",
        "\n",
        "        plt.xlabel('Maximum Probability', fontsize=12)\n",
        "        plt.ylabel('Frequency', fontsize=12)\n",
        "        plt.title('Prediction Probability Distribution', fontsize=14)\n",
        "        plt.legend(fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… í™•ë¥  ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def generate_report(self, save_path='evaluation_report.txt'):\n",
        "        \"\"\"ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(\"DeepLog ëª¨ë¸ ì„±ê³¼í‰ê°€ ë³´ê³ ì„œ\")\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(\"\")\n",
        "\n",
        "        # ê¸°ë³¸ ë©”íŠ¸ë¦­\n",
        "        basic_metrics = self.calculate_basic_metrics()\n",
        "        report.append(\"1. ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ\")\n",
        "        report.append(\"-\" * 70)\n",
        "        for metric, value in basic_metrics.items():\n",
        "            report.append(f\"   {metric:15s}: {value:6.2f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Top-K Accuracy\n",
        "        top_k_results = self.calculate_top_k_accuracy()\n",
        "        report.append(\"2. Top-K Accuracy\")\n",
        "        report.append(\"-\" * 70)\n",
        "        for k, acc in top_k_results.items():\n",
        "            report.append(f\"   {k:10s}: {acc:6.2f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
        "        report.append(\"3. í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ (ìƒìœ„ 10ê°œ)\")\n",
        "        report.append(\"-\" * 70)\n",
        "\n",
        "        # ê°€ì¥ ë¹ˆë²ˆí•œ 10ê°œ í´ë˜ìŠ¤\n",
        "        unique, counts = np.unique(self.true_labels, return_counts=True)\n",
        "        top_10_classes = unique[np.argsort(-counts)][:10]\n",
        "\n",
        "        for cls in top_10_classes:\n",
        "            mask = self.true_labels == cls\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            cls_acc = accuracy_score(\n",
        "                self.true_labels[mask],\n",
        "                self.predictions[mask]\n",
        "            ) * 100\n",
        "\n",
        "            original_id = self.reverse_map.get(cls, cls)\n",
        "            count = mask.sum()\n",
        "\n",
        "            report.append(f\"   Event {original_id:3d}: Accuracy {cls_acc:6.2f}% (ìƒ˜í”Œ: {count:4d}ê°œ)\")\n",
        "\n",
        "        report.append(\"\")\n",
        "        report.append(\"=\" * 70)\n",
        "\n",
        "        # íŒŒì¼ ì €ì¥\n",
        "        with open(save_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(report))\n",
        "\n",
        "        # ì½˜ì†” ì¶œë ¥\n",
        "        print('\\n'.join(report))\n",
        "        print(f\"\\nâœ… í‰ê°€ ë³´ê³ ì„œ ì €ì¥: {save_path}\")\n",
        "\n",
        "        return '\\n'.join(report)\n",
        "\n",
        "# ==================== ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€ ====================\n",
        "class AnomalyDetectionEvaluator:\n",
        "    \"\"\"ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self, model, test_data, event_id_map, window_size, device):\n",
        "        self.model = model\n",
        "        self.test_data = test_data  # (sequences, labels)\n",
        "        self.event_id_map = event_id_map\n",
        "        self.window_size = window_size\n",
        "        self.device = device\n",
        "        self.reverse_map = {v: k for k, v in event_id_map.items()}\n",
        "\n",
        "    def inject_anomalies(self, anomaly_ratio=0.3):\n",
        "        \"\"\"ì •ìƒ ë°ì´í„°ì— ì´ìƒì„ ì£¼ì…\"\"\"\n",
        "        sequences, labels = self.test_data\n",
        "        n_samples = len(sequences)\n",
        "        n_anomalies = int(n_samples * anomaly_ratio)\n",
        "\n",
        "        # ë¬´ì‘ìœ„ë¡œ ì„ íƒ\n",
        "        anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
        "\n",
        "        # ì´ìƒ ë ˆì´ë¸” ìƒì„±\n",
        "        is_anomaly = np.zeros(n_samples, dtype=bool)\n",
        "        is_anomaly[anomaly_indices] = True\n",
        "\n",
        "        # ì´ìƒ ë°ì´í„° ìƒì„± (ë ˆì´ë¸”ì„ ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ë³€ê²½)\n",
        "        modified_labels = labels.copy()\n",
        "        num_classes = len(self.event_id_map)\n",
        "\n",
        "        for idx in anomaly_indices:\n",
        "            # í˜„ì¬ ë ˆì´ë¸”ê³¼ ë‹¤ë¥¸ ë¬´ì‘ìœ„ ë ˆì´ë¸”\n",
        "            current_label = labels[idx]\n",
        "            new_label = np.random.randint(1, num_classes)\n",
        "            while new_label == current_label:\n",
        "                new_label = np.random.randint(1, num_classes)\n",
        "            modified_labels[idx] = new_label\n",
        "\n",
        "        return sequences, modified_labels, is_anomaly\n",
        "\n",
        "    def evaluate_anomaly_detection(self, top_k_values=[1, 3, 5]):\n",
        "        \"\"\"ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "        print(\"=\" * 50)\n",
        "        print(\"ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€ ì‹œì‘\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # ì´ìƒ ì£¼ì…\n",
        "        sequences, anomaly_labels, true_anomalies = self.inject_anomalies(anomaly_ratio=0.3)\n",
        "\n",
        "        print(f\"ì •ìƒ ìƒ˜í”Œ: {(~true_anomalies).sum():,}ê°œ\")\n",
        "        print(f\"ì´ìƒ ìƒ˜í”Œ: {true_anomalies.sum():,}ê°œ\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for top_k in top_k_values:\n",
        "            detected_anomalies = []\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for i in range(len(sequences)):\n",
        "                    seq = torch.LongTensor([sequences[i]]).to(self.device)\n",
        "                    output = self.model(seq)\n",
        "                    probs = torch.softmax(output, dim=1)\n",
        "\n",
        "                    # Top-K ì˜ˆì¸¡\n",
        "                    _, top_k_pred = probs.topk(top_k, dim=1)\n",
        "                    top_k_pred = top_k_pred[0].cpu().numpy()\n",
        "\n",
        "                    # ì‹¤ì œ ë ˆì´ë¸”ì´ Top-Kì— ì—†ìœ¼ë©´ ì´ìƒ\n",
        "                    actual_label = anomaly_labels[i]\n",
        "                    is_detected = actual_label not in top_k_pred\n",
        "                    detected_anomalies.append(is_detected)\n",
        "\n",
        "            detected_anomalies = np.array(detected_anomalies)\n",
        "\n",
        "            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "            tp = np.sum(detected_anomalies & true_anomalies)\n",
        "            fp = np.sum(detected_anomalies & ~true_anomalies)\n",
        "            tn = np.sum(~detected_anomalies & ~true_anomalies)\n",
        "            fn = np.sum(~detected_anomalies & true_anomalies)\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            accuracy = (tp + tn) / len(sequences)\n",
        "\n",
        "            results[f'Top-{top_k}'] = {\n",
        "                'Precision': precision * 100,\n",
        "                'Recall': recall * 100,\n",
        "                'F1-Score': f1 * 100,\n",
        "                'Accuracy': accuracy * 100,\n",
        "                'TP': tp,\n",
        "                'FP': fp,\n",
        "                'TN': tn,\n",
        "                'FN': fn\n",
        "            }\n",
        "\n",
        "        # ê²°ê³¼ ì¶œë ¥\n",
        "        print(\"\\nì´ìƒ íƒì§€ ì„±ëŠ¥:\")\n",
        "        print(\"-\" * 70)\n",
        "        for top_k, metrics in results.items():\n",
        "            print(f\"\\n{top_k}:\")\n",
        "            print(f\"  Precision: {metrics['Precision']:.2f}%\")\n",
        "            print(f\"  Recall:    {metrics['Recall']:.2f}%\")\n",
        "            print(f\"  F1-Score:  {metrics['F1-Score']:.2f}%\")\n",
        "            print(f\"  Accuracy:  {metrics['Accuracy']:.2f}%\")\n",
        "            print(f\"  TP={metrics['TP']}, FP={metrics['FP']}, TN={metrics['TN']}, FN={metrics['FN']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_anomaly_detection_metrics(self, results, save_path='anomaly_detection_metrics.png'):\n",
        "        \"\"\"ì´ìƒ íƒì§€ ë©”íŠ¸ë¦­ ì‹œê°í™”\"\"\"\n",
        "        top_k_values = [int(k.split('-')[1]) for k in results.keys()]\n",
        "\n",
        "        metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "        values = {metric: [results[f'Top-{k}'][metric] for k in top_k_values]\n",
        "                  for metric in metrics}\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        x = np.arange(len(top_k_values))\n",
        "        width = 0.2\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            plt.bar(x + i * width, values[metric], width, label=metric)\n",
        "\n",
        "        plt.xlabel('Top-K', fontsize=12)\n",
        "        plt.ylabel('Score (%)', fontsize=12)\n",
        "        plt.title('Anomaly Detection Performance', fontsize=14)\n",
        "        plt.xticks(x + width * 1.5, [f'Top-{k}' for k in top_k_values])\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… ì´ìƒ íƒì§€ ë©”íŠ¸ë¦­ ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "# ==================== ë©”ì¸ ì‹¤í–‰ ì½”ë“œì— í‰ê°€ ì¶”ê°€ ====================\n",
        "def train_deeplog(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sequences, labels in train_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_deeplog(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in test_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "Q23nkvUaPZ6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== ë©”ì¸ ì‹¤í–‰ ====================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # í•™ìŠµ ì™„ë£Œ í›„ ì¢…í•© í‰ê°€ ì‹¤í–‰\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"5ë‹¨ê³„: ì¢…í•© ì„±ê³¼í‰ê°€\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. ê¸°ë³¸ ì„±ëŠ¥ í‰ê°€\n",
        "    evaluator = DeepLogEvaluator(\n",
        "        model=model,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        num_classes=num_classes,\n",
        "        event_id_map=event_id_map\n",
        "    )\n",
        "\n",
        "    # í‰ê°€ ì‹¤í–‰\n",
        "    evaluator.evaluate()\n",
        "\n",
        "    # 2. í‰ê°€ ë³´ê³ ì„œ ìƒì„±\n",
        "    evaluator.generate_report('deeplog_evaluation_report.txt')\n",
        "\n",
        "    # 3. ì‹œê°í™”\n",
        "    evaluator.plot_confusion_matrix('confusion_matrix.png', max_classes=20)\n",
        "    evaluator.plot_top_k_accuracy('top_k_accuracy.png')\n",
        "    evaluator.plot_class_distribution('class_distribution.png')\n",
        "    evaluator.plot_probability_distribution('probability_distribution.png')\n",
        "\n",
        "    # 4. ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€\n",
        "    anomaly_evaluator = AnomalyDetectionEvaluator(\n",
        "        model=model,\n",
        "        test_data=(X_test, y_test),\n",
        "        event_id_map=event_id_map,\n",
        "        window_size=window_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    anomaly_results = anomaly_evaluator.evaluate_anomaly_detection(top_k_values=[1, 3, 5])\n",
        "    anomaly_evaluator.plot_anomaly_detection_metrics(anomaly_results, 'anomaly_detection_metrics.png')\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nìƒì„±ëœ íŒŒì¼:\")\n",
        "    print(\"  1. best_deeplog_model.pth - í•™ìŠµëœ ëª¨ë¸\")\n",
        "    print(\"  2. deeplog_evaluation_report.txt - í‰ê°€ ë³´ê³ ì„œ\")\n",
        "    print(\"  3. confusion_matrix.png - í˜¼ë™ í–‰ë ¬\")\n",
        "    print(\"  4. top_k_accuracy.png - Top-K ì •í™•ë„\")\n",
        "    print(\"  5. class_distribution.png - í´ë˜ìŠ¤ ë¶„í¬\")\n",
        "    print(\"  6. probability_distribution.png - í™•ë¥  ë¶„í¬\")\n",
        "    print(\"  7. anomaly_detection_metrics.png - ì´ìƒ íƒì§€ ì„±ëŠ¥\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeLzodAHPHBB",
        "outputId": "4e2ebe19-9c7a-4025-cef0-a4c4df382ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "5ë‹¨ê³„: ì¢…í•© ì„±ê³¼í‰ê°€\n",
            "==================================================\n",
            "==================================================\n",
            "ëª¨ë¸ í‰ê°€ ì‹œì‘\n",
            "==================================================\n",
            "âœ… í‰ê°€ ì™„ë£Œ! ì´ 23,950ê°œ ìƒ˜í”Œ\n",
            "======================================================================\n",
            "DeepLog ëª¨ë¸ ì„±ê³¼í‰ê°€ ë³´ê³ ì„œ\n",
            "======================================================================\n",
            "\n",
            "1. ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ\n",
            "----------------------------------------------------------------------\n",
            "   Accuracy       :  87.49%\n",
            "   Precision      :  87.70%\n",
            "   Recall         :  87.49%\n",
            "   F1-Score       :  86.61%\n",
            "\n",
            "2. Top-K Accuracy\n",
            "----------------------------------------------------------------------\n",
            "   Top-1     :  87.49%\n",
            "   Top-3     :  99.39%\n",
            "   Top-5     :  99.93%\n",
            "   Top-10    :  99.95%\n",
            "\n",
            "3. í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ (ìƒìœ„ 10ê°œ)\n",
            "----------------------------------------------------------------------\n",
            "   Event  34: Accuracy  95.87% (ìƒ˜í”Œ: 3246ê°œ)\n",
            "   Event  32: Accuracy  88.80% (ìƒ˜í”Œ: 2188ê°œ)\n",
            "   Event  13: Accuracy  91.73% (ìƒ˜í”Œ: 2091ê°œ)\n",
            "   Event  37: Accuracy  97.45% (ìƒ˜í”Œ: 1842ê°œ)\n",
            "   Event  90: Accuracy  97.86% (ìƒ˜í”Œ: 1821ê°œ)\n",
            "   Event  14: Accuracy  96.28% (ìƒ˜í”Œ: 1694ê°œ)\n",
            "   Event  38: Accuracy  79.86% (ìƒ˜í”Œ: 1415ê°œ)\n",
            "   Event  35: Accuracy  90.67% (ìƒ˜í”Œ: 1158ê°œ)\n",
            "   Event  91: Accuracy  96.87% (ìƒ˜í”Œ: 1149ê°œ)\n",
            "   Event  18: Accuracy  97.26% (ìƒ˜í”Œ:  985ê°œ)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "âœ… í‰ê°€ ë³´ê³ ì„œ ì €ì¥: deeplog_evaluation_report.txt\n",
            "âœ… Confusion Matrix ì €ì¥: confusion_matrix.png\n",
            "âœ… Top-K Accuracy ê·¸ë˜í”„ ì €ì¥: top_k_accuracy.png\n",
            "âœ… í´ë˜ìŠ¤ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: class_distribution.png\n",
            "âœ… í™•ë¥  ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: probability_distribution.png\n",
            "==================================================\n",
            "ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€ ì‹œì‘\n",
            "==================================================\n",
            "ì •ìƒ ìƒ˜í”Œ: 16,765ê°œ\n",
            "ì´ìƒ ìƒ˜í”Œ: 7,185ê°œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3264149151.py:346: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  seq = torch.LongTensor([sequences[i]]).to(self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ì´ìƒ íƒì§€ ì„±ëŠ¥:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Top-1:\n",
            "  Precision: 77.42%\n",
            "  Recall:    99.93%\n",
            "  F1-Score:  87.25%\n",
            "  Accuracy:  91.24%\n",
            "  TP=7180, FP=2094, TN=14671, FN=5\n",
            "\n",
            "Top-3:\n",
            "  Precision: 98.63%\n",
            "  Recall:    98.29%\n",
            "  F1-Score:  98.46%\n",
            "  Accuracy:  99.08%\n",
            "  TP=7062, FP=98, TN=16667, FN=123\n",
            "\n",
            "Top-5:\n",
            "  Precision: 99.78%\n",
            "  Recall:    96.37%\n",
            "  F1-Score:  98.05%\n",
            "  Accuracy:  98.85%\n",
            "  TP=6924, FP=15, TN=16750, FN=261\n",
            "âœ… ì´ìƒ íƒì§€ ë©”íŠ¸ë¦­ ê·¸ë˜í”„ ì €ì¥: anomaly_detection_metrics.png\n",
            "\n",
            "==================================================\n",
            "âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!\n",
            "==================================================\n",
            "\n",
            "ìƒì„±ëœ íŒŒì¼:\n",
            "  1. best_deeplog_model.pth - í•™ìŠµëœ ëª¨ë¸\n",
            "  2. deeplog_evaluation_report.txt - í‰ê°€ ë³´ê³ ì„œ\n",
            "  3. confusion_matrix.png - í˜¼ë™ í–‰ë ¬\n",
            "  4. top_k_accuracy.png - Top-K ì •í™•ë„\n",
            "  5. class_distribution.png - í´ë˜ìŠ¤ ë¶„í¬\n",
            "  6. probability_distribution.png - í™•ë¥  ë¶„í¬\n",
            "  7. anomaly_detection_metrics.png - ì´ìƒ íƒì§€ ì„±ëŠ¥\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}