# ë¡œê·¸ ìˆ˜ì§‘ ë° í†µí•© íŒŒì´í”„ë¼ì¸ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

MSA í™˜ê²½ì—ì„œ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ê³  í†µí•©í•˜ì—¬ ì „ì²˜ë¦¬í•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.

---

## ğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸

```
ë¡œê·¸ ìˆ˜ì§‘ â†’ ë¡œê·¸ í†µí•© â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ í•™ìŠµ
   â†“           â†“          â†“         â†“
 ì—¬ëŸ¬ ì„œë¹„ìŠ¤   í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°  ì •ì œ/ë³€í™˜   í•™ìŠµ ë°ì´í„°
```

---

## ğŸ“Š ë¡œê·¸ ìˆ˜ì§‘ ë‹¨ê³„

### 1. ë¡œê·¸ ì†ŒìŠ¤

**MSA ì„œë¹„ìŠ¤:**
- `gateway`: API Gateway ë¡œê·¸
- `eureka`: Service Discovery ë¡œê·¸
- `user`: User Service ë¡œê·¸
- `research`: Research Service ë¡œê·¸
- `manager`: Manager Service ë¡œê·¸
- `code`: Code Service ë¡œê·¸

**ë¡œê·¸ ìœ„ì¹˜:**
```
logs/
â”œâ”€â”€ gateway/
â”‚   â”œâ”€â”€ gateway_2025-02-24.log
â”‚   â”œâ”€â”€ gateway_2025-02-25.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ eureka/
â”‚   â”œâ”€â”€ eureka_2025-02-24.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ user/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ research/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ manager/
â”‚   â””â”€â”€ ...
â””â”€â”€ code/
    â””â”€â”€ ...
```

---

## ğŸ”§ ë¡œê·¸ ìˆ˜ì§‘ ëª¨ë“ˆ

### `preprocessing/log_collector.py`

```python
#!/usr/bin/env python3
"""
ë¡œê·¸ ìˆ˜ì§‘ ëª¨ë“ˆ
ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ íŒŒì¼ì„ ìˆ˜ì§‘í•˜ê³  í†µí•©
"""

import os
import glob
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)


class LogCollector:
    """ë¡œê·¸ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        log_dirs: Dict[str, Path],
        output_dir: Path,
        date_range: Optional[tuple] = None
    ):
        """
        Args:
            log_dirs: ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬
                {
                    'gateway': Path('logs/gateway'),
                    'eureka': Path('logs/eureka'),
                    ...
                }
            output_dir: í†µí•©ëœ ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            date_range: ë‚ ì§œ ë²”ìœ„ (start_date, end_date) ë˜ëŠ” None (ì „ì²´)
        """
        self.log_dirs = log_dirs
        self.output_dir = output_dir
        self.date_range = date_range
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def collect_logs(
        self,
        service_name: str,
        log_pattern: str = "*.log"
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ ìˆ˜ì§‘
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
            log_pattern: ë¡œê·¸ íŒŒì¼ íŒ¨í„´
        
        Returns:
            ë¡œê·¸ ì—”íŠ¸ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        log_dir = self.log_dirs.get(service_name)
        if not log_dir or not log_dir.exists():
            logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {service_name}")
            return []
        
        log_files = sorted(log_dir.glob(log_pattern))
        all_logs = []
        
        for log_file in log_files:
            # ë‚ ì§œ í•„í„°ë§
            if self.date_range:
                file_date = self._extract_date_from_filename(log_file)
                if file_date and not self._is_in_range(file_date):
                    continue
            
            logs = self._read_log_file(log_file, service_name)
            all_logs.extend(logs)
            logger.info(f"{service_name}: {log_file.name} - {len(logs)}ê°œ ë¡œê·¸ ìˆ˜ì§‘")
        
        return all_logs
    
    def _read_log_file(
        self,
        log_file: Path,
        service_name: str
    ) -> List[Dict[str, Any]]:
        """ë¡œê·¸ íŒŒì¼ ì½ê¸°"""
        logs = []
        
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    log_entry = self._parse_log_line(line, service_name, log_file.name, line_num)
                    if log_entry:
                        logs.append(log_entry)
        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({log_file}): {e}")
        
        return logs
    
    def _parse_log_line(
        self,
        line: str,
        service_name: str,
        filename: str,
        line_num: int
    ) -> Optional[Dict[str, Any]]:
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        # ê¸°ë³¸ ë¡œê·¸ í˜•ì‹: "2025-12-08 17:23:47.950 INFO ..."
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            timestamp_str = line[:23]  # "2025-12-08 17:23:47.950"
            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f")
            
            # ë ˆë²¨ ì¶”ì¶œ
            level = None
            for log_level in ['ERROR', 'WARN', 'INFO', 'DEBUG', 'TRACE']:
                if log_level in line:
                    level = log_level
                    break
            
            # ë©”ì‹œì§€ ì¶”ì¶œ
            message = line[24:].strip()
            
            return {
                'timestamp': timestamp,
                'level': level,
                'message': message,
                'service_name': service_name,
                'source_file': filename,
                'line_number': line_num,
                'raw_line': line
            }
        except Exception as e:
            logger.debug(f"ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨ (line {line_num}): {e}")
            # íŒŒì‹± ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
            return {
                'timestamp': None,
                'level': None,
                'message': line,
                'service_name': service_name,
                'source_file': filename,
                'line_number': line_num,
                'raw_line': line
            }
    
    def _extract_date_from_filename(self, filepath: Path) -> Optional[datetime]:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        # ì˜ˆ: "gateway_2025-02-24.log" -> 2025-02-24
        try:
            filename = filepath.stem
            date_str = filename.split('_')[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„
            return datetime.strptime(date_str, "%Y-%m-%d")
        except:
            return None
    
    def _is_in_range(self, date: datetime) -> bool:
        """ë‚ ì§œê°€ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        if not self.date_range:
            return True
        
        start_date, end_date = self.date_range
        return start_date <= date <= end_date
    
    def collect_all_services(self) -> Dict[str, List[Dict[str, Any]]]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ ìˆ˜ì§‘"""
        all_logs = {}
        
        for service_name in self.log_dirs.keys():
            logs = self.collect_logs(service_name)
            all_logs[service_name] = logs
            logger.info(f"{service_name}: ì´ {len(logs):,}ê°œ ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return all_logs
    
    def merge_logs(
        self,
        all_logs: Dict[str, List[Dict[str, Any]]]
    ) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ë¡œê·¸ë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        
        Args:
            all_logs: ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í†µí•© ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
        """
        merged_logs = []
        
        for service_name, logs in all_logs.items():
            merged_logs.extend(logs)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬
        merged_logs.sort(key=lambda x: x.get('timestamp') or datetime.min)
        
        logger.info(f"í†µí•©ëœ ë¡œê·¸ ìˆ˜: {len(merged_logs):,}ê°œ")
        
        return merged_logs
    
    def save_merged_logs(
        self,
        merged_logs: List[Dict[str, Any]],
        output_filename: str = "merged_logs.json"
    ):
        """í†µí•©ëœ ë¡œê·¸ ì €ì¥"""
        import json
        
        output_path = self.output_dir / output_filename
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_logs, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"í†µí•© ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # í†µê³„ ì •ë³´ ì €ì¥
        stats = self._calculate_statistics(merged_logs)
        stats_path = self.output_dir / "collection_statistics.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í†µê³„ ì •ë³´ ì €ì¥ ì™„ë£Œ: {stats_path}")
    
    def _calculate_statistics(
        self,
        merged_logs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„ ê³„ì‚°"""
        stats = {
            'total_logs': len(merged_logs),
            'by_service': defaultdict(int),
            'by_level': defaultdict(int),
            'date_range': {
                'start': None,
                'end': None
            }
        }
        
        timestamps = []
        
        for log in merged_logs:
            service = log.get('service_name', 'unknown')
            level = log.get('level', 'unknown')
            
            stats['by_service'][service] += 1
            stats['by_level'][level] += 1
            
            timestamp = log.get('timestamp')
            if timestamp:
                timestamps.append(timestamp)
        
        if timestamps:
            stats['date_range']['start'] = min(timestamps).isoformat()
            stats['date_range']['end'] = max(timestamps).isoformat()
        
        return dict(stats)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    from pathlib import Path
    
    parser = argparse.ArgumentParser(description='ë¡œê·¸ ìˆ˜ì§‘ ë° í†µí•©')
    parser.add_argument('--log-dirs', type=str, required=True,
                       help='ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì„œë¹„ìŠ¤:ê²½ë¡œ í˜•ì‹, ì‰¼í‘œë¡œ êµ¬ë¶„)')
    parser.add_argument('--output-dir', type=str, default='logs/merged',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--start-date', type=str, default=None,
                       help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ íŒŒì‹±
    log_dirs = {}
    for item in args.log_dirs.split(','):
        service, path = item.split(':')
        log_dirs[service.strip()] = Path(path.strip())
    
    # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    date_range = None
    if args.start_date and args.end_date:
        start_date = datetime.strptime(args.start_date, "%Y-%m-%d")
        end_date = datetime.strptime(args.end_date, "%Y-%m-%d")
        date_range = (start_date, end_date)
    
    # ë¡œê·¸ ìˆ˜ì§‘
    collector = LogCollector(
        log_dirs=log_dirs,
        output_dir=Path(args.output_dir),
        date_range=date_range
    )
    
    # ëª¨ë“  ì„œë¹„ìŠ¤ ë¡œê·¸ ìˆ˜ì§‘
    all_logs = collector.collect_all_services()
    
    # ë¡œê·¸ ë³‘í•©
    merged_logs = collector.merge_logs(all_logs)
    
    # ì €ì¥
    collector.save_merged_logs(merged_logs)


if __name__ == '__main__':
    main()
```

---

## ğŸ”„ ë¡œê·¸ í†µí•© í”„ë¡œì„¸ìŠ¤

### ë‹¨ê³„ë³„ ì²˜ë¦¬

1. **ë¡œê·¸ ìˆ˜ì§‘**
   - ê° ì„œë¹„ìŠ¤ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ ì½ê¸°
   - ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ì„ íƒì‚¬í•­)

2. **ë¡œê·¸ íŒŒì‹±**
   - íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
   - ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ (ERROR, WARN, INFO, DEBUG)
   - ë©”ì‹œì§€ ì¶”ì¶œ
   - ì„œë¹„ìŠ¤ëª… íƒœê¹…

3. **ë¡œê·¸ ë³‘í•©**
   - íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬
   - ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í†µí•©
   - Trace ID ì—°ê²° (ê°€ëŠ¥í•œ ê²½ìš°)

4. **í†µí•© ë¡œê·¸ ì €ì¥**
   - JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
   - í†µê³„ ì •ë³´ ìƒì„±

---

## ğŸ“Š í†µí•© ë¡œê·¸ í˜•ì‹

### ì €ì¥ í˜•ì‹

```json
[
  {
    "timestamp": "2025-12-08T17:23:47.950000",
    "level": "INFO",
    "message": "Starting Application",
    "service_name": "gateway",
    "source_file": "gateway_2025-12-08.log",
    "line_number": 1,
    "raw_line": "2025-12-08 17:23:47.950 INFO ..."
  },
  {
    "timestamp": "2025-12-08T17:23:48.123000",
    "level": "WARN",
    "message": "Connection timeout",
    "service_name": "gateway",
    "source_file": "gateway_2025-12-08.log",
    "line_number": 2,
    "raw_line": "2025-12-08 17:23:48.123 WARN ..."
  },
  ...
]
```

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
python preprocessing/log_collector.py \
    --log-dirs "gateway:logs/gateway,eureka:logs/eureka,user:logs/user,research:logs/research,manager:logs/manager,code:logs/code" \
    --output-dir logs/merged \
    --start-date 2025-02-24 \
    --end-date 2025-12-08
```

### Python ì½”ë“œë¡œ ì‚¬ìš©

```python
from preprocessing.log_collector import LogCollector
from pathlib import Path
from datetime import datetime

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
log_dirs = {
    'gateway': Path('logs/gateway'),
    'eureka': Path('logs/eureka'),
    'user': Path('logs/user'),
    'research': Path('logs/research'),
    'manager': Path('logs/manager'),
    'code': Path('logs/code')
}

# ë‚ ì§œ ë²”ìœ„ ì„¤ì •
date_range = (
    datetime(2025, 2, 24),
    datetime(2025, 12, 8)
)

# ë¡œê·¸ ìˆ˜ì§‘
collector = LogCollector(
    log_dirs=log_dirs,
    output_dir=Path('logs/merged'),
    date_range=date_range
)

# ëª¨ë“  ì„œë¹„ìŠ¤ ë¡œê·¸ ìˆ˜ì§‘
all_logs = collector.collect_all_services()

# ë¡œê·¸ ë³‘í•©
merged_logs = collector.merge_logs(all_logs)

# ì €ì¥
collector.save_merged_logs(merged_logs, 'merged_logs_2025-02-24_to_2025-12-08.json')
```

---

## ğŸ“ˆ í†µê³„ ì •ë³´

### ìˆ˜ì§‘ í†µê³„ ì˜ˆì‹œ

```json
{
  "total_logs": 2464136,
  "by_service": {
    "gateway": 500000,
    "eureka": 200000,
    "user": 400000,
    "research": 500000,
    "manager": 400000,
    "code": 464136
  },
  "by_level": {
    "INFO": 2000000,
    "WARN": 300000,
    "ERROR": 100000,
    "DEBUG": 64136
  },
  "date_range": {
    "start": "2025-02-24T00:00:00",
    "end": "2025-12-08T23:59:59"
  }
}
```

---

## ğŸ”— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ê³¼ì˜ ì—°ê²°

### ì „ì²´ í”„ë¡œì„¸ìŠ¤

```
1. ë¡œê·¸ ìˆ˜ì§‘ (log_collector.py)
   â†“
2. ë¡œê·¸ í†µí•© (merge_logs)
   â†“
3. ì „ì²˜ë¦¬ (msa_preprocessor.py)
   â†“
4. ëª¨ë¸ í•™ìŠµ ë°ì´í„° ìƒì„±
```

### í†µí•© ìŠ¤í¬ë¦½íŠ¸

```python
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
from preprocessing.log_collector import LogCollector
from preprocessing.msa_preprocessor import MSAPreprocessor

# 1. ë¡œê·¸ ìˆ˜ì§‘ ë° í†µí•©
collector = LogCollector(...)
all_logs = collector.collect_all_services()
merged_logs = collector.merge_logs(all_logs)
collector.save_merged_logs(merged_logs)

# 2. ì „ì²˜ë¦¬
preprocessor = MSAPreprocessor(...)
preprocessed_data = preprocessor.preprocess(merged_logs)
preprocessor.save_preprocessed_data(preprocessed_data)
```

---

## ğŸ’¡ ìµœì í™” íŒ

### 1. ì¦ë¶„ ìˆ˜ì§‘

```python
# ìƒˆë¡œìš´ ë¡œê·¸ë§Œ ìˆ˜ì§‘
collector = LogCollector(
    log_dirs=log_dirs,
    output_dir=output_dir,
    date_range=(last_collected_date, today)
)
```

### 2. ë³‘ë ¬ ì²˜ë¦¬

```python
from concurrent.futures import ThreadPoolExecutor

# ì—¬ëŸ¬ ì„œë¹„ìŠ¤ ë¡œê·¸ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘
with ThreadPoolExecutor(max_workers=6) as executor:
    futures = {
        executor.submit(collector.collect_logs, service): service
        for service in log_dirs.keys()
    }
```

### 3. ì••ì¶• ì €ì¥

```python
import gzip
import json

# ëŒ€ìš©ëŸ‰ ë¡œê·¸ëŠ” ì••ì¶•í•˜ì—¬ ì €ì¥
with gzip.open('merged_logs.json.gz', 'wt', encoding='utf-8') as f:
    json.dump(merged_logs, f, ensure_ascii=False, indent=2)
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë¡œê·¸ ìˆ˜ì§‘ ë‹¨ê³„

- [ ] ê° ì„œë¹„ìŠ¤ ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
- [ ] ë¡œê·¸ íŒŒì¼ í˜•ì‹ í™•ì¸
- [ ] ë‚ ì§œ ë²”ìœ„ ê²°ì •
- [ ] ë¡œê·¸ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] í†µí•© ë¡œê·¸ ê²€ì¦
- [ ] í†µê³„ ì •ë³´ í™•ì¸

### ì „ì²˜ë¦¬ ë‹¨ê³„

- [ ] í†µí•© ë¡œê·¸ ë¡œë“œ
- [ ] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- [ ] ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦
- [ ] ë‹¤ë¥¸ ë©¤ë²„ë“¤ì—ê²Œ ê³µìœ 

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ë¡œê·¸ ìˆ˜ì§‘ë¶€í„° ì „ì²˜ë¦¬ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
