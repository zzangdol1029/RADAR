# LogBERT ëª¨ë¸ í•™ìŠµ ì§„í–‰ ê³„íš ğŸ“š

> ì‘ì„±ì¼: 2026-02-01  
> ëª©í‘œ: output í´ë”ì˜ ì „ì²˜ë¦¬ ì™„ë£Œ ë°ì´í„°ë¡œ LogBERT ëª¨ë¸ í•™ìŠµ

---

## ğŸ“Š í˜„ì¬ ìƒí™© ë¶„ì„

### 1. ì „ì²˜ë¦¬ ì™„ë£Œ ë°ì´í„°

**ìœ„ì¹˜**: `c:\workspace\RADAR\output\`

**ë°ì´í„° í˜„í™©**:
- **ì´ íŒŒì¼ ìˆ˜**: 324ê°œ
- **ê¸°ê°„**: 2025-02-24 ~ 2026-01-15 (ì•½ 11ê°œì›”)
- **ì´ ë°ì´í„° í¬ê¸°**: ì•½ **137 GB**
- **íŒŒì¼ í¬ë§·**: JSON (ë‚ ì§œë³„)

**ë°ì´í„° êµ¬ì¡°**:
```json
{
  "session_id": 0,
  "event_sequence": [1, 5, 1, 12, 3],
  "token_ids": [101, 1, 2, 3, 102, 0, 0],
  "attention_mask": [1, 1, 1, 1, 1, 0, 0],
  "has_error": false,
  "has_warn": true,
  "service_name": "gateway",
  "original_logs": ["..."]
}
```

**ì¼ë³„ ë°ì´í„° í¬ê¸° ë¶„ì„**:

| ê¸°ê°„ | í‰ê·  í¬ê¸° | íŠ¹ì§• |
|------|-----------|------|
| 2025-02 (24-28) | 409 MB | ì´ˆê¸° ë°ì´í„° |
| 2025-03 | 344 MB | ì•ˆì •ì  |
| 2025-04 | 426 MB | ì¦ê°€ |
| 2025-05 | 599 MB | í° ì¦ê°€ |
| 2025-06 | 650 MB | ìµœëŒ€ |
| 2025-07 ~ 2026-01 | 430 MB | ì¼ì • ìœ ì§€ |

**íŠ¹ì´ì **:
- ìµœëŒ€ íŒŒì¼ í¬ê¸°: **1.59 GB** (2025-07-03)
- ìµœì†Œ íŒŒì¼ í¬ê¸°: **9 MB** (2025-11-15)
- ì£¼ë§/ê³µíœ´ì¼: ë°ì´í„° í¬ê¸° ì‘ìŒ

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

### 1. ì£¼ìš” ëª©í‘œ
- **ì •ìƒ ë¡œê·¸ íŒ¨í„´ í•™ìŠµ**: MLM ë°©ì‹ìœ¼ë¡œ ì •ìƒì ì¸ ë¡œê·¸ ì‹œí€€ìŠ¤ í•™ìŠµ
- **ì´ìƒ íƒì§€ ëª¨ë¸ êµ¬ì¶•**: í•™ìŠµëœ íŒ¨í„´ê³¼ ë‹¤ë¥¸ ë¡œê·¸ë¥¼ ì´ìƒìœ¼ë¡œ íƒì§€
- **ì„œë¹„ìŠ¤ë³„ íŠ¹ì„± ë°˜ì˜**: Gateway, Research, Manager ë“± ì„œë¹„ìŠ¤ë³„ íŒ¨í„´ í•™ìŠµ

### 2. ì„±ëŠ¥ ì§€í‘œ
- **MLM Loss**: < 0.5 (ëª©í‘œ)
- **í•™ìŠµ ì•ˆì •ì„±**: Loss ê·¸ë˜í”„ ìˆ˜ë ´
- **ì´ìƒ íƒì§€ ì •í™•ë„**: ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€

---

## ğŸ“‹ í•™ìŠµ ë‹¨ê³„ë³„ ê³„íš

### Phase 1: í™˜ê²½ ì¤€ë¹„ (1ì¼)

#### 1.1 ì‹¤í–‰ í™˜ê²½ í™•ì¸

```bash
# CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU Count: {torch.cuda.device_count()}')"
python -c "import torch; print(f'GPU Name: {torch.cuda.get_device_name(0)}')"
```

#### 1.2 ë©”ëª¨ë¦¬ ì¶”ì •

**ë°ì´í„° í¬ê¸°**: 137 GB (ì „ì²´)  
**ë°°ì¹˜ í¬ê¸°**: 32  
**ì˜ˆìƒ GPU ë©”ëª¨ë¦¬**: ì•½ 8-12 GB (ëª¨ë¸ + ë°°ì¹˜ ë°ì´í„°)

**ê¶Œì¥ ì‚¬ì–‘**:
- GPU: NVIDIA V100/A100 (16GB+ VRAM)
- RAM: 32GB ì´ìƒ
- Storage: SSD ê¶Œì¥ (ë¹ ë¥¸ I/O)

#### 1.3 ì˜ì¡´ì„± í™•ì¸

```bash
cd logbert_training
pip install -r requirements.txt --upgrade
```

**ì£¼ìš” íŒ¨í‚¤ì§€**:
- `torch >= 1.10.0`
- `transformers >= 4.0.0`
- `PyYAML`
- `numpy`
- `tqdm`

---

### Phase 2: ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦ (1-2ì¼)

#### 2.1 ë°ì´í„° ìƒ˜í”Œë§ ì „ëµ

**ì „ì²´ ë°ì´í„° (137GB)ë¥¼ í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ, ë‹¨ê³„ì  ì ‘ê·¼:**

```mermaid
graph LR
    A[ì „ì²´ ë°ì´í„°<br/>324ê°œ íŒŒì¼] --> B{ìƒ˜í”Œë§ ì „ëµ}
    B --> C[ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸<br/>10ê°œ íŒŒì¼]
    B --> D[ì¤‘ê·œëª¨ í•™ìŠµ<br/>100ê°œ íŒŒì¼]
    B --> E[ì „ì²´ í•™ìŠµ<br/>324ê°œ íŒŒì¼]
    
    C --> F[ì„¤ì • ìµœì í™”]
    D --> G[ì„±ëŠ¥ ê²€ì¦]
    E --> H[ìµœì¢… ëª¨ë¸]
```

**ê¶Œì¥ í•™ìŠµ ë‹¨ê³„**:

1. **í…ŒìŠ¤íŠ¸ í•™ìŠµ** (2-3ì‹œê°„):
   - íŒŒì¼: 10ê°œ (ìµœê·¼ 10ì¼)
   - ëª©ì : ì„¤ì • ê²€ì¦, ë©”ëª¨ë¦¬ í™•ì¸
   - ë°ì´í„°: ì•½ 4-5 GB

2. **ì¤‘ê·œëª¨ í•™ìŠµ** (1-2ì¼):
   - íŒŒì¼: 100ê°œ (ìµœê·¼ 3ê°œì›”)
   - ëª©ì : ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
   - ë°ì´í„°: ì•½ 40-50 GB

3. **ì „ì²´ í•™ìŠµ** (3-5ì¼):
   - íŒŒì¼: 324ê°œ (ì „ì²´)
   - ëª©ì : ìµœì¢… ëª¨ë¸ ìƒì„±
   - ë°ì´í„°: 137 GB

#### 2.2 ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

```python
# data_validator.py
import json
import os
from pathlib import Path
from collections import Counter

def validate_data():
    """ì „ì²˜ë¦¬ ë°ì´í„° ê²€ì¦"""
    output_dir = Path("../output")
    files = sorted(output_dir.glob("preprocessed_logs_*.json"))
    
    print(f"ì´ íŒŒì¼ ìˆ˜: {len(files)}")
    
    # ìƒ˜í”Œ íŒŒì¼ ê²€ì¦
    sample_file = files[0]
    print(f"\nìƒ˜í”Œ íŒŒì¼: {sample_file.name}")
    
    with open(sample_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ì„¸ì…˜ ìˆ˜: {len(data)}")
    
    # ì²« ì„¸ì…˜ êµ¬ì¡° í™•ì¸
    if data:
        session = data[0]
        print("\nì²« ì„¸ì…˜ êµ¬ì¡°:")
        for key in session.keys():
            print(f"  - {key}: {type(session[key]).__name__}")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬
        seq_lengths = [len(s.get('event_sequence', [])) for s in data]
        print(f"\nì‹œí€€ìŠ¤ ê¸¸ì´:")
        print(f"  - í‰ê· : {sum(seq_lengths)/len(seq_lengths):.1f}")
        print(f"  - ìµœì†Œ: {min(seq_lengths)}")
        print(f"  - ìµœëŒ€: {max(seq_lengths)}")
        
        # ì„œë¹„ìŠ¤ ë¶„í¬
        services = [s.get('service_name', 'unknown') for s in data]
        service_counts = Counter(services)
        print(f"\nì„œë¹„ìŠ¤ ë¶„í¬:")
        for service, count in service_counts.most_common():
            print(f"  - {service}: {count}")

if __name__ == '__main__':
    validate_data()
```

**ì‹¤í–‰**:
```bash
cd logbert_training
python data_validator.py
```

#### 2.3 í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬

```python
# split_data.py
import json
import random
from pathlib import Path

def split_train_val(train_ratio=0.9):
    """í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬"""
    output_dir = Path("../output")
    files = sorted(output_dir.glob("preprocessed_logs_*.json"))
    
    # íŒŒì¼ ì…”í”Œ ë° ë¶„ë¦¬
    random.shuffle(files)
    split_idx = int(len(files) * train_ratio)
    
    train_files = files[:split_idx]
    val_files = files[split_idx:]
    
    print(f"í•™ìŠµ íŒŒì¼: {len(train_files)}")
    print(f"ê²€ì¦ íŒŒì¼: {len(val_files)}")
    
    # íŒŒì¼ ëª©ë¡ ì €ì¥
    with open('train_files.txt', 'w') as f:
        for file in train_files:
            f.write(f"{file}\n")
    
    with open('val_files.txt', 'w') as f:
        for file in val_files:
            f.write(f"{file}\n")

if __name__ == '__main__':
    split_train_val()
```

---

### Phase 3: í…ŒìŠ¤íŠ¸ í•™ìŠµ (1ì¼)

#### 3.1 ì„¤ì • íŒŒì¼ ìˆ˜ì •

**íŒŒì¼**: `training_config_test.yaml`

```yaml
# LogBERT í…ŒìŠ¤íŠ¸ í•™ìŠµ ì„¤ì •

# ëª¨ë¸ ì„¤ì •
model:
  vocab_size: 10000          # Event ID + Special Tokens
  hidden_size: 768           # BERT-base í¬ê¸°
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

# í•™ìŠµ ì„¤ì •
training:
  batch_size: 32             # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
  learning_rate: 0.00002     # 2e-5
  weight_decay: 0.01
  num_epochs: 3              # í…ŒìŠ¤íŠ¸: 3 ì—í­
  total_steps: 10000
  min_lr: 0.000001           # 1e-6
  max_grad_norm: 1.0
  mask_prob: 0.15            # MLM ë§ˆìŠ¤í‚¹ ë¹„ìœ¨
  log_interval: 50           # ìì£¼ ë¡œê·¸ ì¶œë ¥
  save_interval: 500         # ìì£¼ ì €ì¥
  num_workers: 4

# ë°ì´í„° ì„¤ì •
data:
  preprocessed_dir: "../output"
  max_seq_length: 512
  # í…ŒìŠ¤íŠ¸ìš©: ìµœê·¼ 10ê°œ íŒŒì¼ë§Œ ì‚¬ìš©
  limit_files: 10

# ì¶œë ¥ ì„¤ì •
output_dir: "checkpoints_test"
```

#### 3.2 í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹¤í–‰

```bash
cd logbert_training

# í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹œì‘
python train.py --config training_config_test.yaml
```

#### 3.3 ì˜ˆìƒ ê²°ê³¼

**í•™ìŠµ ì‹œê°„**: 2-3ì‹œê°„ (10ê°œ íŒŒì¼, 3 ì—í­)  
**ì²´í¬í¬ì¸íŠ¸**:
- `checkpoints_test/checkpoint_step_500.pt`
- `checkpoints_test/checkpoint_step_1000.pt`
- `checkpoints_test/best_model.pt`
- `checkpoints_test/epoch_3.pt`

**ëª¨ë‹ˆí„°ë§ í•­ëª©**:
```
Epoch 1/3, Step 50/3333
  Loss: 4.2345
  Avg Loss: 4.3210
  LR: 0.00002
  GPU Memory: 8.5 GB / 16.0 GB
```

#### 3.4 ë¬¸ì œ í•´ê²°

**CUDA Out of Memory ë°œìƒ ì‹œ**:
```yaml
training:
  batch_size: 16  # ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
```

**í•™ìŠµì´ ë„ˆë¬´ ëŠë¦´ ì‹œ**:
```yaml
training:
  num_workers: 8  # ì›Œì»¤ ìˆ˜ ì¦ê°€
```

---

### Phase 4: ì¤‘ê·œëª¨ í•™ìŠµ (2-3ì¼)

#### 4.1 ì„¤ì • íŒŒì¼

**íŒŒì¼**: `training_config_medium.yaml`

```yaml
# LogBERT ì¤‘ê·œëª¨ í•™ìŠµ ì„¤ì •

model:
  vocab_size: 10000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

training:
  batch_size: 32           # ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
  learning_rate: 0.00002
  weight_decay: 0.01
  num_epochs: 10           # ì¤‘ê·œëª¨: 10 ì—í­
  total_steps: 100000
  min_lr: 0.000001
  max_grad_norm: 1.0
  mask_prob: 0.15
  log_interval: 100
  save_interval: 1000
  num_workers: 8           # ì›Œì»¤ ì¦ê°€

data:
  preprocessed_dir: "../output"
  max_seq_length: 512
  limit_files: 100         # ìµœê·¼ 100ê°œ íŒŒì¼

output_dir: "checkpoints_medium"
```

#### 4.2 í•™ìŠµ ì‹¤í–‰

```bash
# ë°±ê·¸ë¼ìš´ë“œë¡œ í•™ìŠµ ì‹¤í–‰ (ê¶Œì¥)
nohup python train.py --config training_config_medium.yaml > train_medium.log 2>&1 &

# ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f train_medium.log
```

#### 4.3 í•™ìŠµ ê³¡ì„  ë¶„ì„

```bash
# í•™ìŠµ ê³¡ì„  í”Œë¡¯
python plot_training_curve.py \
  --checkpoint-dir checkpoints_medium \
  --output training_curve_medium.png
```

**í™•ì¸ ì‚¬í•­**:
- Lossê°€ ìˆ˜ë ´í•˜ëŠ”ê°€?
- Overfitting ì§•í›„ëŠ” ì—†ëŠ”ê°€?
- Learning rate scheduleì´ ì ì ˆí•œê°€?

---

### Phase 5: ì „ì²´ í•™ìŠµ (3-5ì¼)

#### 5.1 ìµœì¢… ì„¤ì • íŒŒì¼

**íŒŒì¼**: `training_config_full.yaml`

```yaml
# LogBERT ì „ì²´ í•™ìŠµ ì„¤ì • (ìµœì¢…)

model:
  vocab_size: 10000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

training:
  batch_size: 32           # ì•ˆì •ì ì¸ ë°°ì¹˜ í¬ê¸°
  learning_rate: 0.00002
  weight_decay: 0.01
  num_epochs: 20           # ì „ì²´: 20 ì—í­
  total_steps: 500000      # ì¦ê°€
  min_lr: 0.000001
  max_grad_norm: 1.0
  mask_prob: 0.15
  log_interval: 100
  save_interval: 5000      # 5000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
  num_workers: 8

data:
  preprocessed_dir: "../output"
  max_seq_length: 512
  # limit_files ì œê±° -> ì „ì²´ íŒŒì¼ ì‚¬ìš©

output_dir: "checkpoints_full"
```

#### 5.2 í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `run_full_training.sh`

```bash
#!/bin/bash

# ì „ì²´ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

echo "=== LogBERT ì „ì²´ í•™ìŠµ ì‹œì‘ ==="
date

# GPU í™•ì¸
nvidia-smi

# í•™ìŠµ ì‹œì‘
python train.py \
  --config training_config_full.yaml \
  > train_full.log 2>&1

echo "=== í•™ìŠµ ì™„ë£Œ ==="
date
```

**ì‹¤í–‰**:
```bash
chmod +x run_full_training.sh
nohup ./run_full_training.sh &
```

#### 5.3 í•™ìŠµ ëª¨ë‹ˆí„°ë§

**ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**:
```bash
# ë¡œê·¸ í™•ì¸
tail -f train_full.log

# GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë””ìŠ¤í¬ I/O í™•ì¸ (Linux)
iostat -x 5
```

**ì²´í¬í¬ì¸íŠ¸ í¬ê¸° í™•ì¸**:
```bash
du -h checkpoints_full/
```

---

### Phase 6: ëª¨ë¸ í‰ê°€ (1ì¼)

#### 6.1 ì´ìƒ íƒì§€ ì¶”ë¡ 

```bash
# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì¶”ë¡ 
python detect_anomalies.py \
  --checkpoint checkpoints_full/best_model.pt \
  --input ../output/preprocessed_logs_2026-01-15.json \
  --output anomaly_results.json \
  --threshold 2.0
```

#### 6.2 ê²°ê³¼ ë¶„ì„

**íŒŒì¼**: `analyze_results.py`

```python
import json
from collections import Counter

def analyze_anomaly_results(results_file):
    """ì´ìƒ íƒì§€ ê²°ê³¼ ë¶„ì„"""
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    # ì´ìƒ ì ìˆ˜ ë¶„í¬
    scores = [r['anomaly_score'] for r in results]
    print(f"ì´ìƒ ì ìˆ˜ í†µê³„:")
    print(f"  - í‰ê· : {sum(scores)/len(scores):.2f}")
    print(f"  - ìµœì†Œ: {min(scores):.2f}")
    print(f"  - ìµœëŒ€: {max(scores):.2f}")
    
    # ì´ìƒ íƒì§€ ë¹„ìœ¨
    anomalies = [r for r in results if r['is_anomaly']]
    print(f"\nì´ìƒ íƒì§€:")
    print(f"  - ì „ì²´ ì„¸ì…˜: {len(results)}")
    print(f"  - ì´ìƒ ì„¸ì…˜: {len(anomalies)}")
    print(f"  - ì´ìƒ ë¹„ìœ¨: {len(anomalies)/len(results)*100:.1f}%")
    
    # ì„œë¹„ìŠ¤ë³„ ë¶„í¬
    service_anomalies = Counter(r['service_name'] for r in anomalies)
    print(f"\nì„œë¹„ìŠ¤ë³„ ì´ìƒ íƒì§€:")
    for service, count in service_anomalies.most_common():
        print(f"  - {service}: {count}")

if __name__ == '__main__':
    analyze_anomaly_results('anomaly_results.json')
```

#### 6.3 ì„±ëŠ¥ ì§€í‘œ

**í‰ê°€ ì§€í‘œ**:
- **Precision**: ì •ë°€ë„
- **Recall**: ì¬í˜„ìœ¨
- **F1-Score**: ì¡°í™” í‰ê· 
- **AUC-ROC**: ROC ê³¡ì„  ì•„ë˜ ë©´ì 

**ì‹œê°í™”**:
```bash
# ROC ê³¡ì„  í”Œë¡¯
python plot_roc_curve.py \
  --results anomaly_results.json \
  --output roc_curve.png
```

---

## ğŸ“Š ì˜ˆìƒ ì¼ì • ë° ë¦¬ì†ŒìŠ¤

### ì¼ì • (ì´ 9-14ì¼)

```mermaid
gantt
    title LogBERT í•™ìŠµ ì¼ì •
    dateFormat  YYYY-MM-DD
    section ì¤€ë¹„
    í™˜ê²½ ì¤€ë¹„           :a1, 2026-02-01, 1d
    ë°ì´í„° ê²€ì¦         :a2, after a1, 2d
    section í•™ìŠµ
    í…ŒìŠ¤íŠ¸ í•™ìŠµ         :b1, after a2, 1d
    ì¤‘ê·œëª¨ í•™ìŠµ         :b2, after b1, 3d
    ì „ì²´ í•™ìŠµ           :b3, after b2, 5d
    section í‰ê°€
    ëª¨ë¸ í‰ê°€           :c1, after b3, 1d
    ê²°ê³¼ ë¶„ì„           :c2, after c1, 1d
```

### ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤

| ë‹¨ê³„ | GPU ì‚¬ìš© | ì‹œê°„ | ë°ì´í„° í¬ê¸° |
|------|----------|------|-------------|
| í…ŒìŠ¤íŠ¸ í•™ìŠµ | 1x GPU (8-12 GB VRAM) | 2-3ì‹œê°„ | 4-5 GB |
| ì¤‘ê·œëª¨ í•™ìŠµ | 1x GPU (12-16 GB VRAM) | 1-2ì¼ | 40-50 GB |
| ì „ì²´ í•™ìŠµ | 1x GPU (16-32 GB VRAM) | 3-5ì¼ | 137 GB |

**ê¶Œì¥ GPU**:
- NVIDIA V100 (16GB)
- NVIDIA A100 (40GB) - ìµœì 
- RTX 3090 (24GB)
- RTX 4090 (24GB)

---

## ğŸ”§ ì£¼ìš” íŒŒì¼ ë° ìŠ¤í¬ë¦½íŠ¸

### ìƒì„± í•„ìš” íŒŒì¼

```
logbert_training/
â”œâ”€â”€ training_config_test.yaml      # í…ŒìŠ¤íŠ¸ í•™ìŠµ ì„¤ì •
â”œâ”€â”€ training_config_medium.yaml    # ì¤‘ê·œëª¨ í•™ìŠµ ì„¤ì •
â”œâ”€â”€ training_config_full.yaml      # ì „ì²´ í•™ìŠµ ì„¤ì •
â”œâ”€â”€ data_validator.py              # ë°ì´í„° ê²€ì¦
â”œâ”€â”€ split_data.py                  # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
â”œâ”€â”€ run_full_training.sh           # ì „ì²´ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ analyze_results.py             # ê²°ê³¼ ë¶„ì„
```

### ê¸°ì¡´ íŒŒì¼ í™œìš©

```
logbert_training/
â”œâ”€â”€ train.py                       # âœ… ì´ë¯¸ ì¡´ì¬
â”œâ”€â”€ model.py                       # âœ… ì´ë¯¸ ì¡´ì¬
â”œâ”€â”€ dataset.py                     # âœ… ì´ë¯¸ ì¡´ì¬
â”œâ”€â”€ detect_anomalies.py            # âœ… ì´ë¯¸ ì¡´ì¬
â”œâ”€â”€ plot_training_curve.py         # âœ… ì´ë¯¸ ì¡´ì¬
â””â”€â”€ requirements.txt               # âœ… ì´ë¯¸ ì¡´ì¬
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: í™˜ê²½ ì¤€ë¹„ âœ…
- [ ] CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
- [ ] GPU ë©”ëª¨ë¦¬ í™•ì¸
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜
- [ ] ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 200GB)

### Phase 2: ë°ì´í„° ì¤€ë¹„ âœ…
- [ ] ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
- [ ] ìƒ˜í”Œ ë°ì´í„° í™•ì¸

### Phase 3: í…ŒìŠ¤íŠ¸ í•™ìŠµ âœ…
- [ ] í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹¤í–‰
- [ ] ì²´í¬í¬ì¸íŠ¸ ìƒì„± í™•ì¸
- [ ] Loss ìˆ˜ë ´ í™•ì¸

### Phase 4: ì¤‘ê·œëª¨ í•™ìŠµ âœ…
- [ ] ì¤‘ê·œëª¨ ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] ì¤‘ê·œëª¨ í•™ìŠµ ì‹¤í–‰
- [ ] í•™ìŠµ ê³¡ì„  ë¶„ì„
- [ ] ì„±ëŠ¥ ê²€ì¦

### Phase 5: ì „ì²´ í•™ìŠµ âœ…
- [ ] ìµœì¢… ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] ì „ì²´ í•™ìŠµ ì‹¤í–‰
- [ ] ì •ê¸° ì²´í¬í¬ì¸íŠ¸ í™•ì¸
- [ ] ìµœì¢… ëª¨ë¸ í‰ê°€

### Phase 6: ëª¨ë¸ í‰ê°€ âœ…
- [ ] ì´ìƒ íƒì§€ ì¶”ë¡  ì‹¤í–‰
- [ ] ê²°ê³¼ ë¶„ì„
- [ ] ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
- [ ] ì‹œê°í™”

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ê´€ë¦¬

**ë¬¸ì œ**: `CUDA Out of Memory`  
**í•´ê²°**:
```yaml
training:
  batch_size: 16  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ
  num_workers: 4  # ì›Œì»¤ ìˆ˜ ê°ì†Œ
```

### 2. ë””ìŠ¤í¬ ê³µê°„

**ë¬¸ì œ**: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±  
**í•´ê²°**:
- ì²´í¬í¬ì¸íŠ¸ ì •ê¸° ì‚­ì œ
- `save_interval` ì¦ê°€
- ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì œê±°

### 3. í•™ìŠµ ì†ë„

**ë¬¸ì œ**: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼  
**í•´ê²°**:
```yaml
training:
  batch_size: 64    # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (GPU ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
  num_workers: 12   # ì›Œì»¤ ìˆ˜ ì¦ê°€
```

### 4. Lossê°€ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ

**ë¬¸ì œ**: Lossê°€ ê³„ì† ë†’ìŒ  
**í•´ê²°**:
```yaml
training:
  learning_rate: 0.00001  # í•™ìŠµë¥  ê°ì†Œ
  num_epochs: 30          # ì—í­ ìˆ˜ ì¦ê°€
  weight_decay: 0.05      # ì •ê·œí™” ê°•í™”
```

### 5. í•™ìŠµ ì¤‘ë‹¨ ë° ì¬ê°œ

**ì¤‘ë‹¨ ì‹œ ì²´í¬í¬ì¸íŠ¸ í™œìš©**:
```bash
# ìµœì‹  ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
python train.py \
  --config training_config_full.yaml \
  --resume checkpoints_full/checkpoint_step_25000.pt
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ì •ëŸ‰ì  ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ ê°’ |
|------|---------|
| **MLM Loss (í•™ìŠµ)** | < 0.5 |
| **MLM Loss (ê²€ì¦)** | < 0.6 |
| **ì´ìƒ íƒì§€ Precision** | > 0.8 |
| **ì´ìƒ íƒì§€ Recall** | > 0.7 |
| **F1-Score** | > 0.75 |

### ì •ì„±ì  ì§€í‘œ

- âœ… Loss ê·¸ë˜í”„ê°€ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´
- âœ… ê²€ì¦ Lossì™€ í•™ìŠµ Loss ì°¨ì´ < 0.2 (Overfitting ë°©ì§€)
- âœ… ì„œë¹„ìŠ¤ë³„ ì´ìƒ íƒì§€ê°€ ê· í˜•ìˆê²Œ ì‘ë™
- âœ… ì‹¤ì œ ì—ëŸ¬ ë¡œê·¸ë¥¼ ë†’ì€ í™•ë¥ ë¡œ íƒì§€

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

### LogBERT í•™ìŠµ ì™„ë£Œ í›„

1. **ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶•**
   - DeepLog, LogLSTM, LogTCN ëª¨ë¸ í•™ìŠµ
   - 4ê°œ ëª¨ë¸ í†µí•©

2. **ì¹˜ëª…ë„ ê³„ì‚° ëª¨ë“ˆ**
   - ì´ìƒ ì ìˆ˜ â†’ ì¹˜ëª…ë„ ë³€í™˜
   - ì„œë¹„ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©

3. **ì†ŒìŠ¤ ì½”ë“œ RAG**
   - ì½”ë“œ íŒŒì‹± ë° ë²¡í„°í™”
   - ë¡œê·¸-ì½”ë“œ ì—°ê²°

4. **API ì„œë²„ êµ¬ì¶•**
   - FastAPI êµ¬í˜„
   - ì—”ë“œí¬ì¸íŠ¸ ê°œë°œ

---

## ğŸ’¡ íŒ ë° ê¶Œì¥ì‚¬í•­

### 1. ì ì§„ì  ì ‘ê·¼
- ì‘ì€ ë°ì´í„°ë¡œ ì‹œì‘ â†’ ì ì§„ì  í™•ëŒ€
- ì„¤ì • ìµœì í™” í›„ ì „ì²´ í•™ìŠµ

### 2. ì •ê¸° ë°±ì—…
```bash
# ì²´í¬í¬ì¸íŠ¸ ë°±ì—…
rsync -av checkpoints_full/ backup/checkpoints_full_$(date +%Y%m%d)/
```

### 3. ì‹¤í—˜ ì¶”ì 
- í•™ìŠµ ë¡œê·¸ ë³´ê´€
- ì„¤ì • íŒŒì¼ ë²„ì „ ê´€ë¦¬
- ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡

### 4. GPU íš¨ìœ¨ì„±
- Mixed Precision (FP16) ì‚¬ìš© ê³ ë ¤
- Gradient Accumulation í™œìš©
- Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)

### 5. ëª¨ë‹ˆí„°ë§ ë„êµ¬
- TensorBoard í™œìš©
- Weights & Biases (wandb) ê³ ë ¤
- ì‹¤ì‹œê°„ ì•Œë¦¼ ì„¤ì •

---

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

**ë¬¸ì œ ë°œìƒ ì‹œ**:
1. ë¡œê·¸ íŒŒì¼ í™•ì¸ (`train_*.log`)
2. GPU ë©”ëª¨ë¦¬ í™•ì¸ (`nvidia-smi`)
3. ì„¤ì • íŒŒì¼ ê²€ì¦
4. ì²´í¬í¬ì¸íŠ¸ ë³µêµ¬

**ì¶”ê°€ ë¬¸ì„œ**:
- [LogBERT README](file:///c:/workspace/RADAR/logbert_training/README.md)
- [QUICK_START](file:///c:/workspace/RADAR/logbert_training/QUICK_START.md)
- [TRAINING_DETAILS](file:///c:/workspace/RADAR/logbert_training/TRAINING_DETAILS.md)

---

**ì‘ì„±ì¼**: 2026-02-01  
**ì˜ˆìƒ ì™„ë£Œì¼**: 2026-02-14  
**í”„ë¡œì íŠ¸**: RADAR - MSA ë¡œê·¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ
