# LogBERT 전체 학습 설정 (324개 파일)
# 환경: GPU 서버 (NVIDIA GPU)
# 목적: 최종 모델 학습
# 데이터: 전체 324개 파일
# 예상 시간: 3-5일 (RTX 4090 기준)

# 모델 설정
model:
  vocab_size: 10000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

# 학습 설정 (GPU 서버 최적화)
training:
  batch_size: 64             # GPU 메모리에 따라 조정 (32-128)
  learning_rate: 0.00002     # 2e-5
  weight_decay: 0.01
  num_epochs: 3              # 전체 학습
  total_steps: 100000
  min_lr: 0.000001
  max_grad_norm: 1.0
  mask_prob: 0.15
  log_interval: 100          # 로그 주기
  save_interval: 5000        # 체크포인트 저장 주기
  num_workers: 8             # GPU 서버 CPU 코어 수에 따라 조정

# 데이터 설정
data:
  preprocessed_dir: "./output"  # GPU 서버에서 데이터 경로
  max_seq_length: 512
  limit_files: null          # null = 전체 파일 사용

# 출력 설정
output_dir: "./checkpoints_full"
