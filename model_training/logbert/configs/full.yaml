# LogBERT 전체 학습 설정 (324개 파일)
# 환경: GPU 서버 (V100/A100 권장)
# 목적: 최종 모델 생성
# 데이터: 전체 324개 파일 (약 141GB)
# 예상 시간: 3-5일

# 모델 설정
model:
  vocab_size: 10000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1

# 학습 설정 (GPU 최적화)
training:
  batch_size: 32             # GPU 환경
  learning_rate: 0.00002
  weight_decay: 0.01
  num_epochs: 20             # 전체 학습
  total_steps: 500000
  min_lr: 0.000001
  max_grad_norm: 1.0
  mask_prob: 0.15
  log_interval: 100
  save_interval: 5000        # 큰 간격
  num_workers: 8             # GPU 서버

# 데이터 설정
data:
  preprocessed_dir: "../../../output"
  max_seq_length: 512
  # limit_files 없음 -> 전체 324개 파일 사용

# 출력 설정
output_dir: "../checkpoints_full"
