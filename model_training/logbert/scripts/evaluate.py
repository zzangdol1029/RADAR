#!/usr/bin/env python3
"""
LogBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1-Score ê³„ì‚°

ì‚¬ìš©ë²•:
python scripts/evaluate.py \
    --checkpoint checkpoints_quick_xpu/checkpoints/best_model.pt \
    --config configs/test_quick_xpu.yaml \
    --validation-data ../output/preprocessed_logs_000.json \
    --normal-ratio 0.8
"""

import os
import sys
import json
import yaml
import torch
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆë“¤ì„ importí•˜ê¸° ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent.parent / 'logbert_training'))

from model import create_logbert_model
from dataset import LogBERTDataset

logger = logging.getLogger(__name__)


def setup_logging(log_file: Path = None):
    """ë¡œê¹… ì„¤ì • - UTF-8 ì¸ì½”ë”© ì§€ì›"""
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format))
    
    root_logger.addHandler(console_handler)
    
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter(log_format))
        root_logger.addHandler(file_handler)
        logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    return root_logger


def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ìˆ«ì ê°’ ë³€í™˜
    if 'model' in config:
        for key in config['model']:
            if isinstance(config['model'][key], str):
                try:
                    config['model'][key] = int(config['model'][key]) if '.' not in config['model'][key] else float(config['model'][key])
                except ValueError:
                    pass
    
    if 'data' in config:
        if 'max_seq_length' in config['data']:
            config['data']['max_seq_length'] = int(config['data']['max_seq_length'])
    
    return config


def load_model(checkpoint_path: str, config: Dict[str, Any], device: torch.device):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    logger.info(f"ëª¨ë¸ ë¡œë”© ì¤‘: {checkpoint_path}")
    
    # ëª¨ë¸ ìƒì„±
    model = create_logbert_model(config['model'])
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # state_dict ë¡œë“œ
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì •ë³´:")
        logger.info(f"  Global Step: {checkpoint.get('global_step', 'N/A')}")
        logger.info(f"  Best Loss: {checkpoint.get('best_loss', 'N/A'):.4f}")
    else:
        model.load_state_dict(checkpoint)
    
    model.to(device)
    model.eval()
    
    logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model


def calculate_anomaly_scores(
    model: torch.nn.Module,
    sessions: List[Dict],
    device: torch.device,
    max_seq_length: int,
    vocab_size: int
) -> List[float]:
    """ì„¸ì…˜ë“¤ì˜ ì´ìƒ ì ìˆ˜ ê³„ì‚°"""
    anomaly_scores = []
    
    with torch.no_grad():
        for session in sessions:
            # í† í° ì‹œí€€ìŠ¤ ì¤€ë¹„ (token_ids í•„ë“œ ì‚¬ìš©)
            tokens = session.get('token_ids', session.get('tokens', []))
            if len(tokens) == 0:
                continue
            
            # íŒ¨ë”©/ìë¥´ê¸°
            if len(tokens) > max_seq_length:
                tokens = tokens[:max_seq_length]
            
            # í…ì„œ ë³€í™˜
            input_ids = torch.tensor([tokens], dtype=torch.long).to(device)
            attention_mask = torch.ones_like(input_ids)
            labels = input_ids.clone()
            
            # Loss ê³„ì‚° (ì´ìƒ ì ìˆ˜)
            try:
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs['loss'].item()
                anomaly_scores.append(loss)
            except Exception as e:
                logger.warning(f"ì„¸ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    return anomaly_scores


def load_validation_data(data_file: str, normal_ratio: float = 0.8, max_samples: int = None) -> Tuple[List[Dict], List[Dict]]:
    """ê²€ì¦ ë°ì´í„° ë¡œë“œ ë° ì •ìƒ/ì´ìƒ ë¶„ë¦¬
    
    Args:
        data_file: ì „ì²˜ë¦¬ëœ JSON íŒŒì¼
        normal_ratio: ì •ìƒ ë°ì´í„° ë¹„ìœ¨ (0.8 = ì• 80%ë¥¼ ì •ìƒìœ¼ë¡œ ê°„ì£¼)
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©, ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ ì œí•œ ê°€ëŠ¥)
    
    Returns:
        (normal_sessions, anomaly_sessions)
    """
    logger.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì¤‘: {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ ê²½ìš°
    if isinstance(data, list):
        sessions = data
    else:
        sessions = data.get('sessions', [])
    
    total_sessions = len(sessions)
    
    # ìƒ˜í”Œë§ (ì§€ì •ëœ ê²½ìš°)
    if max_samples is not None and max_samples < total_sessions:
        import random
        random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
        sessions = random.sample(sessions, max_samples)
        logger.info(f"âš¡ ìƒ˜í”Œë§: {total_sessions}ê°œ â†’ {max_samples}ê°œ (ë¹ ë¥¸ í‰ê°€)")
    
    total_sessions = len(sessions)
    
    # ì •ìƒ/ì´ìƒ ë¶„ë¦¬ (ì•ë¶€ë¶„ì„ ì •ìƒ, ë’·ë¶€ë¶„ì„ ì´ìƒìœ¼ë¡œ ê°„ì£¼)
    split_idx = int(total_sessions * normal_ratio)
    normal_sessions = sessions[:split_idx]
    anomaly_sessions = sessions[split_idx:]
    
    logger.info(f"ì´ ì„¸ì…˜ ìˆ˜: {total_sessions}")
    logger.info(f"ì •ìƒ ì„¸ì…˜: {len(normal_sessions)} ({len(normal_sessions)/total_sessions*100:.1f}%)")
    logger.info(f"ì´ìƒ ì„¸ì…˜: {len(anomaly_sessions)} ({len(anomaly_sessions)/total_sessions*100:.1f}%)")
    
    return normal_sessions, anomaly_sessions


def calculate_metrics(
    normal_scores: List[float],
    anomaly_scores: List[float],
    threshold: float
) -> Dict[str, float]:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    # ë ˆì´ë¸” ìƒì„± (0: ì •ìƒ, 1: ì´ìƒ)
    y_true = [0] * len(normal_scores) + [1] * len(anomaly_scores)
    
    # ì˜ˆì¸¡ ìƒì„± (ì„ê³„ê°’ ê¸°ì¤€)
    y_pred = [int(score >= threshold) for score in normal_scores] + \
             [int(score >= threshold) for score in anomaly_scores]
    
    # ì ìˆ˜ (ROC AUCìš©)
    y_scores = normal_scores + anomaly_scores
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    try:
        roc_auc = roc_auc_score(y_true, y_scores)
    except ValueError:
        roc_auc = 0.0
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'confusion_matrix': cm,
        'true_negative': int(tn),
        'false_positive': int(fp),
        'false_negative': int(fn),
        'true_positive': int(tp),
        'y_true': y_true,
        'y_pred': y_pred,
        'y_scores': y_scores
    }


def find_optimal_threshold(
    normal_scores: List[float],
    anomaly_scores: List[float],
    num_thresholds: int = 100
) -> Tuple[float, Dict[str, float]]:
    """ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1-Score ìµœëŒ€í™”)"""
    min_score = min(min(normal_scores), min(anomaly_scores))
    max_score = max(max(normal_scores), max(anomaly_scores))
    
    thresholds = np.linspace(min_score, max_score, num_thresholds)
    best_threshold = None
    best_metrics = None
    best_f1 = 0.0
    
    for threshold in thresholds:
        metrics = calculate_metrics(normal_scores, anomaly_scores, threshold)
        if metrics['f1_score'] > best_f1:
            best_f1 = metrics['f1_score']
            best_threshold = threshold
            best_metrics = metrics
    
    return best_threshold, best_metrics


def plot_score_distribution(
    normal_scores: List[float],
    anomaly_scores: List[float],
    threshold: float,
    output_path: Path
):
    """ì ìˆ˜ ë¶„í¬ ì‹œê°í™”"""
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.hist(normal_scores, bins=50, alpha=0.7, label='ì •ìƒ', color='blue', edgecolor='black')
    plt.hist(anomaly_scores, bins=50, alpha=0.7, label='ì´ìƒ', color='red', edgecolor='black')
    plt.axvline(threshold, color='green', linestyle='--', linewidth=2, label=f'ì„ê³„ê°’: {threshold:.4f}')
    plt.xlabel('ì´ìƒ ì ìˆ˜ (Loss)')
    plt.ylabel('ë¹ˆë„')
    plt.title('ì •ìƒ vs ì´ìƒ ì ìˆ˜ ë¶„í¬')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.boxplot([normal_scores, anomaly_scores], labels=['ì •ìƒ', 'ì´ìƒ'])
    plt.axhline(threshold, color='green', linestyle='--', linewidth=2, label=f'ì„ê³„ê°’: {threshold:.4f}')
    plt.ylabel('ì´ìƒ ì ìˆ˜ (Loss)')
    plt.title('ì •ìƒ vs ì´ìƒ ì ìˆ˜ ë°•ìŠ¤í”Œë¡¯')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    logger.info(f"ğŸ“Š ì ìˆ˜ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: {output_path}")
    plt.close()


def plot_confusion_matrix(cm: np.ndarray, output_path: Path):
    """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['ì •ìƒ', 'ì´ìƒ'],
        yticklabels=['ì •ìƒ', 'ì´ìƒ'],
        cbar_kws={'label': 'ê°œìˆ˜'}
    )
    plt.xlabel('ì˜ˆì¸¡')
    plt.ylabel('ì‹¤ì œ')
    plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)')
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    logger.info(f"ğŸ“Š í˜¼ë™ í–‰ë ¬ ì €ì¥: {output_path}")
    plt.close()


def save_evaluation_results(
    results: Dict[str, Any],
    output_path: Path
):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    serializable_results = {}
    for key, value in results.items():
        if isinstance(value, np.ndarray):
            serializable_results[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_results[key] = float(value)
        else:
            serializable_results[key] = value
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='LogBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--config', type=str, required=True,
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--validation-data', type=str, required=True,
                       help='ê²€ì¦ ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--normal-ratio', type=float, default=0.8,
                       help='ì •ìƒ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.8)')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë¹ ë¥¸ í‰ê°€ìš©, ì˜ˆ: 1000)')
    parser.add_argument('--output-dir', type=str, default='evaluation_results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--log-file', type=str, default=None,
                       help='ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    if args.log_file:
        log_file = Path(args.log_file)
    else:
        script_dir = Path(__file__).parent
        logs_dir = script_dir.parent / 'logs'
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = logs_dir / f'evaluation_{timestamp}.log'
    
    setup_logging(log_file)
    
    logger.info("=" * 80)
    logger.info("LogBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    logger.info("=" * 80)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_model(args.checkpoint, config, device)
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì˜µì…˜ í¬í•¨)
    normal_sessions, anomaly_sessions = load_validation_data(
        args.validation_data,
        args.normal_ratio,
        args.max_samples  # ì¶”ê°€
    )
    
    # ì´ìƒ ì ìˆ˜ ê³„ì‚°
    logger.info("\n" + "=" * 80)
    logger.info("ì´ìƒ ì ìˆ˜ ê³„ì‚° ì¤‘...")
    logger.info("=" * 80)
    
    max_seq_length = config['data']['max_seq_length']
    vocab_size = config['model']['vocab_size']
    
    logger.info("ì •ìƒ ì„¸ì…˜ í‰ê°€ ì¤‘...")
    normal_scores = calculate_anomaly_scores(
        model, normal_sessions, device, max_seq_length, vocab_size
    )
    
    logger.info("ì´ìƒ ì„¸ì…˜ í‰ê°€ ì¤‘...")
    anomaly_scores = calculate_anomaly_scores(
        model, anomaly_sessions, device, max_seq_length, vocab_size
    )
    
    logger.info(f"âœ… ì •ìƒ ì„¸ì…˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(normal_scores)}ê°œ")
    logger.info(f"âœ… ì´ìƒ ì„¸ì…˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(anomaly_scores)}ê°œ")
    
    # ì ìˆ˜ í†µê³„
    logger.info("\n" + "=" * 80)
    logger.info("ì ìˆ˜ í†µê³„")
    logger.info("=" * 80)
    logger.info(f"ì •ìƒ ì„¸ì…˜ - í‰ê· : {np.mean(normal_scores):.4f}, í‘œì¤€í¸ì°¨: {np.std(normal_scores):.4f}")
    logger.info(f"ì •ìƒ ì„¸ì…˜ - ìµœì†Œ: {np.min(normal_scores):.4f}, ìµœëŒ€: {np.max(normal_scores):.4f}")
    logger.info(f"ì´ìƒ ì„¸ì…˜ - í‰ê· : {np.mean(anomaly_scores):.4f}, í‘œì¤€í¸ì°¨: {np.std(anomaly_scores):.4f}")
    logger.info(f"ì´ìƒ ì„¸ì…˜ - ìµœì†Œ: {np.min(anomaly_scores):.4f}, ìµœëŒ€: {np.max(anomaly_scores):.4f}")
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    logger.info("\n" + "=" * 80)
    logger.info("ìµœì  ì„ê³„ê°’ íƒìƒ‰ ì¤‘...")
    logger.info("=" * 80)
    
    best_threshold, best_metrics = find_optimal_threshold(
        normal_scores, anomaly_scores
    )
    
    logger.info(f"âœ… ìµœì  ì„ê³„ê°’: {best_threshold:.4f}")
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    logger.info("=" * 80)
    logger.info(f"ì •í™•ë„ (Accuracy):  {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)")
    logger.info(f"ì •ë°€ë„ (Precision): {best_metrics['precision']:.4f} ({best_metrics['precision']*100:.2f}%)")
    logger.info(f"ì¬í˜„ìœ¨ (Recall):    {best_metrics['recall']:.4f} ({best_metrics['recall']*100:.2f}%)")
    logger.info(f"F1-Score:          {best_metrics['f1_score']:.4f} ({best_metrics['f1_score']*100:.2f}%)")
    logger.info(f"ROC AUC:           {best_metrics['roc_auc']:.4f}")
    
    logger.info("\ní˜¼ë™ í–‰ë ¬:")
    logger.info(f"  True Negative (TN):  {best_metrics['true_negative']:4d} (ì •ìƒì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡)")
    logger.info(f"  False Positive (FP): {best_metrics['false_positive']:4d} (ì •ìƒì„ ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡)")
    logger.info(f"  False Negative (FN): {best_metrics['false_negative']:4d} (ì´ìƒì„ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡)")
    logger.info(f"  True Positive (TP):  {best_metrics['true_positive']:4d} (ì´ìƒì„ ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡)")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì‹œê°í™”
    logger.info("\n" + "=" * 80)
    logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
    logger.info("=" * 80)
    
    plot_score_distribution(
        normal_scores, anomaly_scores, best_threshold,
        output_dir / 'score_distribution.png'
    )
    
    plot_confusion_matrix(
        best_metrics['confusion_matrix'],
        output_dir / 'confusion_matrix.png'
    )
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'checkpoint': args.checkpoint,
        'validation_data': args.validation_data,
        'optimal_threshold': float(best_threshold),
        'metrics': {
            'accuracy': float(best_metrics['accuracy']),
            'precision': float(best_metrics['precision']),
            'recall': float(best_metrics['recall']),
            'f1_score': float(best_metrics['f1_score']),
            'roc_auc': float(best_metrics['roc_auc']),
        },
        'confusion_matrix': {
            'true_negative': best_metrics['true_negative'],
            'false_positive': best_metrics['false_positive'],
            'false_negative': best_metrics['false_negative'],
            'true_positive': best_metrics['true_positive'],
        },
        'statistics': {
            'normal_mean': float(np.mean(normal_scores)),
            'normal_std': float(np.std(normal_scores)),
            'normal_min': float(np.min(normal_scores)),
            'normal_max': float(np.max(normal_scores)),
            'anomaly_mean': float(np.mean(anomaly_scores)),
            'anomaly_std': float(np.std(anomaly_scores)),
            'anomaly_min': float(np.min(anomaly_scores)),
            'anomaly_max': float(np.max(anomaly_scores)),
        }
    }
    
    save_evaluation_results(results, output_dir / 'evaluation_results.json')
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… í‰ê°€ ì™„ë£Œ!")
    logger.info("=" * 80)
    logger.info(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == '__main__':
    main()
