#!/usr/bin/env python3
"""
LogBERT í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
CUDA GPU, Intel XPU, CPUë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import os
import sys
import yaml
import torch
import logging
from pathlib import Path
from typing import Dict, Any
from datetime import datetime

# Intel Extension for PyTorch (ì„ íƒì )
try:
    import intel_extension_for_pytorch as ipex
    IPEX_AVAILABLE = True
except ImportError:
    IPEX_AVAILABLE = False

# ë¡œì»¬ ëª¨ë“ˆ import
from model import create_logbert_model
from dataset import LogBERTDataset, create_dataloader, collate_fn

logger = logging.getLogger(__name__)


def setup_logging(log_file: Path = None):
    """ë¡œê¹… ì„¤ì • - UTF-8 ì¸ì½”ë”© ì§€ì›"""
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    # Windowsì—ì„œ ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format))
    root_logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter(log_format))
        root_logger.addHandler(file_handler)
        logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    return root_logger


def get_device():
    """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (XPU > CUDA > CPU)"""
    # Intel XPU í™•ì¸
    if IPEX_AVAILABLE and hasattr(torch, 'xpu') and torch.xpu.is_available():
        device = torch.device('xpu')
        logger.info(f"ğŸš€ Intel GPU ì‚¬ìš©: {torch.xpu.get_device_name(0)}")
        logger.info(f"   XPU ë””ë°”ì´ìŠ¤ ìˆ˜: {torch.xpu.device_count()}")
        return device, 'xpu'
    
    # NVIDIA CUDA í™•ì¸
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        logger.info(f"ğŸš€ NVIDIA GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
        logger.info(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        logger.info(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        return device, 'cuda'
    
    # CPU fallback
    else:
        device = torch.device('cpu')
        logger.warning("âš ï¸  CPU ëª¨ë“œ (GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)")
        return device, 'cpu'


class LogBERTTrainer:
    """LogBERT í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device, self.device_type = get_device()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = Path(config.get('output_dir', 'checkpoints'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model = create_logbert_model(config['model'])
        self.model.to(self.device)
        
        # Multi-GPU ì§€ì›
        self.use_multi_gpu = False
        if self.device_type == 'cuda' and torch.cuda.device_count() > 1:
            logger.info(f"ğŸ”§ Multi-GPU ì‚¬ìš©: {torch.cuda.device_count()}ê°œ GPU")
            self.model = torch.nn.DataParallel(self.model)
            self.use_multi_gpu = True

        # Mixed Precision (AMP) ì„¤ì •
        self.use_amp = config['training'].get('use_amp', True) and self.device_type == 'cuda'
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()
            logger.info("âœ… Mixed Precision (AMP) í™œì„±í™”")    
        
        # ì˜µí‹°ë§ˆì´ì €
        learning_rate = float(config['training']['learning_rate'])
        weight_decay = float(config['training'].get('weight_decay', 0.01))
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
        )
        
        # Intel GPU ìµœì í™” (IPEX)
        if self.device_type == 'xpu' and IPEX_AVAILABLE:
            logger.info("ğŸ”§ Intel GPU ìµœì í™” ì ìš© ì¤‘...")
            self.model, self.optimizer = ipex.optimize(
                self.model, 
                optimizer=self.optimizer,
                dtype=torch.float32
            )
            logger.info("âœ… IPEX ìµœì í™” ì™„ë£Œ!")
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        from torch.optim.lr_scheduler import CosineAnnealingLR
        total_steps = int(config['training'].get('total_steps', 100000))
        min_lr = float(config['training'].get('min_lr', 1e-6))
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=min_lr,
        )
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.best_loss = float('inf')
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
        self.checkpoint_dir = self.output_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    def train_epoch(self, dataloader, epoch: int) -> float:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        logger.info(f"ğŸ”„ [Epoch {epoch}] train_epoch í•¨ìˆ˜ ì§„ì… ì„±ê³µ")
        
        from tqdm import tqdm
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch}/{self.config['training']['num_epochs']}",
            total=len(dataloader),
            unit="batch",
            leave=True,
            ncols=100
        )
        
        logger.info(f"â³ [Epoch {epoch}] ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
        
        for i, batch in enumerate(progress_bar):
            if i == 0:
                logger.info(f"âœ… [Epoch {epoch}] ì²« ë²ˆì§¸ ë°°ì¹˜ ë¡œë“œ ì™„ë£Œ! GPU ì—°ì‚° ì‹œì‘")
        
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
            self.optimizer.zero_grad()
            
            # Mixed Precision (AMP) ì ìš© Forward pass
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                    )
                    loss = outputs['loss']
                    
                    # Multi-GPU ì‚¬ìš© ì‹œ ë²¡í„°ë¡œ ë°˜í™˜ëœ Lossë¥¼ ìŠ¤ì¹¼ë¼ë¡œ í‰ê· í™”
                    if self.use_multi_gpu:
                        loss = loss.mean()
                
                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (GradScaler í™œìš©)
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer) # Clipping ì „ unscale í•„ìˆ˜
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['training'].get('max_grad_norm', 1.0))
                self.scaler.step(self.optimizer)
                self.scaler.update()
            
            else:
                # ì¼ë°˜ ì •ë°€ë„ í•™ìŠµ (CPU/XPU/ê¸°ë³¸ CUDA í™˜ê²½)
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels,
                )
                loss = outputs['loss']

                if self.use_multi_gpu:
                    loss = loss.mean()

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['training'].get('max_grad_norm', 1.0))
                self.optimizer.step()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ ë° í†µê³„ ê¸°ë¡
            self.scheduler.step()
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ë° ë¡œê¹…
            if self.global_step % self.config['training'].get('log_interval', 100) == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                avg_loss_val = total_loss / num_batches

                # í™”ë©´ì— ë³´ì´ëŠ” tqdm ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg': f'{avg_loss_val:.4f}',
                    'lr': f'{current_lr:.2e}',
                })

                # íŒŒì¼ì— ê¸°ë¡ë˜ëŠ” ë¡œê±° ì—…ë°ì´íŠ¸ (ë‚˜ì¤‘ì— ë¶„ì„ìš©)
                logger.info(
                    f"[Step {self.global_step}] Loss={loss.item():.4f}, Avg={avg_loss_val:.4f}, LR={current_lr:.2e}"
                )

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.global_step % self.config['training'].get('save_interval', 5000) == 0:
                self.save_checkpoint(f'checkpoint_step_{self.global_step}')
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return avg_loss
    
    def save_checkpoint(self, name: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoint_dir / f'{name}.pt'
        
        # Multi-GPU ëª¨ë¸ ì²˜ë¦¬
        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
        
        checkpoint = {
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_loss': self.best_loss,
            'config': self.config,
            'device_type': self.device_type,
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def train(self, train_dataloader, num_epochs: int):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        logger.info("=" * 80)
        logger.info("ğŸš€ LogBERT í•™ìŠµ ì‹œì‘")
        logger.info("=" * 80)
        logger.info(f"ë””ë°”ì´ìŠ¤: {self.device} ({self.device_type.upper()})")
        logger.info(f"ì´ ì—í­: {num_epochs}")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.config['training']['batch_size']}")
        logger.info(f"í•™ìŠµë¥ : {self.config['training']['learning_rate']}")
        logger.info("=" * 80)
        
        for epoch in range(1, num_epochs + 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"ì—í­ {epoch}/{num_epochs} ì‹œì‘")
            logger.info(f"{'='*80}")
            
            avg_loss = self.train_epoch(train_dataloader, epoch)
            
            logger.info(f"\nì—í­ {epoch}/{num_epochs} ì™„ë£Œ")
            logger.info(f"  í‰ê·  Loss: {avg_loss:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_loss < self.best_loss:
                improvement = self.best_loss - avg_loss
                self.best_loss = avg_loss
                self.save_checkpoint('best_model')
                logger.info(f"  âœ… ìµœê³  ì„±ëŠ¥! (ê°œì„ : {improvement:.4f})")
            
            # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸
            self.save_checkpoint(f'epoch_{epoch}')
        
        logger.info("=" * 80)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ìµœê³  Loss: {self.best_loss:.4f}")
        logger.info("=" * 80)


def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ìˆ«ì ê°’ ë³€í™˜
    if 'training' in config:
        for key in ['learning_rate', 'weight_decay', 'min_lr', 'max_grad_norm', 'mask_prob']:
            if key in config['training']:
                config['training'][key] = float(config['training'][key])
        for key in ['batch_size', 'num_epochs', 'total_steps', 'log_interval', 'save_interval', 'num_workers']:
            if key in config['training']:
                config['training'][key] = int(config['training'][key])
    
    if 'model' in config:
        for key in config['model']:
            if isinstance(config['model'][key], (int, float)):
                continue
            try:
                if '.' in str(config['model'][key]):
                    config['model'][key] = float(config['model'][key])
                else:
                    config['model'][key] = int(config['model'][key])
            except (ValueError, TypeError):
                pass
    
    if 'data' in config:
        if 'max_seq_length' in config['data']:
            config['data']['max_seq_length'] = int(config['data']['max_seq_length'])
        if 'limit_files' in config['data'] and config['data']['limit_files'] is not None:
            config['data']['limit_files'] = int(config['data']['limit_files'])
    
    return config


def get_data_files(preprocessed_dir: str, limit_files: int = None) -> list:
    """ì „ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    script_dir = Path(__file__).parent
    
    if not Path(preprocessed_dir).is_absolute():
        data_dir = (script_dir / preprocessed_dir).resolve()
    else:
        data_dir = Path(preprocessed_dir)
    
    logger.info(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    if not data_dir.exists():
        raise FileNotFoundError(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    files = sorted(data_dir.glob("preprocessed_logs_*.json"))
    logger.info(f"ë°œê²¬ëœ ì „ì²´ ë°ì´í„° íŒŒì¼: {len(files)}ê°œ")
    
    if limit_files is not None and limit_files > 0:
        if len(files) > limit_files:
            files = files[-limit_files:]
            logger.info(f"âš™ï¸  limit_files: ìµœê·¼ {limit_files}ê°œ íŒŒì¼ë§Œ ì‚¬ìš©")
    
    logger.info(f"âœ… ì‚¬ìš©í•  íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    
    return [str(f) for f in files]


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LogBERT í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--config', type=str, required=True,
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì˜ˆ: configs/test_quick.yaml)')
    parser.add_argument('--data-dir', type=str, default=None,
                       help='ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: configs íŒŒì¼ ì„¤ì • ì‚¬ìš©)')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: configs íŒŒì¼ ì„¤ì • ì‚¬ìš©)')
    parser.add_argument('--log-file', type=str, default=None,
                       help='ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: logs/train_YYYYMMDD_HHMMSS.log)')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if args.log_file:
        log_file = Path(args.log_file)
    else:
        script_dir = Path(__file__).parent
        logs_dir = script_dir / 'logs'
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        config_name = Path(args.config).stem if args.config else 'train'
        log_file = logs_dir / f'train_{config_name}_{timestamp}.log'
    
    # ë¡œê¹… ì´ˆê¸°í™”
    setup_logging(log_file)
    
    logger.info("=" * 80)
    logger.info("LogBERT í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    logger.info("=" * 80)
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.data_dir:
        config['data']['preprocessed_dir'] = args.data_dir
    if args.output_dir:
        config['output_dir'] = args.output_dir
    
    # ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
    if 'output_dir' not in config:
        script_dir = Path(__file__).parent
        config['output_dir'] = str(script_dir / 'checkpoints')
    
    # ë°ì´í„° íŒŒì¼
    limit_files = config['data'].get('limit_files')
    data_files = get_data_files(
        config['data']['preprocessed_dir'],
        limit_files=limit_files
    )
    
    if len(data_files) == 0:
        logger.error("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    logger.info("\n" + "=" * 80)
    logger.info("ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    logger.info("=" * 80)
    
    dataset = LogBERTDataset(
        data_files=data_files,
        max_seq_length=config['data']['max_seq_length'],
        mask_prob=config['training'].get('mask_prob', 0.15),
        vocab_size=config['model']['vocab_size'],
    )
    
    logger.info(f"âœ… ì´ ì„¸ì…˜ ìˆ˜: {len(dataset):,}ê°œ")
    
    # DataLoader
    num_workers = config['training'].get('num_workers', 4)
    
    # ë””ë°”ì´ìŠ¤ì— ë”°ë¼ pin_memory ì„¤ì •
    _, device_type = get_device()
    pin_memory = (device_type == 'cuda')
    
    dataloader = create_dataloader(
        dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True,     # ì¼ê¾¼ ìœ ì§€
        prefetch_factor=4,           # ë°ì´í„° ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
        collate_fn=collate_fn
    )
    
    logger.info(f"âœ… DataLoader ìƒì„± ì™„ë£Œ (ë°°ì¹˜ ìˆ˜: {len(dataloader):,})")
    
    # í•™ìŠµê¸° ìƒì„±
    logger.info("\n" + "=" * 80)
    logger.info("í•™ìŠµê¸° ì´ˆê¸°í™”...")
    logger.info("=" * 80)
    
    trainer = LogBERTTrainer(config)
    
    # í•™ìŠµ ì‹œì‘
    trainer.train(
        train_dataloader=dataloader,
        num_epochs=config['training']['num_epochs'],
    )
    
    logger.info("\nâœ… ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == '__main__':
    main()
