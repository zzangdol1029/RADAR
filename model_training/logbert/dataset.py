import json
import torch
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from torch.utils.data import Dataset, DataLoader

logger = logging.getLogger(__name__)

class LogBERTDataset(Dataset):
    """
    LogBERT í•™ìŠµìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ìµœì¢… ìµœì í™” ë²„ì „)
    
    1. Lazy Loading: íŒŒì¼ ê²½ë¡œë§Œ ì¸ë±ì‹±í•˜ì—¬ RAM ì ìœ ìœ¨ 99% ì ˆê°
    2. Single File Caching: ë™ì¼ íŒŒì¼ ì ‘ê·¼ ì‹œ ì¬ë¡œë“œ ë°©ì§€ë¡œ I/O ì†ë„ ê·¹ëŒ€í™”
    """
    
    # BERT í‘œì¤€ íŠ¹ìˆ˜ í† í° ID ì •ì˜
    PAD_TOKEN_ID = 0
    CLS_TOKEN_ID = 101
    SEP_TOKEN_ID = 102
    MASK_TOKEN_ID = 103
    UNK_TOKEN_ID = 100
    
    def __init__(
        self,
        data_files: List[str],
        max_seq_length: int = 512,
        mask_prob: float = 0.15,
        random_mask_prob: float = 0.1,
        keep_mask_prob: float = 0.1,
        vocab_size: int = 10000,
    ):
        self.data_files = [str(f) for f in data_files]
        self.max_seq_length = max_seq_length
        self.mask_prob = mask_prob
        self.random_mask_prob = random_mask_prob
        self.keep_mask_prob = keep_mask_prob
        self.vocab_size = vocab_size
        
        # [ìºì‹± ìµœì í™”] í˜„ì¬ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ íŒŒì¼ ì •ë³´ë¥¼ ì €ì¥
        self.current_file_path = None
        self.current_data = None
        
        # ì‹¤ì œ ì„¸ì…˜ ë°ì´í„° ëŒ€ì‹  (íŒŒì¼_ì¸ë±ìŠ¤, ì„¸ì…˜_ì¸ë±ìŠ¤) ìœ„ì¹˜ ì§€ë„ ìƒì„±
        self.index_map = []
        self._build_index()
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(self.index_map):,}ê°œ ì„¸ì…˜ ì¸ë±ì‹±ë¨")

    def _build_index(self):
        """íŒŒì¼ë³„ ì„¸ì…˜ ê°œìˆ˜ë¥¼ íŒŒì•…í•˜ì—¬ ìœ„ì¹˜ ì§€ë„ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
        logger.info("ğŸ” ë°ì´í„° ìœ„ì¹˜ ì¸ë±ì‹± ì¤‘... (RAM ì ìœ  ë°©ì§€ ëª¨ë“œ)")
        
        random.shuffle(self.data_files)
        
        for file_idx, file_path in enumerate(self.data_files):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for session_idx in range(len(data)):
                            self.index_map.append((file_idx, session_idx))
                    del data # ë©”ëª¨ë¦¬ ì¦‰ì‹œ ë°˜í™˜
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")

    def __len__(self) -> int:
        return len(self.index_map)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """DataLoaderê°€ ìš”ì²­í•  ë•Œ ìºì‹œë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        file_idx, session_idx = self.index_map[idx]
        file_path = self.data_files[file_idx]
        
        try:
            # [ìˆ˜ì • í•µì‹¬] ìš”ì²­ íŒŒì¼ì´ í˜„ì¬ ìºì‹œëœ íŒŒì¼ê³¼ ë‹¤ë¥¼ ë•Œë§Œ ìƒˆë¡œ ë¡œë“œ
            if self.current_file_path != file_path:
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.current_data = json.load(f)
                self.current_file_path = file_path

            session = self.current_data[session_idx]
            
            # í† í° ë° ë§ˆìŠ¤í¬ ë³µì‚¬
            token_ids = list(session['token_ids'])
            attention_mask = list(session['attention_mask'])
            
            # 1. Truncation
            if len(token_ids) > self.max_seq_length:
                token_ids = token_ids[:self.max_seq_length]
                attention_mask = attention_mask[:self.max_seq_length]
            
            # 2. Padding
            seq_len = len(token_ids)
            if seq_len < self.max_seq_length:
                padding_len = self.max_seq_length - seq_len
                token_ids.extend([self.PAD_TOKEN_ID] * padding_len)
                attention_mask.extend([0] * padding_len)
            
            # 3. Tensor ë³€í™˜
            input_ids = torch.tensor(token_ids, dtype=torch.long)
            attention_mask = torch.tensor(attention_mask, dtype=torch.long)
            
            # 4. MLM ë ˆì´ë¸” ìƒì„±
            labels = input_ids.clone()
            masked_indices = self._create_masked_lm_predictions(input_ids, attention_mask)
            labels[~masked_indices] = -100
            
            return {
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'labels': labels,
            }
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ (Index {idx}): {e}")
            return self.__getitem__(0)

    def _create_masked_lm_predictions(self, input_ids, attention_mask):
        """BERT ìŠ¤íƒ€ì¼ ë§ˆìŠ¤í‚¹ ì „ëµ"""
        valid_positions = (attention_mask == 1) & \
                         (input_ids != self.CLS_TOKEN_ID) & \
                         (input_ids != self.SEP_TOKEN_ID)
        
        valid_indices = torch.where(valid_positions)[0]
        if len(valid_indices) == 0:
            return torch.zeros_like(input_ids, dtype=torch.bool)
            
        num_to_mask = max(1, int(len(valid_indices) * self.mask_prob))
        masked_indices = random.sample(valid_indices.tolist(), min(num_to_mask, len(valid_indices)))
        
        for idx in masked_indices:
            rand = random.random()
            if rand < 0.8: # [MASK]
                input_ids[idx] = self.MASK_TOKEN_ID
            elif rand < 0.9: # Random token
                input_ids[idx] = random.randint(1, self.vocab_size - 1)
        
        masked_mask = torch.zeros_like(input_ids, dtype=torch.bool)
        masked_mask[masked_indices] = True
        return masked_mask

def collate_fn(batch):
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch]),
    }

def create_dataloader(
    dataset: LogBERTDataset,
    batch_size: int = 32,
    shuffle: bool = False,
    num_workers: int = 4,
    pin_memory: bool = True,
    persistent_workers: bool = True, 
    prefetch_factor: int = 4,
    collate_fn: callable = None 
) -> DataLoader:
    """ëŒ€ê·œëª¨ ë°ì´í„° ë¡œë”© ìµœì í™” ë²„ì „"""
    
    if torch.backends.mps.is_available():
        pin_memory = False

    # num_workersê°€ 0ì¼ ë•ŒëŠ” persistent_workers ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
    if num_workers == 0:
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0,
            pin_memory=pin_memory,
            collate_fn=collate_fn
        )
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers, 
        prefetch_factor=prefetch_factor, 
        collate_fn=collate_fn
    )