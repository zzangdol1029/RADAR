# DeepLog ëª¨ë¸ í•™ìŠµ

LSTM ê¸°ë°˜ ë¡œê·¸ ì´ìƒ íƒì§€ ëª¨ë¸ **DeepLog**ì˜ í•™ìŠµ ëª¨ë“ˆì…ë‹ˆë‹¤.

> **ë…¼ë¬¸**: "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning" (CCS 2017)

## ğŸŒŸ ì£¼ìš” ê¸°ëŠ¥

### 1. Lazy Loading ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- **120GB ë°ì´í„°ë„ OOM ì—†ì´ ì²˜ë¦¬** ê°€ëŠ¥
- `ijson` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° JSON íŒŒì‹±
- ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë“œë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”

### 2. ë©€í‹° GPU ì§€ì›
- **DataParallel**ì„ í†µí•œ 4 GPU ë¶„ì‚° í•™ìŠµ
- Tesla V100-DGXS-32GB x 4 í™˜ê²½ ìµœì í™”

### 3. ìƒì„¸í•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§
- ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬/í™œìš©ë¥ /ì˜¨ë„/ì „ë ¥ ì¶”ì 
- ë°°ì¹˜ë³„/ì—í­ë³„ loss ë° ì‹œê°„ ë¡œê¹…
- ì˜ˆìƒ ë‚¨ì€ ì‹œê°„(ETA) í‘œì‹œ

### 4. ì•ˆì •ì ì¸ í•™ìŠµ
- Mixed Precision Training (FP16) ì§€ì›
- Early Stopping
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µì›

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
deeplog_training/
â”œâ”€â”€ config.yaml          # í•™ìŠµ ì„¤ì • íŒŒì¼
â”œâ”€â”€ model.py             # DeepLog ëª¨ë¸ ì •ì˜
â”œâ”€â”€ dataset.py           # Lazy Loading Dataset
â”œâ”€â”€ train.py             # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ evaluate.py          # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ utils.py             # GPU ëª¨ë‹ˆí„°ë§, Early Stopping ë“±
â”œâ”€â”€ requirements.txt     # ì˜ì¡´ì„± ëª©ë¡
â””â”€â”€ README.md            # ì‚¬ìš© ê°€ì´ë“œ
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
cd deeplog_training
pip install -r requirements.txt

# ijson í•„ìˆ˜ ì„¤ì¹˜ (Lazy Loadingì˜ í•µì‹¬)
pip install ijson
```

### 2. ì„¤ì • í™•ì¸

`config.yaml` íŒŒì¼ì—ì„œ ë°ì´í„° ê²½ë¡œë¥¼ í™•ì¸/ìˆ˜ì •í•©ë‹ˆë‹¤:

```yaml
data:
  preprocessed_dir: "/home/zzangdol/RADAR/preprocessing/output"
  max_seq_length: 512
  validation_split: 0.1
```

### 3. í•™ìŠµ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰
python train.py

# ì„¤ì • íŒŒì¼ ì§€ì •
python train.py --config config.yaml

# ë°ì´í„° ê²½ë¡œ ì§ì ‘ ì§€ì •
python train.py --data-dir /path/to/data

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
python train.py --epochs 100 --batch-size 128 --lr 0.0005

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
python train.py --resume outputs/checkpoints/epoch_10.pt
```

### 4. ì„œë²„ì—ì„œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

```bash
# nohup ì‚¬ìš©
nohup python train.py > training.log 2>&1 &

# ë˜ëŠ” screen ì‚¬ìš©
screen -S deeplog_training
python train.py
# Ctrl+A, Dë¡œ ë¶„ë¦¬
```

---

## âš™ï¸ ì„¤ì • ê°€ì´ë“œ

### ëª¨ë¸ ì„¤ì • (`model`)

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|-------|------|
| `vocab_size` | 10000 | ì–´íœ˜ í¬ê¸° (Event ID ìˆ˜) |
| `embedding_dim` | 128 | ì„ë² ë”© ì°¨ì› |
| `hidden_size` | 256 | LSTM ì€ë‹‰ì¸µ í¬ê¸° |
| `num_layers` | 2 | LSTM ë ˆì´ì–´ ìˆ˜ |
| `dropout` | 0.2 | ë“œë¡­ì•„ì›ƒ í™•ë¥  |

### í•™ìŠµ ì„¤ì • (`training`)

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|-------|------|
| `batch_size` | 256 | ë°°ì¹˜ í¬ê¸° (4 GPU ê¸°ì¤€, GPUë‹¹ 64) |
| `learning_rate` | 0.001 | ì´ˆê¸° í•™ìŠµë¥  |
| `num_epochs` | 50 | ì´ ì—í­ ìˆ˜ |
| `max_grad_norm` | 1.0 | Gradient Clipping |
| `use_multi_gpu` | true | ë©€í‹° GPU ì‚¬ìš© |
| `mixed_precision` | true | FP16 í•™ìŠµ |

### Early Stopping ì„¤ì •

```yaml
training:
  early_stopping:
    enabled: true
    patience: 5        # ê°œì„  ì—†ì´ ì§€ì†ë  ìˆ˜ ìˆëŠ” ì—í­ ìˆ˜
    min_delta: 0.0001  # ê°œì„ ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ìµœì†Œ ë³€í™”ëŸ‰
```

### Lazy Loading ì„¤ì •

```yaml
data:
  lazy_loading:
    enabled: true
    buffer_size: 10000    # ë©”ëª¨ë¦¬ì— ìœ ì§€í•  ìƒ˜í”Œ ìˆ˜
    shuffle_buffer: true  # ë²„í¼ ë‚´ ì…”í”Œ ì—¬ë¶€
```

---

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ì½˜ì†” ì¶œë ¥ ì˜ˆì‹œ

```
2026-02-08 20:30:15 | INFO | [Step 1000] Loss: 2.3456 (avg: 2.4567) | LR: 9.90e-04 | GPU: [0:12345MB|95%] [1:12340MB|94%] | Time: 10.5m (ETA: 2h 30m)

GPU ìƒíƒœ (Step 1000):
  GPU 0: ë©”ëª¨ë¦¬: 12345/32510MB (38.0%) | í™œìš©ë¥ : 95% | ì˜¨ë„: 65Â°C | ì „ë ¥: 250W
  GPU 1: ë©”ëª¨ë¦¬: 12340/32510MB (37.9%) | í™œìš©ë¥ : 94% | ì˜¨ë„: 64Â°C | ì „ë ¥: 248W
  GPU 2: ë©”ëª¨ë¦¬: 12350/32510MB (38.0%) | í™œìš©ë¥ : 96% | ì˜¨ë„: 66Â°C | ì „ë ¥: 252W
  GPU 3: ë©”ëª¨ë¦¬: 12342/32510MB (37.9%) | í™œìš©ë¥ : 93% | ì˜¨ë„: 63Â°C | ì „ë ¥: 245W
```

### ì¶œë ¥ íŒŒì¼

í•™ìŠµ ì™„ë£Œ í›„ ë‹¤ìŒ ìœ„ì¹˜ì— íŒŒì¼ì´ ì €ì¥ë©ë‹ˆë‹¤:

```
/home/zzangdol/silverw/deeplog/
â”œâ”€â”€ training.log                     # ì „ì²´ í•™ìŠµ ë¡œê·¸
â”œâ”€â”€ training_history.json            # í•™ìŠµ ì´ë ¥ (loss, lr ë“±)
â”œâ”€â”€ evaluation_results_YYYYMMDD.json # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (JSON)
â”œâ”€â”€ evaluation_report_YYYYMMDD.txt   # ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸ (í…ìŠ¤íŠ¸)
â””â”€â”€ output/
    â””â”€â”€ checkpoints/
        â”œâ”€â”€ best_model.pt            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        â”œâ”€â”€ epoch_1.pt               # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸
        â”œâ”€â”€ epoch_2.pt
        â””â”€â”€ step_5000.pt             # ìŠ¤í…ë³„ ì²´í¬í¬ì¸íŠ¸
```

---

## ğŸ”§ ë©”ëª¨ë¦¬ ìµœì í™” ê°€ì´ë“œ

### OOM ë°œìƒ ì‹œ ì¡°ì¹˜

1. **ë°°ì¹˜ í¬ê¸° ê°ì†Œ**
   ```bash
   python train.py --batch-size 128
   ```

2. **ë²„í¼ í¬ê¸° ê°ì†Œ** (config.yaml)
   ```yaml
   data:
     lazy_loading:
       buffer_size: 5000
   ```

3. **ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ**
   ```yaml
   data:
     max_seq_length: 256
   ```

4. **Mixed Precision í™œì„±í™”** (ê¸°ë³¸ í™œì„±í™”)
   ```yaml
   training:
     mixed_precision: true
   ```

---

## ğŸ“ˆ í•™ìŠµ ê²°ê³¼ ë¶„ì„

`training_history.json` íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ê³¡ì„ ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import json
import matplotlib.pyplot as plt

with open('outputs/training_history.json', 'r') as f:
    history = json.load(f)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train')
if history['val_loss']:
    plt.plot(history['val_loss'], label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')

plt.subplot(1, 2, 2)
plt.plot(history['learning_rate'])
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')

plt.tight_layout()
plt.savefig('training_curve.png')
plt.show()
```

---

## ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ë ¤ë©´:

```bash
# ìµœê³  ëª¨ë¸ í‰ê°€
python evaluate.py --checkpoint /home/zzangdol/silverw/deeplog/output/checkpoints/best_model.pt

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ í‰ê°€
python evaluate.py --checkpoint /path/to/checkpoint.pt --output-dir /home/zzangdol/silverw/deeplog
```

### í‰ê°€ ì§€í‘œ

1. **Top-k Accuracy**: ë‹¤ìŒ ë¡œê·¸ ì˜ˆì¸¡ì´ top-k ì•ˆì— ìˆëŠ” ë¹„ìœ¨
   - Top-1, Top-5, Top-10, Top-20 ì •í™•ë„ ì¸¡ì •

2. **ì´ìƒ íƒì§€ ì„±ëŠ¥** (ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°):
   - Precision, Recall, F1 Score
   - False Positive Rate
   - ë‹¤ì–‘í•œ ì„ê³„ê°’(P90, P95, P99)ì—ì„œì˜ ì„±ëŠ¥

### í‰ê°€ ê²°ê³¼ ì˜ˆì‹œ

```
================================================================================
DeepLog ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸
================================================================================
í‰ê°€ ì‹œê°„: 2026-02-08 22:30:00

[ ë‹¤ìŒ ë¡œê·¸ ì˜ˆì¸¡ ì •í™•ë„ ]
  - Evaluation Loss: 1.2345
  - Total Predictions: 1,000,000
  - Top-1 Accuracy: 0.6523 (65.23%)
  - Top-5 Accuracy: 0.8234 (82.34%)
  - Top-10 Accuracy: 0.8912 (89.12%)
  - Top-20 Accuracy: 0.9234 (92.34%)

[ ì´ìƒ ì ìˆ˜ í†µê³„ (ì •ìƒ ë°ì´í„°) ]
  - í‰ê· : 0.1234
  - í‘œì¤€í¸ì°¨: 0.0567
  - ìƒ˜í”Œ ìˆ˜: 50,000

[ ì¶”ì²œ ì„ê³„ê°’ ]
  - p90: 0.2345
  - p95: 0.3456
  - p99: 0.4567
================================================================================
```

---

## ğŸ§ª ì´ìƒ íƒì§€ (Inference)

í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤:

```python
import torch
from model import DeepLog

# ëª¨ë¸ ë¡œë“œ
model = DeepLog(vocab_size=10000, hidden_size=256, num_layers=2)
checkpoint = torch.load('outputs/checkpoints/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ì´ìƒ ì ìˆ˜ ê³„ì‚° (top-k ë°©ì‹)
input_ids = torch.tensor([[1, 5, 3, 7, 2, 8]])  # ë¡œê·¸ ì‹œí€€ìŠ¤
anomaly_scores = model.calculate_anomaly_score(input_ids, top_k=10)
print(f"ì´ìƒ ì ìˆ˜: {anomaly_scores.item():.4f}")

# ì´ìƒ ì ìˆ˜ > ì„ê³„ê°’ì´ë©´ ì´ìƒìœ¼ë¡œ íŒë‹¨
threshold = 0.3
is_anomaly = anomaly_scores.item() > threshold
print(f"ì´ìƒ ì—¬ë¶€: {is_anomaly}")
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### ijson ì„¤ì¹˜ ì˜¤ë¥˜

```bash
# yajl ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
sudo apt-get install libyajl-dev  # Ubuntu
pip install ijson
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# í˜„ì¬ GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# Pythonì—ì„œ ìºì‹œ ì •ë¦¬
import torch
torch.cuda.empty_cache()
```

### ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜

```python
# CPUì—ì„œ ë¡œë“œ
checkpoint = torch.load('checkpoint.pt', map_location='cpu')
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **DeepLog ë…¼ë¬¸**: [CCS 2017](https://dl.acm.org/doi/10.1145/3133956.3134015)
- **PyTorch LSTM**: [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
- **ijson**: [GitHub](https://github.com/ICRAR/ijson)
