#!/usr/bin/env python3
"""
DeepLog í•™ìŠµìš© Lazy Loading Dataset
120GB ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬

í•µì‹¬ ê¸°ëŠ¥:
1. íŒŒì¼ ë‹¨ìœ„ Lazy Loading - ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
2. Generator íŒ¨í„´ì„ í†µí•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
3. ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë“œë¥¼ í†µí•œ OOM ë°©ì§€
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset
from typing import List, Dict, Any, Optional, Iterator, Generator
from pathlib import Path
import logging
import random
import os
import mmap
from collections import deque
import pickle
import time
import ijson  # ìŠ¤íŠ¸ë¦¬ë° JSON íŒŒì‹±ì„ ìœ„í•´

logger = logging.getLogger(__name__)


class LazyLogDataset(IterableDataset):
    """
    Lazy Loading ê¸°ë°˜ ë¡œê·¸ ë°ì´í„°ì…‹
    
    íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì½ì–´ë“¤ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    120GB ë°ì´í„°ë„ ì ì€ ë©”ëª¨ë¦¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    
    # Special Tokens
    PAD_TOKEN_ID = 0
    CLS_TOKEN_ID = 101
    SEP_TOKEN_ID = 102
    
    def __init__(
        self,
        data_files: List[str],
        max_seq_length: int = 512,
        buffer_size: int = 10000,
        shuffle_buffer: bool = True,
        vocab_size: int = 10000,
        world_size: int = 1,
        rank: int = 0,
    ):
        """
        Args:
            data_files: ì „ì²˜ë¦¬ëœ JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            buffer_size: ë©”ëª¨ë¦¬ì— ìœ ì§€í•  ìƒ˜í”Œ ìˆ˜ (ì…”í”Œìš©)
            shuffle_buffer: ë²„í¼ ë‚´ ì…”í”Œ ì—¬ë¶€
            vocab_size: ì–´íœ˜ í¬ê¸°
            world_size: ë¶„ì‚° í•™ìŠµ ì‹œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            rank: í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìˆœìœ„
        """
        self.data_files = sorted(data_files)
        self.max_seq_length = max_seq_length
        self.buffer_size = buffer_size
        self.shuffle_buffer = shuffle_buffer
        self.vocab_size = vocab_size
        self.world_size = world_size
        self.rank = rank
        
        # íŒŒì¼ë³„ ìƒ˜í”Œ ìˆ˜ ìºì‹± (ì „ì²´ í¬ê¸° ì¶”ì •ìš©)
        self._file_sample_counts = {}
        self._total_samples = None
        
        logger.info(f"LazyLogDataset ì´ˆê¸°í™”")
        logger.info(f"  - ë°ì´í„° íŒŒì¼ ìˆ˜: {len(data_files)}")
        logger.info(f"  - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_length}")
        logger.info(f"  - ë²„í¼ í¬ê¸°: {buffer_size:,}")
        logger.info(f"  - ë²„í¼ ì…”í”Œ: {shuffle_buffer}")
    
    def _count_samples_in_file(self, file_path: str) -> int:
        """íŒŒì¼ ë‚´ ìƒ˜í”Œ ìˆ˜ ì¹´ìš´íŠ¸ (ë¹ ë¥¸ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •)"""
        if file_path in self._file_sample_counts:
            return self._file_sample_counts[file_path]
        
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        cache_file = f"{file_path}.sample_count.cache"
        
        # 1. ìºì‹œê°€ ìˆê³  ìµœì‹ ì´ë©´ ì‚¬ìš©
        if os.path.exists(cache_file):
            try:
                file_mtime = os.path.getmtime(file_path)
                cache_mtime = os.path.getmtime(cache_file)
                
                if cache_mtime >= file_mtime:
                    with open(cache_file, 'r') as f:
                        count = int(f.read().strip())
                        self._file_sample_counts[file_path] = count
                        logger.debug(f"ìºì‹œì—ì„œ ìƒ˜í”Œ ìˆ˜ ë¡œë“œ: {file_path} â†’ {count:,}")
                        return count
            except Exception as e:
                logger.debug(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë¹ ë¥¸ ì¶”ì • (ê¸°ë³¸ ì „ëµ)
        file_size = os.path.getsize(file_path)
        
        # í‰ê·  ìƒ˜í”Œ í¬ê¸° ì¶”ì • (JSON í¬ë§· ê¸°ì¤€)
        # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì¸¡ì •í•œ ê°’ìœ¼ë¡œ ë³´ì • ê°€ëŠ¥
        avg_sample_size = 800  # bytes (ë³´ìˆ˜ì  ì¶”ì •)
        
        count = max(1, file_size // avg_sample_size)
        
        logger.debug(f"íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •: {file_path} â†’ {count:,} ìƒ˜í”Œ ({file_size:,} bytes)")
        
        # 3. ìºì‹œ ì €ì¥ (ë¹„ë™ê¸°ì ìœ¼ë¡œ ë‚˜ì¤‘ì— ì €ì¥ ê°€ëŠ¥)
        try:
            with open(cache_file, 'w') as f:
                f.write(str(count))
        except Exception as e:
            logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        self._file_sample_counts[file_path] = count
        return count
    
    def get_total_samples(self, force_estimate: bool = True) -> int:
        """ì „ì²´ ìƒ˜í”Œ ìˆ˜ ë°˜í™˜ (ì¶”ì •ì¹˜)
        
        Args:
            force_estimate: Trueë©´ ì •í™•í•œ ì¹´ìš´íŠ¸ ëŒ€ì‹  ë¹ ë¥¸ ì¶”ì •ì¹˜ ì‚¬ìš©
        """
        if self._total_samples is None:
            start_time = time.time()
            total = 0
            
            logger.info(f"ìƒ˜í”Œ ìˆ˜ ì¶”ì • ì¤‘... ({len(self.data_files)} íŒŒì¼)")
            
            for i, file_path in enumerate(self.data_files):
                total += self._count_samples_in_file(file_path)
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸ (íŒŒì¼ì´ ë§ì„ ë•Œ)
                if (i + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"  ì§„í–‰: {i+1}/{len(self.data_files)} íŒŒì¼ ({elapsed:.1f}s)")
            
            self._total_samples = total
            elapsed = time.time() - start_time
            logger.info(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜ (ì¶”ì •): {self._total_samples:,} (ì†Œìš” ì‹œê°„: {elapsed:.2f}s)")
        
        return self._total_samples
    
    def _stream_samples_from_file(self, file_path: str) -> Generator[Dict, None, None]:
        """íŒŒì¼ì—ì„œ ìƒ˜í”Œì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ê¸°"""
        try:
            with open(file_path, 'rb') as f:
                # ijsonì„ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ JSON ìŠ¤íŠ¸ë¦¬ë°
                for session in ijson.items(f, 'item'):
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸
                    if 'token_ids' not in session:
                        continue
                    
                    yield session
                    
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {file_path} - {e}")
    
    def _process_sample(self, session: Dict) -> Dict[str, torch.Tensor]:
        """ìƒ˜í”Œì„ í…ì„œë¡œ ë³€í™˜"""
        token_ids = session.get('token_ids', [])
        attention_mask = session.get('attention_mask', [1] * len(token_ids))
        
        # ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        if len(token_ids) != len(attention_mask):
            attention_mask = [1] * len(token_ids)
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        if len(token_ids) > self.max_seq_length:
            token_ids = token_ids[:self.max_seq_length]
            attention_mask = attention_mask[:self.max_seq_length]
        
        # íŒ¨ë”©
        seq_len = len(token_ids)
        if seq_len < self.max_seq_length:
            padding_len = self.max_seq_length - seq_len
            token_ids = token_ids + [self.PAD_TOKEN_ID] * padding_len
            attention_mask = attention_mask + [0] * padding_len
        
        # vocab_size ë²”ìœ„ ì œí•œ
        token_ids = [min(tid, self.vocab_size - 1) for tid in token_ids]
        
        # í…ì„œ ë³€í™˜
        input_ids = torch.tensor(token_ids, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        
        # labelsëŠ” input_idsì™€ ë™ì¼ (ë‹¤ìŒ í† í° ì˜ˆì¸¡ í•™ìŠµ)
        labels = input_ids.clone()
        # íŒ¨ë”© ìœ„ì¹˜ëŠ” -100ìœ¼ë¡œ ì„¤ì • (lossì—ì„œ ë¬´ì‹œ)
        labels[attention_mask == 0] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
        }
    
    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:
        """ë°ì´í„° ì´í„°ë ˆì´í„°"""
        worker_info = torch.utils.data.get_worker_info()
        
        # ë©€í‹° ì›Œì»¤ í™˜ê²½ì—ì„œ íŒŒì¼ ë¶„ë°°
        if worker_info is not None:
            num_workers = worker_info.num_workers
            worker_id = worker_info.id
        else:
            num_workers = 1
            worker_id = 0
        
        # ë¶„ì‚° í•™ìŠµ + ë©€í‹° ì›Œì»¤ ê³ ë ¤
        total_workers = self.world_size * num_workers
        global_worker_id = self.rank * num_workers + worker_id
        
        # íŒŒì¼ì„ ì›Œì»¤ë“¤ì—ê²Œ ë¶„ë°°
        files_for_worker = [
            f for i, f in enumerate(self.data_files)
            if i % total_workers == global_worker_id
        ]
        
        # ì…”í”Œ ë²„í¼
        buffer = deque(maxlen=self.buffer_size)
        
        for file_path in files_for_worker:
            for session in self._stream_samples_from_file(file_path):
                sample = self._process_sample(session)
                
                if self.shuffle_buffer:
                    buffer.append(sample)
                    
                    # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ëœë¤í•˜ê²Œ í•˜ë‚˜ êº¼ë‚´ì„œ yield
                    if len(buffer) >= self.buffer_size:
                        idx = random.randint(0, len(buffer) - 1)
                        yield buffer[idx]
                        del buffer[idx]
                else:
                    yield sample
        
        # ë²„í¼ì— ë‚¨ì€ ìƒ˜í”Œ ì²˜ë¦¬
        if self.shuffle_buffer:
            indices = list(range(len(buffer)))
            random.shuffle(indices)
            for idx in indices:
                yield buffer[idx]


class StreamingLogDataset(IterableDataset):
    """
    ë”ìš± ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹
    
    íŒŒì¼ì„ í•œ ì¤„ì”© ì½ìœ¼ë©° ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ijsonì´ ì—†ëŠ” í™˜ê²½ì„ ìœ„í•œ ëŒ€ì•ˆì…ë‹ˆë‹¤.
    """
    
    PAD_TOKEN_ID = 0
    
    def __init__(
        self,
        data_files: List[str],
        max_seq_length: int = 512,
        vocab_size: int = 10000,
        samples_per_file: int = -1,  # -1 means all
    ):
        self.data_files = sorted(data_files)
        self.max_seq_length = max_seq_length
        self.vocab_size = vocab_size
        self.samples_per_file = samples_per_file
        
        logger.info(f"StreamingLogDataset ì´ˆê¸°í™”: {len(data_files)} íŒŒì¼")
    
    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:
        worker_info = torch.utils.data.get_worker_info()
        
        if worker_info is not None:
            files_per_worker = len(self.data_files) // worker_info.num_workers
            start_idx = worker_info.id * files_per_worker
            end_idx = start_idx + files_per_worker
            if worker_info.id == worker_info.num_workers - 1:
                end_idx = len(self.data_files)
            files_to_process = self.data_files[start_idx:end_idx]
        else:
            files_to_process = self.data_files
        
        for file_path in files_to_process:
            try:
                yield from self._process_file(file_path)
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {file_path} - {e}")
                continue
    
    def _process_file(self, file_path: str) -> Generator[Dict[str, torch.Tensor], None, None]:
        """íŒŒì¼ì—ì„œ ìƒ˜í”Œ ìƒì„±"""
        count = 0
        
        try:
            # íŒŒì¼ì„ í†µì§¸ë¡œ ì½ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬
            with open(file_path, 'rb') as f:
                try:
                    import ijson
                    parser = ijson.items(f, 'item')
                except ImportError:
                    # ijsonì´ ì—†ìœ¼ë©´ ì¼ë°˜ json ì‚¬ìš© (ë©”ëª¨ë¦¬ ì£¼ì˜)
                    logger.warning("ijson ë¯¸ì„¤ì¹˜, ì¼ë°˜ JSON ë¡œë“œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì£¼ì˜)")
                    f.seek(0)
                    import json
                    data = json.load(f)
                    parser = iter(data)
                
                for session in parser:
                    if self.samples_per_file > 0 and count >= self.samples_per_file:
                        break
                    
                    token_ids = session.get('token_ids', [])
                    if not token_ids:
                        continue
                    
                    attention_mask = session.get('attention_mask', [1] * len(token_ids))
                    
                    # ê¸¸ì´ ì¡°ì •
                    if len(token_ids) > self.max_seq_length:
                        token_ids = token_ids[:self.max_seq_length]
                        attention_mask = attention_mask[:self.max_seq_length]
                    elif len(token_ids) < self.max_seq_length:
                        pad_len = self.max_seq_length - len(token_ids)
                        token_ids = token_ids + [self.PAD_TOKEN_ID] * pad_len
                        attention_mask = attention_mask + [0] * pad_len
                    
                    # Vocab ë²”ìœ„ ì œí•œ
                    token_ids = [min(t, self.vocab_size - 1) for t in token_ids]
                    
                    input_ids = torch.tensor(token_ids, dtype=torch.long)
                    attn_mask = torch.tensor(attention_mask, dtype=torch.long)
                    labels = input_ids.clone()
                    labels[attn_mask == 0] = -100
                    
                    yield {
                        'input_ids': input_ids,
                        'attention_mask': attn_mask,
                        'labels': labels,
                    }
                    count += 1
                    
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {file_path} - {e}")


class InMemoryLogDataset(Dataset):
    """
    ë©”ëª¨ë¦¬ ë‚´ ë°ì´í„°ì…‹ (ì‘ì€ ë°ì´í„°ì…‹ ë˜ëŠ” ê²€ì¦ìš©)
    
    ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
    ê²€ì¦ ë°ì´í„°ì…‹ì²˜ëŸ¼ ì‘ì€ ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤.
    """
    
    PAD_TOKEN_ID = 0
    
    def __init__(
        self,
        data_files: List[str],
        max_seq_length: int = 512,
        vocab_size: int = 10000,
        max_samples: Optional[int] = None,
    ):
        self.max_seq_length = max_seq_length
        self.vocab_size = vocab_size
        self.samples = []
        
        logger.info(f"InMemoryLogDataset ë¡œë“œ ì¤‘...")
        self._load_data(data_files, max_samples)
        logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(self.samples):,} ìƒ˜í”Œ")
    
    def _load_data(self, data_files: List[str], max_samples: Optional[int] = None):
        """ë°ì´í„° ë¡œë“œ"""
        total_loaded = 0
        
        for file_path in data_files:
            if max_samples and total_loaded >= max_samples:
                break
            
            try:
                with open(file_path, 'rb') as f:
                    try:
                        import ijson
                        for session in ijson.items(f, 'item'):
                            if max_samples and total_loaded >= max_samples:
                                break
                            
                            token_ids = session.get('token_ids', [])
                            if token_ids:
                                self.samples.append(session)
                                total_loaded += 1
                    except ImportError:
                        f.seek(0)
                        import json
                        data = json.load(f)
                        for session in data:
                            if max_samples and total_loaded >= max_samples:
                                break
                            token_ids = session.get('token_ids', [])
                            if token_ids:
                                self.samples.append(session)
                                total_loaded += 1
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {file_path} - {e}")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        session = self.samples[idx]
        
        token_ids = session.get('token_ids', [])
        attention_mask = session.get('attention_mask', [1] * len(token_ids))
        
        # ê¸¸ì´ ì¡°ì •
        if len(token_ids) > self.max_seq_length:
            token_ids = token_ids[:self.max_seq_length]
            attention_mask = attention_mask[:self.max_seq_length]
        elif len(token_ids) < self.max_seq_length:
            pad_len = self.max_seq_length - len(token_ids)
            token_ids = token_ids + [self.PAD_TOKEN_ID] * pad_len
            attention_mask = attention_mask + [0] * pad_len
        
        # Vocab ë²”ìœ„ ì œí•œ
        token_ids = [min(t, self.vocab_size - 1) for t in token_ids]
        
        input_ids = torch.tensor(token_ids, dtype=torch.long)
        attn_mask = torch.tensor(attention_mask, dtype=torch.long)
        labels = input_ids.clone()
        labels[attn_mask == 0] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attn_mask,
            'labels': labels,
        }


def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    ë°°ì¹˜ ë°ì´í„° ê²°í•© í•¨ìˆ˜
    
    Args:
        batch: ë°°ì¹˜ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ë°°ì¹˜ ë”•ì…”ë„ˆë¦¬
    """
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch]),
    }


def create_dataloaders(
    data_files: List[str],
    config: Dict[str, Any],
    validation_split: float = 0.1,
    rank: int = 0,
    world_size: int = 1,
) -> tuple:
    """
    í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„±

    ìë™ìœ¼ë¡œ íŒŒì¼ í˜•ì‹ì„ ê°ì§€í•˜ì—¬ ìµœì ì˜ ë°ì´í„°ì…‹ ì‚¬ìš©:
    - .parquet â†’ ParquetLogDataset (10-20ë°° ë¹ ë¦„)
    - .json â†’ LazyLogDataset (í•˜ìœ„ í˜¸í™˜ì„±)

    Args:
        data_files: ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        validation_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        rank: DDP rank (ê¸°ë³¸ê°’: 0)
        world_size: DDP world size (ê¸°ë³¸ê°’: 1)

    Returns:
        (train_dataloader, val_dataloader) íŠœí”Œ
    """
    # ì„¤ì • ì¶”ì¶œ
    data_config = config.get('data', {})
    training_config = config.get('training', {})

    max_seq_length = data_config.get('max_seq_length', 512)
    vocab_size = config.get('model', {}).get('vocab_size', 10000)
    batch_size = training_config.get('batch_size', 64)
    num_workers = training_config.get('num_workers', 4)

    lazy_config = data_config.get('lazy_loading', {})
    buffer_size = lazy_config.get('buffer_size', 10000)
    shuffle_buffer = lazy_config.get('shuffle_buffer', True)

    # íŒŒì¼ í˜•ì‹ ê°ì§€ (ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€)
    if not data_files:
        raise ValueError("ë°ì´í„° íŒŒì¼ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    file_ext = Path(data_files[0]).suffix.lower()
    use_parquet = (file_ext == '.parquet')

    if use_parquet:
        logger.info("ğŸš€ Parquet ë°ì´í„°ì…‹ ì‚¬ìš© (ê³ ì† I/O)")
        try:
            from dataset_parquet import ParquetLogDataset, InMemoryParquetDataset
        except ImportError:
            logger.error("dataset_parquet.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. JSON ë°ì´í„°ì…‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            use_parquet = False

    if not use_parquet:
        logger.info("JSON ë°ì´í„°ì…‹ ì‚¬ìš© (Lazy Loading)")

    # íŒŒì¼ì„ í•™ìŠµ/ê²€ì¦ìœ¼ë¡œ ë¶„ë¦¬
    random.shuffle(data_files)
    val_count = max(1, int(len(data_files) * validation_split))
    val_files = data_files[:val_count]
    train_files = data_files[val_count:]

    logger.info(f"í•™ìŠµ íŒŒì¼: {len(train_files)}, ê²€ì¦ íŒŒì¼: {len(val_files)}")

    # í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    if use_parquet:
        train_dataset = ParquetLogDataset(
            data_files=train_files,
            max_seq_length=max_seq_length,
            vocab_size=vocab_size,
            buffer_size=buffer_size,
            shuffle_buffer=shuffle_buffer,
            world_size=world_size,
            rank=rank,
        )
        val_dataset = InMemoryParquetDataset(
            data_files=val_files,
            max_seq_length=max_seq_length,
            vocab_size=vocab_size,
            max_samples=50000,
        )
    else:
        train_dataset = LazyLogDataset(
            data_files=train_files,
            max_seq_length=max_seq_length,
            buffer_size=buffer_size,
            shuffle_buffer=shuffle_buffer,
            vocab_size=vocab_size,
        )
        # DDP ì§€ì› (IterableDataset)
        train_dataset.world_size = world_size
        train_dataset.rank = rank

        val_dataset = InMemoryLogDataset(
            data_files=val_files,
            max_seq_length=max_seq_length,
            vocab_size=vocab_size,
            max_samples=50000,
        )

    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
        prefetch_factor=training_config.get('prefetch_factor', 2) if num_workers > 0 else None,
        persistent_workers=training_config.get('persistent_workers', False) if num_workers > 0 else False,
        drop_last=True,  # DDPì—ì„œ ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
        drop_last=False,  # ê²€ì¦ì—ì„œëŠ” ëª¨ë“  ë°ì´í„° ì‚¬ìš©
    )

    return train_loader, val_loader
