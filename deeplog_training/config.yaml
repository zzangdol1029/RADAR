# DeepLog í•™ìŠµ ì„¤ì • íŒŒì¼
# Tesla V100-DGXS-32GB x 4 í™˜ê²½ì— ìµœì í™”

# ëª¨ë¸ ì„¤ì •
model:
  vocab_size: 10000          # ì–´íœ˜ í¬ê¸° (Event ID + Special Tokens)
  embedding_dim: 128         # ì„ë² ë”© ì°¨ì›
  hidden_size: 256           # LSTM ì€ë‹‰ì¸µ í¬ê¸°
  num_layers: 2              # LSTM ë ˆì´ì–´ ìˆ˜
  dropout: 0.2               # ë“œë¡­ì•„ì›ƒ í™•ë¥ 

# í•™ìŠµ ì„¤ì •
training:
  # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
  batch_size: 512                      # GPUë‹¹ ë°°ì¹˜ í¬ê¸°
  gradient_accumulation_steps: 4       # ğŸ†• Gradient Accumulation (512*4*4=8192 effective batch)
  learning_rate: 0.001                 # ì´ˆê¸° í•™ìŠµë¥ 
  weight_decay: 0.0001                 # ê°€ì¤‘ì¹˜ ê°ì‡  (L2 ì •ê·œí™”)
  num_epochs: 50                       # ì´ ì—í­ ìˆ˜
  max_grad_norm: 1.0                   # Gradient Clipping ìµœëŒ€ê°’

  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
  scheduler_type: "reduce_on_plateau"  # ReduceLROnPlateau ì‚¬ìš©
  min_lr: 0.000001                     # ìµœì†Œ í•™ìŠµë¥ 
  warmup_steps: 1000                   # ì›Œë°ì—… ìŠ¤í…

  # ë¡œê¹… ì„¤ì •
  log_interval: 100                    # ë°°ì¹˜ë³„ ë¡œê·¸ ì¶œë ¥ ê°„ê²©
  save_interval: 3000                  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (ìŠ¤í…)
  
  # âœ… Early Stopping ì„¤ì • (í™œì„±í™”)
  early_stopping:
    enabled: true             # Early Stopping í™œì„±í™”
    patience: 5               # 5 ì—í­ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
    min_delta: 0.0001         # ê°œì„ ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ìµœì†Œ ë³€í™”ëŸ‰

  # âœ… ReduceLROnPlateau íŒŒë¼ë¯¸í„°
  lr_scheduler:
    factor: 0.5               # í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨ (1/2ë¡œ ê°ì†Œ)
    patience: 2               # 2 ì—í­ ê°œì„  ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ (Early Stoppingë³´ë‹¤ ì§§ê²Œ)
    min_lr: 0.000001          # ìµœì†Œ í•™ìŠµë¥ 
    threshold: 0.0001         # ê°œì„  íŒë‹¨ ì„ê³„ê°’

  # GPU ì„¤ì •
  use_multi_gpu: true                  # ë©€í‹° GPU ì‚¬ìš© ì—¬ë¶€
  use_ddp: true                        # ğŸ†• DistributedDataParallel ì‚¬ìš© (DataParallel ëŒ€ì‹ )
  mixed_precision: true                # Mixed Precision Training (FP16)

  # ë°ì´í„° ë¡œë” ì„¤ì •
  num_workers: 8                       # ğŸ”„ ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜ (DDPì—ì„œëŠ” í”„ë¡œì„¸ìŠ¤ë‹¹)
  prefetch_factor: 6                   # ğŸ”„ í”„ë¦¬í˜ì¹˜ íŒ©í„° ì¦ê°€
  persistent_workers: true             # ğŸ†• ì›Œì»¤ ì¬ì‚¬ìš© (ì—í­ ê°„)

  # H100 ìµœì í™” (ì¡°ê±´ë¶€ - A100/H100 í™˜ê²½ì—ì„œë§Œ í™œì„±í™”)
  h100_optimizations:
    enabled: false                     # H100 í™˜ê²½ì—ì„œ trueë¡œ ë³€ê²½
    compile_model: false               # torch.compile (PyTorch 2.0+)
    use_fp8: false                     # FP8 í•™ìŠµ (Transformer Engine)
    use_cuda_graphs: false             # CUDA Graphs

# ë°ì´í„° ì„¤ì •
data:
  # ğŸ”„ Parquet ì‚¬ìš© (10-20ë°° ë¹ ë¥¸ I/O) - JSONì—ì„œ ë³€í™˜ í›„ ê²½ë¡œ ì—…ë°ì´íŠ¸
  preprocessed_dir: "/home/zzangdol/RADAR/preprocessing/output_parquet"
  # JSON í´ë°± (í•˜ìœ„ í˜¸í™˜ì„±)
  preprocessed_dir_json: "/home/zzangdol/RADAR/preprocessing/output"

  max_seq_length: 512                  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
  validation_split: 0.1                # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (10%)

  # íŒŒì¼ íŒ¨í„´
  file_pattern: "*.parquet"            # ğŸ”„ Parquet íŒ¨í„´
  file_pattern_json: "preprocessed_logs_*.json"  # JSON í´ë°±

  # Lazy Loading ì„¤ì •
  lazy_loading:
    enabled: true
    buffer_size: 20000                 # ğŸ”„ ë²„í¼ í¬ê¸° ì¦ê°€ (ë” ë‚˜ì€ ì…”í”Œë§)
    shuffle_buffer: true

# ì¶œë ¥ ì„¤ì •
output:
  base_dir: "/home/zzangdol/silverw/deeplog"
  dir: "/home/zzangdol/silverw/deeplog/output"
  checkpoint_dir: "checkpoints"
  log_file: "training.log"
  eval_dir: "/home/zzangdol/silverw/deeplog"

# ëª¨ë‹ˆí„°ë§ ì„¤ì •
monitoring:
  gpu_log_interval: 50
  use_tensorboard: false
  tensorboard_dir: "runs"

#  Top-g Accuracy í‰ê°€ ì„¤ì • ì¶”ê°€
evaluation:
  top_g: 9                    # DeepLog ë…¼ë¬¸ ê¸°ì¤€ g=9
  eval_interval: 1            # ë§¤ ì—í­ë§ˆë‹¤ í‰ê°€