#!/usr/bin/env python3
"""
DeepLog ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ë©”ì¸)
Tesla V100-DGXS-32GB x 4 í™˜ê²½ì— ìµœì í™”

í•µì‹¬ ê¸°ëŠ¥:
1. Lazy Loadingìœ¼ë¡œ 120GB ë°ì´í„° ì²˜ë¦¬ (OOM ë°©ì§€)
2. ë©€í‹° GPU í•™ìŠµ (DataParallel)
3. Mixed Precision Training (FP16)
4. ìƒì„¸í•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§ (GPU ìƒíƒœ, ì‹œê°„, loss ë“±)
5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µì›
6. Early Stopping
"""

import os
import sys
import json
import yaml
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

from model import DeepLog, create_deeplog_model
from dataset import LazyLogDataset, InMemoryLogDataset, create_dataloaders, collate_fn
from utils import (
    GPUMonitor, TrainingTimer, EarlyStopping, AverageMeter,
    get_lr_scheduler, setup_logging, print_training_banner
)

# ê²½ê³  ì–µì œ
warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.parallel._functions')

logger = logging.getLogger(__name__)


class DeepLogTrainer:
    """DeepLog ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.training_config = config.get('training', {})
        self.model_config = config.get('model', {})
        self.data_config = config.get('data', {})
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_gpus = torch.cuda.device_count()
        self.use_multi_gpu = self.training_config.get('use_multi_gpu', True) and self.num_gpus > 1
        
        logger.info(f"ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"GPU ìˆ˜: {self.num_gpus}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_config = config.get('output', {})
        self.base_dir = Path(output_config.get('base_dir', '/home/zzangdol/silverw/deeplog'))
        self.output_dir = Path(output_config.get('dir', '/home/zzangdol/silverw/deeplog/output'))
        self.checkpoint_dir = self.output_dir / output_config.get('checkpoint_dir', 'checkpoints')
        self.eval_dir = Path(output_config.get('eval_dir', '/home/zzangdol/silverw/deeplog'))
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.eval_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ìƒì„±
        self.model = create_deeplog_model(self.model_config)
        self.model.to(self.device)
        
        # ë©€í‹° GPU ë˜í•‘
        if self.use_multi_gpu:
            logger.info(f"DataParallelë¡œ {self.num_gpus}ê°œ GPUì— ëª¨ë¸ ë°°í¬")
            self.model = nn.DataParallel(self.model)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=float(self.training_config.get('learning_rate', 0.001)),
            weight_decay=float(self.training_config.get('weight_decay', 0.0001)),
        )
        
        # Mixed Precision
        self.use_amp = self.training_config.get('mixed_precision', True) and torch.cuda.is_available()
        self.scaler = GradScaler() if self.use_amp else None
        logger.info(f"Mixed Precision (FP16): {self.use_amp}")
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        
        # ëª¨ë‹ˆí„°ë§ ë„êµ¬
        monitoring_config = config.get('monitoring', {})
        self.gpu_monitor = GPUMonitor(log_interval=monitoring_config.get('gpu_log_interval', 50))
        self.timer = TrainingTimer()
        
        # Early Stopping ì„¤ì •
        es_config = self.training_config.get('early_stopping', {})
        if es_config.get('enabled', False):
            self.early_stopping = EarlyStopping(
                patience=es_config.get('patience', 10),
                min_delta=es_config.get('min_delta', 0.0001),
                mode='min',
                restore_best=True
            )
        else:
            self.early_stopping = None
        
        # í•™ìŠµ ë¡œê·¸
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'epoch_times': [],
        }
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (trainì—ì„œ ì„¤ì •ë¨)
        self.scheduler = None
    
    def train_epoch(self, train_loader, epoch: int) -> float:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        
        loss_meter = AverageMeter('Loss')
        batch_time_meter = AverageMeter('BatchTime')
        
        self.timer.start_epoch(epoch)
        
        pbar = tqdm(
            enumerate(train_loader),
            desc=f"Epoch {epoch}/{self.training_config.get('num_epochs', 50)}",
            unit='batch',
            leave=True,
            ncols=120,
        )
        
        batch_start = datetime.now()
        
        # ReduceLROnPlateau ì—¬ë¶€ í™•ì¸
        is_plateau_scheduler = self.training_config.get('scheduler_type') == 'reduce_on_plateau'
        
        for batch_idx, batch in pbar:
            input_ids = batch['input_ids'].to(self.device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            if self.use_amp:
                with autocast():
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                    )
                    loss = outputs['loss']
                    
                    if isinstance(loss, torch.Tensor) and loss.dim() > 0:
                        loss = loss.mean()
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.training_config.get('max_grad_norm', 1.0)
                )
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels,
                )
                loss = outputs['loss']
                
                if isinstance(loss, torch.Tensor) and loss.dim() > 0:
                    loss = loss.mean()
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.training_config.get('max_grad_norm', 1.0)
                )
                self.optimizer.step()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (ReduceLROnPlateau ì œì™¸ - ì—í­ ë‹¨ìœ„ë¡œë§Œ ì—…ë°ì´íŠ¸)
            if self.scheduler is not None and not is_plateau_scheduler:
                self.scheduler.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            batch_time = (datetime.now() - batch_start).total_seconds()
            batch_start = datetime.now()
            
            loss_meter.update(loss.item())
            batch_time_meter.update(batch_time)
            
            self.global_step += 1
            self.timer.step()
            
            # Progress bar ì—…ë°ì´íŠ¸
            current_lr = self.optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg': f'{loss_meter.avg:.4f}',
                'lr': f'{current_lr:.2e}',
                'ms/batch': f'{batch_time*1000:.0f}',
            })
            
            # ìƒì„¸ ë¡œê¹…
            log_interval = self.training_config.get('log_interval', 100)
            if self.global_step % log_interval == 0:
                gpu_summary = self.gpu_monitor.get_summary()
                time_summary = self.timer.get_summary()
                
                logger.info(
                    f"[Step {self.global_step}] "
                    f"Loss: {loss.item():.4f} (avg: {loss_meter.avg:.4f}) | "
                    f"LR: {current_lr:.2e} | "
                    f"GPU: {gpu_summary} | "
                    f"Time: {time_summary['elapsed']} (ETA: {time_summary['eta']})"
                )
            
            # GPU ìƒíƒœ ë¡œê¹…
            self.gpu_monitor.log_gpu_status(self.global_step)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_interval = self.training_config.get('save_interval', 5000)
            if self.global_step % save_interval == 0:
                self.save_checkpoint(f'step_{self.global_step}')
        
        pbar.close()
        
        epoch_time = self.timer.end_epoch()
        
        logger.info(f"Epoch {epoch} ì™„ë£Œ: Loss={loss_meter.avg:.4f}, ì‹œê°„={epoch_time:.1f}s")
        
        return loss_meter.avg
    
    @torch.no_grad()
    def validate(self, val_loader, calculate_topg: bool = True) -> Dict[str, float]:
        """ê²€ì¦ (Loss + Top-g Accuracy)
        
        Args:
            val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
            calculate_topg: Top-g Accuracy ê³„ì‚° ì—¬ë¶€
        
        Returns:
            {'val_loss': float, 'topg_accuracy': float}
        """
        self.model.eval()
        
        loss_meter = AverageMeter('ValLoss')
        
        # Top-g Accuracy ê³„ì‚°ìš©
        eval_config = self.config.get('evaluation', {})
        g = eval_config.get('top_g', 9)
        topg_correct = 0
        topg_total = 0
        
        pbar = tqdm(val_loader, desc="Validation", leave=False, ncols=120)
        
        for batch in pbar:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            if self.use_amp:
                with autocast():
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                    )
            else:
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels,
                )
            
            loss = outputs['loss']
            if isinstance(loss, torch.Tensor) and loss.dim() > 0:
                loss = loss.mean()
            
            loss_meter.update(loss.item())
            
            # Top-g Accuracy ê³„ì‚°
            if calculate_topg:
                logits = outputs['logits']  # [batch, seq_len, vocab_size]
                
                # ê° ìœ„ì¹˜ì—ì„œ ìƒìœ„ gê°œ ì˜ˆì¸¡
                _, top_indices = torch.topk(logits, k=g, dim=-1)  # [batch, seq_len, g]
                
                # ì •ë‹µì´ ìƒìœ„ gê°œ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                labels_expanded = labels.unsqueeze(-1).expand_as(top_indices)
                matches = (top_indices == labels_expanded).any(dim=-1)  # [batch, seq_len]
                
                # padding ì œì™¸ (-100ì€ padding í† í°)
                valid_mask = (labels != -100)
                topg_correct += (matches & valid_mask).sum().item()
                topg_total += valid_mask.sum().item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            if calculate_topg and topg_total > 0:
                current_acc = topg_correct / topg_total
                pbar.set_postfix({
                    'val_loss': f'{loss_meter.avg:.4f}',
                    f'top{g}_acc': f'{current_acc:.4f}'
                })
            else:
                pbar.set_postfix({'val_loss': f'{loss_meter.avg:.4f}'})
        
        pbar.close()
        
        result = {'val_loss': loss_meter.avg}
        
        if calculate_topg and topg_total > 0:
            topg_accuracy = topg_correct / topg_total
            result['topg_accuracy'] = topg_accuracy
            logger.info(f"Top-{g} Accuracy: {topg_accuracy:.4f} ({topg_correct:,}/{topg_total:,})")
        
        return result
    
    @torch.no_grad()
    def calculate_topg_accuracy(self, val_loader, g: int = 9) -> float:
        """DeepLog Top-g Accuracy ê³„ì‚°
        
        Args:
            val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
            g: ìƒìœ„ gê°œ ì˜ˆì¸¡ í›„ë³´ (ë…¼ë¬¸ì—ì„œëŠ” g=9)
        
        Returns:
            Top-g Accuracy (0.0 ~ 1.0)
        """
        self.model.eval()
        correct = 0
        total = 0
        
        for batch in tqdm(val_loader, desc="Top-g Accuracy", leave=False):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            outputs = self.model(input_ids=input_ids)
            logits = outputs['logits']  # [batch, seq_len, vocab_size]
            
            # ê° ìœ„ì¹˜ì—ì„œ ìƒìœ„ gê°œ ì˜ˆì¸¡
            _, top_indices = torch.topk(logits, k=g, dim=-1)  # [batch, seq_len, g]
            
            # ì •ë‹µì´ ìƒìœ„ gê°œ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            labels_expanded = labels.unsqueeze(-1).expand_as(top_indices)
            matches = (top_indices == labels_expanded).any(dim=-1)  # [batch, seq_len]
            
            # padding ì œì™¸
            valid_mask = (labels != -100)
            correct += (matches & valid_mask).sum().item()
            total += valid_mask.sum().item()
        
        accuracy = correct / total if total > 0 else 0.0
        return accuracy
    
    def save_checkpoint(self, name: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoint_dir / f'{name}.pt'
        
        # DataParallel ëª¨ë¸ì¸ ê²½ìš° moduleì„ í†µí•´ ì ‘ê·¼
        model_state = (
            self.model.module.state_dict()
            if isinstance(self.model, nn.DataParallel)
            else self.model.state_dict()
        )
        
        checkpoint = {
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'global_step': self.global_step,
            'current_epoch': self.current_epoch,
            'best_loss': self.best_loss,
            'config': self.config,
            'training_history': self.training_history,
        }
        
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if isinstance(self.model, nn.DataParallel):
            self.model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # ìƒíƒœ ë³µì›
        self.global_step = checkpoint.get('global_step', 0)
        self.current_epoch = checkpoint.get('current_epoch', 0)
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        self.training_history = checkpoint.get('training_history', self.training_history)
        
        # Scaler ë¡œë“œ
        if self.scaler is not None and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
        logger.info(f"  Step: {self.global_step}, Epoch: {self.current_epoch}, Best Loss: {self.best_loss:.4f}")
    
    def train(self, train_loader, val_loader=None, num_epochs: Optional[int] = None):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        if num_epochs is None:
            num_epochs = self.training_config.get('num_epochs', 50)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        try:
            total_steps = len(train_loader) * num_epochs
        except TypeError:
            batch_size = self.training_config.get('batch_size', 64)
            num_files = len(train_loader.dataset.data_files) if hasattr(train_loader.dataset, 'data_files') else 50
            estimated_samples_per_file = 1500000
            estimated_samples = num_files * estimated_samples_per_file
            total_steps = (estimated_samples // batch_size) * num_epochs
            logger.info(f"ì´ ìŠ¤í… ì¶”ì •ì¹˜ ì‚¬ìš©: {total_steps:,} (íŒŒì¼ {num_files}ê°œ ê¸°ì¤€)")
        
        self.timer.total_steps = total_steps
        self.timer.total_epochs = num_epochs
        
        self.scheduler = get_lr_scheduler(self.optimizer, self.training_config, total_steps)
        
        # ReduceLROnPlateau ì—¬ë¶€ í™•ì¸
        is_plateau_scheduler = self.training_config.get('scheduler_type') == 'reduce_on_plateau'
        
        # í•™ìŠµ ì‹œì‘ ë¡œê·¸
        print_training_banner(self.config)
        logger.info(f"ì´ ì—í­: {num_epochs}")
        logger.info(f"ì˜ˆìƒ ì´ ìŠ¤í…: {total_steps:,}")
        logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…: {self.training_config.get('scheduler_type', 'cosine')}")
        
        self.timer.start()
        
        # ì—í­ ë£¨í”„
        start_epoch = self.current_epoch + 1
        for epoch in range(start_epoch, num_epochs + 1):
            self.current_epoch = epoch
            
            logger.info(f"\n{'='*80}")
            logger.info(f"Epoch {epoch}/{num_epochs} ì‹œì‘")
            logger.info(f"{'='*80}")
            
            # í•™ìŠµ
            train_loss = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_result = None
            val_loss = None
            topg_accuracy = None
            
            if val_loader is not None:
                eval_config = self.config.get('evaluation', {})
                eval_interval = eval_config.get('eval_interval', 1)
                
                # Top-g AccuracyëŠ” ë§¤ ì—í­ë§ˆë‹¤ ë˜ëŠ” ì„¤ì •ëœ ê°„ê²©ë§ˆë‹¤ ê³„ì‚°
                calculate_topg = (epoch % eval_interval == 0)
                
                val_result = self.validate(val_loader, calculate_topg=calculate_topg)
                val_loss = val_result['val_loss']
                topg_accuracy = val_result.get('topg_accuracy', None)
                
                logger.info(f"Validation Loss: {val_loss:.4f}")
                if topg_accuracy is not None:
                    logger.info(f"Top-g Accuracy: {topg_accuracy:.4f}")
            
            # ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (validation loss ê¸°ì¤€)
            if self.scheduler is not None and is_plateau_scheduler:
                if val_loss is not None:
                    old_lr = self.optimizer.param_groups[0]['lr']
                    self.scheduler.step(val_loss)
                    new_lr = self.optimizer.param_groups[0]['lr']
                    
                    if new_lr != old_lr:
                        logger.warning(f"ğŸ”½ í•™ìŠµë¥  ê°ì†Œ: {old_lr:.2e} â†’ {new_lr:.2e}")
            
            # í•™ìŠµ ì´ë ¥ ì €ì¥
            self.training_history['train_loss'].append(train_loss)
            if val_loss is not None:
                self.training_history['val_loss'].append(val_loss)
            if topg_accuracy is not None:
                if 'topg_accuracy' not in self.training_history:
                    self.training_history['topg_accuracy'] = []
                self.training_history['topg_accuracy'].append(topg_accuracy)
            
            self.training_history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])
            self.training_history['epoch_times'].append(self.timer.epoch_times[-1] if self.timer.epoch_times else 0)
            
            # ìµœê³  ëª¨ë¸ ì €ì¥ (validation loss ê¸°ì¤€)
            current_loss = val_loss if val_loss is not None else train_loss
            if current_loss < self.best_loss:
                self.best_loss = current_loss
                self.save_checkpoint('best_model')
                logger.info(f"âœ… ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! Loss: {current_loss:.4f}")
            
            # ì—í­ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint(f'epoch_{epoch}')
            
            # Early Stopping ì²´í¬ (validation loss ê¸°ì¤€)
            if self.early_stopping is not None:
                if self.early_stopping(current_loss, self.model, epoch):
                    logger.warning("ğŸ›‘ Early Stopping ë°œë™!")
                    break
            
            # ì—í­ ìš”ì•½
            time_summary = self.timer.get_summary()
            summary_lines = [
                f"\nEpoch {epoch} ìš”ì•½:",
                f"  - Train Loss: {train_loss:.4f}",
                f"  - Val Loss: {val_loss:.4f if val_loss else 'N/A'}",
            ]
            
            if topg_accuracy is not None:
                summary_lines.append(f"  - Top-g Accuracy: {topg_accuracy:.4f}")
            
            summary_lines.extend([
                f"  - Best Loss: {self.best_loss:.4f}",
                f"  - Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}",
                f"  - ê²½ê³¼ ì‹œê°„: {time_summary['elapsed']}",
                f"  - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {time_summary['eta']}"
            ])
            
            logger.info("\n".join(summary_lines))
        
        # í•™ìŠµ ì™„ë£Œ
        if self.early_stopping is not None and self.early_stopping.stopped:
            logger.info("Early Stoppingìœ¼ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œ - ìµœê³  ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³µì› ì¤‘...")
            self.early_stopping.restore_best_weights(self.model)
        
        total_time = self.timer.get_elapsed_time()
        logger.info(f"\n{'='*80}")
        logger.info(f"í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ìµœê³  Loss: {self.best_loss:.4f}")
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {self.timer.format_time(total_time)}")
        logger.info(f"{'='*80}")
        
        # í•™ìŠµ ì´ë ¥ ì €ì¥
        history_path = self.base_dir / 'training_history.json'
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, indent=2, ensure_ascii=False)
        logger.info(f"í•™ìŠµ ì´ë ¥ ì €ì¥: {history_path}")
        
        # í‰ê°€ ìë™ ì‹¤í–‰
        self._run_evaluation()
    
    def _run_evaluation(self):
        """í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìë™ ì‹¤í–‰"""
        logger.info("\n" + "=" * 80)
        logger.info("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")
        logger.info("=" * 80)
        
        try:
            from evaluate import DeepLogEvaluator, evaluate_model
            
            # ìµœê³  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            best_checkpoint = self.checkpoint_dir / 'best_model.pt'
            
            if not best_checkpoint.exists():
                logger.warning(f"ìµœê³  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_checkpoint}")
                return
            
            # ë°ì´í„° íŒŒì¼ ëª©ë¡
            data_dir = self.data_config.get('preprocessed_dir', '')
            file_pattern = self.data_config.get('file_pattern', 'preprocessed_logs_*.json')
            
            data_path = Path(data_dir)
            data_files = sorted(data_path.glob(file_pattern))
            
            if not data_files:
                data_files = sorted(data_path.glob('*.json'))
            
            if not data_files:
                logger.warning(f"í‰ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
                return
            
            # í‰ê°€ ì‹¤í–‰
            results = evaluate_model(
                checkpoint_path=str(best_checkpoint),
                config=self.config,
                data_files=[str(f) for f in data_files],
                output_dir=str(self.eval_dir),
                max_samples=50000,
            )
            
            logger.info("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!")
            logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.eval_dir}")
            
        except ImportError as e:
            logger.warning(f"í‰ê°€ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ìˆ˜ë™ìœ¼ë¡œ í‰ê°€í•˜ë ¤ë©´: python evaluate.py --checkpoint <checkpoint_path>")
        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()


def get_data_files(data_dir: str, pattern: str = "preprocessed_logs_*.json") -> List[str]:
    """ë°ì´í„° íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise FileNotFoundError(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    files = sorted(data_path.glob(pattern))
    
    if not files:
        # ëŒ€ì•ˆ: ëª¨ë“  JSON íŒŒì¼ ê²€ìƒ‰
        files = sorted(data_path.glob("*.json"))
    
    logger.info(f"ë°œê²¬ëœ ë°ì´í„° íŒŒì¼: {len(files)}ê°œ")
    
    return [str(f) for f in files]


def load_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    if config_path is None:
        base_dir = Path(__file__).parent
        config_path = str(base_dir / 'config.yaml')
    
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • ë¡œë“œ: {config_path}")
        return config
    else:
        logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
        return {
            'model': {
                'vocab_size': 10000,
                'embedding_dim': 128,
                'hidden_size': 256,
                'num_layers': 2,
                'dropout': 0.2,
            },
            'training': {
                'batch_size': 64,
                'learning_rate': 0.001,
                'num_epochs': 50,
                'max_grad_norm': 1.0,
            },
            'data': {
                'preprocessed_dir': '/home/zzangdol/RADAR/preprocessing/output',
                'max_seq_length': 512,
            },
            'output': {
                'base_dir': '/home/zzangdol/silverw/deeplog',
                'dir': '/home/zzangdol/silverw/deeplog/output',
                'checkpoint_dir': 'checkpoints',
                'eval_dir': '/home/zzangdol/silverw/deeplog',
            },
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='DeepLog ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--config', type=str, default=None,
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--data-dir', type=str, default=None,
                       help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--resume', type=str, default=None,
                       help='ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--epochs', type=int, default=None,
                       help='í•™ìŠµ ì—í­ ìˆ˜')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=None,
                       help='í•™ìŠµë¥ ')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config['data']['preprocessed_dir'] = args.data_dir
    if args.output_dir:
        config['output']['dir'] = args.output_dir
    if args.epochs:
        config['training']['num_epochs'] = args.epochs
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.lr:
        config['training']['learning_rate'] = args.lr
    
    # ë¡œê¹… ì„¤ì • (base_dirì— ë¡œê·¸ ì €ì¥)
    output_config = config.get('output', {})
    base_dir = Path(output_config.get('base_dir', '/home/zzangdol/silverw/deeplog'))
    output_dir = Path(output_config.get('dir', '/home/zzangdol/silverw/deeplog/output'))
    base_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)
    log_file = base_dir / output_config.get('log_file', 'training.log')
    setup_logging(str(log_file))
    
    # ë°ì´í„° íŒŒì¼ ëª©ë¡
    data_dir = config['data']['preprocessed_dir']
    file_pattern = config['data'].get('file_pattern', 'preprocessed_logs_*.json')
    data_files = get_data_files(data_dir, file_pattern)
    
    if not data_files:
        logger.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # DataLoader ìƒì„±
    logger.info("ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader = create_dataloaders(
        data_files=data_files,
        config=config,
        validation_split=config['data'].get('validation_split', 0.1),
    )
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = DeepLogTrainer(config)
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # í•™ìŠµ ì‹œì‘
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.epochs or config['training']['num_epochs'],
    )


if __name__ == '__main__':
    main()
