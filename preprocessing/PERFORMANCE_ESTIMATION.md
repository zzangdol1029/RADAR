# 로그 전처리 성능 및 예상 소요 시간

## 현재 로그 데이터 현황

### 전체 통계
- **총 로그 라인 수**: 약 347,986,016줄 (약 3억 4천만 줄)
- **로그 파일 수**: 다수 (gateway, manager, research, eureka 등)

## 처리 속도 추정

### 단계별 처리 속도

#### 1. 로그 읽기
- **속도**: 약 100,000 - 500,000줄/초
- **병목**: 디스크 I/O

#### 2. 로그 정리 (Cleaning)
- **속도**: 약 50,000 - 200,000줄/초
- **병목**: 정규식 매칭

#### 3. Drain3 파싱 (가장 느린 단계)
- **속도**: 약 1,000 - 5,000줄/초
- **병목**: 템플릿 마이닝 알고리즘
- **영향 요인**:
  - 로그 복잡도
  - 고유 이벤트 수
  - Drain3 설정 (depth, similarity threshold)

#### 4. 세션화
- **속도**: 약 5,000 - 20,000줄/초
- **병목**: 타임스탬프 파싱 및 그룹화

#### 5. 인코딩 및 메타데이터 결합
- **속도**: 약 10,000 - 50,000줄/초
- **병목**: JSON 직렬화

### 전체 처리 속도 (보수적 추정)

**예상 처리 속도**: 약 **2,000 - 3,000줄/초**

Drain3 파싱이 가장 느린 단계이므로, 전체 속도는 Drain3 속도에 근사합니다.

## 예상 소요 시간 계산

### 전체 로그 처리 (347,986,016줄)

#### 최적 시나리오 (3,000줄/초)
```
347,986,016줄 ÷ 3,000줄/초 = 115,995초
≈ 32시간 (약 1.3일)
```

#### 일반 시나리오 (2,000줄/초)
```
347,986,016줄 ÷ 2,000줄/초 = 173,993초
≈ 48시간 (약 2일)
```

#### 보수적 시나리오 (1,000줄/초)
```
347,986,016줄 ÷ 1,000줄/초 = 347,986초
≈ 97시간 (약 4일)
```

### 주요 로그 파일별 예상 시간

#### manager.log (약 28,000줄)
- **예상 시간**: 약 9-28초

#### gateway.log (약 61,000줄)
- **예상 시간**: 약 20-61초

#### research.log (약 24,000줄)
- **예상 시간**: 약 8-24초

## 성능 최적화 방법

### 1. 병렬 처리
```python
# 멀티프로세싱으로 여러 파일 동시 처리
from multiprocessing import Pool

def process_file(log_file):
    preprocessor = LogPreprocessor(config)
    return preprocessor.process_log_file(log_file)

with Pool(processes=4) as pool:
    results = pool.map(process_file, log_files)
```

**예상 개선**: 4코어 기준 약 2-3배 속도 향상

### 2. Drain3 설정 최적화
```yaml
# drain3_config.yaml
drain_sim_th: 0.5        # 0.4 → 0.5 (더 빠르지만 덜 정확)
drain_depth: 3           # 4 → 3 (더 빠름)
```

**예상 개선**: 약 20-30% 속도 향상

### 3. 배치 처리
- 큰 파일을 청크 단위로 분할 처리
- 메모리 사용량 감소

### 4. 관계 추적 비활성화
```yaml
# preprocessing_config.yaml
enable_correlation: false  # 관계 추적 비활성화
```

**예상 개선**: 약 10-20% 속도 향상

## 실제 측정 방법

### 벤치마크 스크립트

```python
import time
from log_preprocessor import LogPreprocessor, load_config

config = load_config()
preprocessor = LogPreprocessor(config)

# 샘플 파일로 측정
test_file = "logs/manager.log"
start_time = time.time()

sessions = preprocessor.process_log_file(test_file, stream_mode=True)

elapsed = time.time() - start_time
print(f"처리 시간: {elapsed:.2f}초")
print(f"처리 속도: {line_count/elapsed:.0f}줄/초")
```

## 권장 사항

### 대용량 로그 처리 전략

1. **샘플링 처리**
   - 전체 로그의 일부만 먼저 처리하여 패턴 확인
   - 예: 최근 1일치 로그만 처리

2. **배치 처리**
   - 날짜별 또는 파일별로 나누어 처리
   - 예: 하루치씩 처리

3. **병렬 처리**
   - 여러 파일을 동시에 처리
   - CPU 코어 수만큼 프로세스 생성

4. **증분 처리**
   - 이미 처리된 로그는 건너뛰기
   - 새로운 로그만 처리

### 예상 처리 계획

#### 옵션 1: 전체 처리 (권장하지 않음)
- **소요 시간**: 약 2-4일
- **장점**: 완전한 데이터셋
- **단점**: 시간이 너무 오래 걸림

#### 옵션 2: 샘플링 처리 (권장)
- **방법**: 최근 1주일치 또는 1개월치만 처리
- **소요 시간**: 약 2-8시간
- **장점**: 빠른 결과 확인

#### 옵션 3: 배치 처리 (권장)
- **방법**: 날짜별로 나누어 처리
- **소요 시간**: 파일당 1-2시간
- **장점**: 점진적 처리, 중단 가능

## 모니터링

### 진행 상황 확인

```python
# 로그 파일 처리 중 진행률 출력
logger.info(f"처리 완료: {line_count}줄, {session_count}개 세션 생성")
```

### 예상 남은 시간 계산

```
처리된 줄 수: 10,000줄
소요 시간: 5초
처리 속도: 2,000줄/초

남은 줄 수: 347,976,016줄
예상 남은 시간: 347,976,016 ÷ 2,000 = 173,988초 ≈ 48시간
```

## 결론

### 현재 데이터 규모 기준
- **예상 소요 시간**: 약 **2-4일** (전체 처리 시)
- **권장 방법**: 샘플링 또는 배치 처리
- **최적화 후**: 약 **1-2일** (병렬 처리 + 설정 최적화)

### 실용적 접근
1. 먼저 작은 샘플(1일치)로 테스트
2. 처리 속도 확인 후 전체 처리 계획 수립
3. 필요시 병렬 처리 및 설정 최적화 적용

