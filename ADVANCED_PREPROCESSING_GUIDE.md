# ê³ ê¸‰ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” **LogBERT, DeepLog, LogLSTM, LogTCN** ëª¨ë¸ì˜ ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ ìƒì„¸í•˜ê³  ì •í™•ë„ê°€ ë†’ì€ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

### ëª©í‘œ
- âœ… **ìµœê³  ì •í™•ë„**: ëª¨ë¸ ì„±ëŠ¥ ìµœëŒ€í™”
- âœ… **ìƒì„¸í•œ íŠ¹ì§• ì¶”ì¶œ**: ì‹œê°„ì  íŒ¨í„´, ì„œë¹„ìŠ¤ ì˜ì¡´ì„±, ì—ëŸ¬ ì „íŒŒ ë“±
- âœ… **ë°ì´í„° í’ˆì§ˆ ìµœìš°ì„ **: ë…¸ì´ì¦ˆ ì œê±°, ì´ìƒì¹˜ ì²˜ë¦¬, ë°ì´í„° ê²€ì¦
- âœ… **ëª¨ë¸ë³„ ìµœì í™”**: ê° ëª¨ë¸ì˜ íŠ¹ì„±ì— ë§ëŠ” ì „ì²˜ë¦¬

### ì›ì¹™
- **ë°ì´í„°ëŠ” ìµœëŒ€í•œ í™œìš©**: ê°€ëŠ¥í•œ ëª¨ë“  ë¡œê·¸ ë°ì´í„° ì‚¬ìš©
- **ì „ì²˜ë¦¬ ì‹œê°„/ìì› ë¬´ì œí•œ**: ì •í™•ë„ ìš°ì„ 
- **ë³µì¡ë„ í—ˆìš©**: ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©

---

## ğŸ—ï¸ ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```
ì›ë³¸ ë¡œê·¸ íŒŒì¼ë“¤ (ì„œë¹„ìŠ¤ë³„)
    â†“
[1ë‹¨ê³„] ë¡œê·¸ ì •ë¦¬ ë° ê³ ê¸‰ íŒŒì‹±
    â†“
[2ë‹¨ê³„] Trace ID ì¶”ì¶œ ë° ê²€ì¦
    â†“
[3ë‹¨ê³„] ë‹¤ì¤‘ ì„¸ì…˜í™” ì „ëµ
    â†“
[4ë‹¨ê³„] ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
    â†“
[5ë‹¨ê³„] MSA ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©
    â†“
[6ë‹¨ê³„] ëª¨ë¸ë³„ ìµœì í™” ì¸ì½”ë”©
    â†“
[7ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§
    â†“
[8ë‹¨ê³„] ë°ì´í„° ì¦ê°• (ì„ íƒ)
    â†“
ì „ì²˜ë¦¬ëœ ë°ì´í„° (ëª¨ë¸ë³„ ìµœì í™”)
```

---

## ğŸ“ ë‹¨ê³„ë³„ ìƒì„¸ ì „ì²˜ë¦¬ ë°©ë²•

### 1ë‹¨ê³„: ë¡œê·¸ ì •ë¦¬ ë° ê³ ê¸‰ íŒŒì‹±

#### 1.1 ë‹¤ì¸µ ë¡œê·¸ ì •ë¦¬ (Multi-layer Cleaning)

**ëª©ì **: ëª¨ë“  ë…¸ì´ì¦ˆì™€ ë¶ˆí•„ìš”í•œ ë°ì´í„° ì œê±°

```python
class AdvancedLogCleaner:
    """ê³ ê¸‰ ë¡œê·¸ ì •ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Spring Boot ë°°ë„ˆ íŒ¨í„´ (í™•ì¥)
        self.banner_patterns = [
            r'\.\s+____.*?Spring Boot.*?::',
            r':: Spring Boot ::',
            r'-----------------------------------------------------------------------------------------',
            r'Started .*? in \d+\.\d+ seconds',
            r'Running Spring Boot',
            r'Spring Boot version',
        ]
        
        # ë¹ˆ ì¤„ ë° ê³µë°± íŒ¨í„´
        self.empty_patterns = [
            r'^\s*$',  # ë¹ˆ ì¤„
            r'^\s+$',  # ê³µë°±ë§Œ
            r'^---+$',  # êµ¬ë¶„ì„ 
        ]
        
        # ë¶ˆí•„ìš”í•œ ë¡œê·¸ íŒ¨í„´
        self.noise_patterns = [
            r'^DEBUG.*?org\.springframework\.',  # Spring ë‚´ë¶€ ë””ë²„ê·¸
            r'^TRACE.*?',  # TRACE ë ˆë²¨ (ë„ˆë¬´ ìƒì„¸)
            r'^.*?\.(jar|class) loaded$',  # í´ë˜ìŠ¤ ë¡œë”©
        ]
    
    def clean_log_line(self, line: str) -> Optional[str]:
        """ë‹¤ì¸µ ì •ë¦¬"""
        # 1. ì¸ì½”ë”© ì •ê·œí™”
        line = self._normalize_encoding(line)
        
        # 2. ë°°ë„ˆ ì œê±°
        for pattern in self.banner_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                return None
        
        # 3. ë¹ˆ ì¤„ ì œê±°
        for pattern in self.empty_patterns:
            if re.match(pattern, line):
                return None
        
        # 4. ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
        for pattern in self.noise_patterns:
            if re.match(pattern, line, re.IGNORECASE):
                return None
        
        # 5. ì•ë’¤ ê³µë°± ì œê±°
        line = line.strip()
        
        # 6. ìµœì†Œ ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ì€ ë¡œê·¸ ì œê±°)
        if len(line) < 10:
            return None
        
        return line
    
    def _normalize_encoding(self, line: str) -> str:
        """ì¸ì½”ë”© ì •ê·œí™”"""
        # UTF-8ë¡œ í†µì¼
        try:
            line = line.encode('utf-8', errors='ignore').decode('utf-8')
        except:
            pass
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”
        line = line.replace('\x00', '')  # NULL ë¬¸ì ì œê±°
        line = line.replace('\r\n', '\n')  # ì¤„ë°”ê¿ˆ í†µì¼
        
        return line
```

#### 1.2 ê³ ê¸‰ ë¡œê·¸ íŒŒì‹± (Drain3 ìµœì í™”)

**ëª©ì **: ì •í™•í•œ í…œí”Œë¦¿ ì¶”ì¶œ ë° íŒŒë¼ë¯¸í„° ë¶„ë¦¬

```python
class AdvancedLogParser:
    """ê³ ê¸‰ ë¡œê·¸ íŒŒì„œ (Drain3 ìµœì í™”)"""
    
    def __init__(self, drain3_config_path: str):
        # Drain3 ì„¤ì • ìµœì í™”
        self.drain3_config = {
            'depth': 4,  # íŠ¸ë¦¬ ê¹Šì´ ì¦ê°€ (ë” ì •êµí•œ íŒŒì‹±)
            'st': 0.5,  # ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì¶”ë©´ ë” ì„¸ë°€í•˜ê²Œ)
            'max_children': 100,  # ìµœëŒ€ ìì‹ ë…¸ë“œ ìˆ˜ ì¦ê°€
            'max_clusters': None,  # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì œí•œ ì—†ìŒ
        }
        
        self.parser = Drain3(
            config=self.drain3_config,
            persistence_handler=FilePersistenceHandler(drain3_config_path)
        )
        
        # Event ID ë§¤í•‘
        self.event_id_map = {}  # template -> event_id
        self.next_event_id = 1
    
    def parse_log(self, log_line: str, service_name: str) -> Optional[Dict]:
        """ê³ ê¸‰ íŒŒì‹±"""
        try:
            # Drain3 íŒŒì‹±
            result = self.parser.parse(log_line)
            
            if not result:
                return None
            
            template = result.get('template', '')
            parameters = result.get('parameters', [])
            
            # Event ID í• ë‹¹
            if template not in self.event_id_map:
                self.event_id_map[template] = self.next_event_id
                self.next_event_id += 1
            
            event_id = self.event_id_map[template]
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (ì •êµí•˜ê²Œ)
            timestamp = self._extract_timestamp_advanced(log_line)
            
            # ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ (ì •êµí•˜ê²Œ)
            level = self._extract_level_advanced(log_line)
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_metadata(log_line, service_name)
            
            return {
                'original': log_line,
                'template': template,
                'parameters': parameters,
                'event_id': event_id,
                'timestamp': timestamp,
                'level': level,
                'service_name': service_name,
                'metadata': metadata,
                'parameter_count': len(parameters),
                'template_length': len(template),
            }
        
        except Exception as e:
            logger.debug(f"íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_timestamp_advanced(self, log_line: str) -> Optional[datetime]:
        """ê³ ê¸‰ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì§€ì›
        timestamp_patterns = [
            # Spring Boot í‘œì¤€ í˜•ì‹
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3})',
            # ISO 8601 í˜•ì‹
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3})',
            # Unix íƒ€ì„ìŠ¤íƒ¬í”„
            r'(\d{10}\.\d{3})',
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, log_line)
            if match:
                timestamp_str = match.group(1)
                try:
                    # Spring Boot í˜•ì‹
                    if '.' in timestamp_str and 'T' not in timestamp_str:
                        return datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f")
                    # ISO 8601 í˜•ì‹
                    elif 'T' in timestamp_str:
                        return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    # Unix íƒ€ì„ìŠ¤íƒ¬í”„
                    else:
                        return datetime.fromtimestamp(float(timestamp_str))
                except:
                    continue
        
        return None
    
    def _extract_level_advanced(self, log_line: str) -> str:
        """ê³ ê¸‰ ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ"""
        # ì •ê·œì‹ íŒ¨í„´ (ìˆœì„œ ì¤‘ìš”)
        level_patterns = [
            (r'\bERROR\b', 'ERROR'),
            (r'\bWARN\b', 'WARN'),
            (r'\bWARNING\b', 'WARN'),
            (r'\bINFO\b', 'INFO'),
            (r'\bDEBUG\b', 'DEBUG'),
            (r'\bTRACE\b', 'TRACE'),
        ]
        
        for pattern, level in level_patterns:
            if re.search(pattern, log_line, re.IGNORECASE):
                return level
        
        return 'UNKNOWN'
    
    def _extract_metadata(self, log_line: str, service_name: str) -> Dict:
        """ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        # ìŠ¤ë ˆë“œëª… ì¶”ì¶œ
        thread_match = re.search(r'\[([^\]]+)\]', log_line)
        if thread_match:
            metadata['thread'] = thread_match.group(1)
        
        # HTTP ë©”ì„œë“œ ì¶”ì¶œ
        http_methods = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH']
        for method in http_methods:
            if method in log_line:
                metadata['http_method'] = method
                break
        
        # HTTP ìƒíƒœ ì½”ë“œ ì¶”ì¶œ
        status_match = re.search(r'status[=:](\d{3})', log_line, re.IGNORECASE)
        if status_match:
            metadata['http_status'] = int(status_match.group(1))
        
        # IP ì£¼ì†Œ ì¶”ì¶œ
        ip_match = re.search(r'\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\b', log_line)
        if ip_match:
            metadata['ip_address'] = ip_match.group(1)
        
        # URL ì¶”ì¶œ
        url_match = re.search(r'(https?://[^\s]+|/[^\s]+)', log_line)
        if url_match:
            metadata['url'] = url_match.group(1)
        
        return metadata
```

---

### 2ë‹¨ê³„: Trace ID ì¶”ì¶œ ë° ê²€ì¦

#### 2.1 ë‹¤ì¤‘ ë°©ë²• Trace ID ì¶”ì¶œ

**ëª©ì **: ëª¨ë“  ê°€ëŠ¥í•œ ë°©ë²•ìœ¼ë¡œ Trace ID ì¶”ì¶œ

```python
class AdvancedTraceExtractor:
    """ê³ ê¸‰ Trace ID ì¶”ì¶œê¸°"""
    
    def __init__(self):
        # Trace ID íŒ¨í„´ (í™•ì¥)
        self.trace_patterns = [
            # JSON í˜•ì‹
            (r'"trace_id"\s*:\s*"([^"]+)"', 'json'),
            (r'"traceId"\s*:\s*"([^"]+)"', 'json'),
            (r'"X-Trace-Id"\s*:\s*"([^"]+)"', 'json'),
            (r'"correlationId"\s*:\s*"([^"]+)"', 'json'),
            
            # HTTP í—¤ë” í˜•ì‹
            (r'X-Trace-Id[:\s]+([a-zA-Z0-9-]+)', 'http'),
            (r'trace_id[:\s]+([a-zA-Z0-9-]+)', 'http'),
            (r'traceId[:\s]+([a-zA-Z0-9-]+)', 'http'),
            
            # Spring Cloud Sleuth í˜•ì‹
            (r'\[([a-zA-Z0-9-]+),', 'sleuth'),
            (r'\[([a-zA-Z0-9]{16,32})\]', 'sleuth'),
            
            # UUID í˜•ì‹
            (r'([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', 'uuid'),
            
            # 16ì§„ìˆ˜ í˜•ì‹ (32ì)
            (r'([0-9a-f]{32})', 'hex'),
            
            # ìˆ«ì í˜•ì‹ (ê¸´ ìˆ«ì)
            (r'\b(\d{16,32})\b', 'numeric'),
        ]
    
    def extract_trace_id(self, log_line: str, service_name: str) -> Optional[Dict]:
        """ë‹¤ì¤‘ ë°©ë²•ìœ¼ë¡œ Trace ID ì¶”ì¶œ ë° ê²€ì¦"""
        candidates = []
        
        # ëª¨ë“  íŒ¨í„´ ì‹œë„
        for pattern, source in self.trace_patterns:
            matches = re.finditer(pattern, log_line, re.IGNORECASE)
            for match in matches:
                trace_id = match.group(1)
                
                # Trace ID ê²€ì¦
                if self._validate_trace_id(trace_id):
                    candidates.append({
                        'trace_id': trace_id,
                        'source': source,
                        'confidence': self._calculate_confidence(trace_id, source),
                        'position': match.start()
                    })
        
        if not candidates:
            return None
        
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ Trace ID ì„ íƒ
        best = max(candidates, key=lambda x: x['confidence'])
        
        return {
            'trace_id': best['trace_id'],
            'source': best['source'],
            'confidence': best['confidence'],
            'all_candidates': candidates  # ëª¨ë“  í›„ë³´ ì €ì¥
        }
    
    def _validate_trace_id(self, trace_id: str) -> bool:
        """Trace ID ìœ íš¨ì„± ê²€ì¦"""
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ì œì™¸
        if len(trace_id) < 8 or len(trace_id) > 64:
            return False
        
        # íŠ¹ìˆ˜ ë¬¸ì ì œì™¸ (ì¼ë¶€ í—ˆìš©)
        if re.search(r'[^a-zA-Z0-9\-_]', trace_id):
            return False
        
        return True
    
    def _calculate_confidence(self, trace_id: str, source: str) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜
        source_weights = {
            'json': 1.0,
            'sleuth': 0.9,
            'http': 0.8,
            'uuid': 0.9,
            'hex': 0.7,
            'numeric': 0.6,
        }
        confidence *= source_weights.get(source, 0.5)
        
        # ê¸¸ì´ë³„ ê°€ì¤‘ì¹˜ (16-32ìì¼ ë•Œ ìµœê³ )
        length = len(trace_id)
        if 16 <= length <= 32:
            confidence *= 1.0
        elif 8 <= length < 16 or 32 < length <= 64:
            confidence *= 0.9
        else:
            confidence *= 0.7
        
        # í˜•ì‹ë³„ ê°€ì¤‘ì¹˜ (UUID í˜•ì‹ì´ë©´ ë†’ìŒ)
        if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', trace_id, re.IGNORECASE):
            confidence *= 1.1
        
        return min(confidence, 1.0)
```

#### 2.2 Trace ID ì—°ê²° ë° ê²€ì¦

```python
class TraceValidator:
    """Trace ID ê²€ì¦ ë° ì—°ê²° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.trace_registry = {}  # trace_id -> metadata
        self.service_traces = defaultdict(set)  # service -> trace_ids
    
    def register_trace(self, trace_id: str, service_name: str, timestamp: datetime):
        """Trace ID ë“±ë¡"""
        if trace_id not in self.trace_registry:
            self.trace_registry[trace_id] = {
                'services': set(),
                'first_seen': timestamp,
                'last_seen': timestamp,
                'log_count': 0,
            }
        
        self.trace_registry[trace_id]['services'].add(service_name)
        self.trace_registry[trace_id]['last_seen'] = max(
            self.trace_registry[trace_id]['last_seen'],
            timestamp
        )
        self.trace_registry[trace_id]['log_count'] += 1
        self.service_traces[service_name].add(trace_id)
    
    def validate_trace(self, trace_id: str) -> Dict:
        """Trace ID ê²€ì¦"""
        if trace_id not in self.trace_registry:
            return {
                'valid': False,
                'reason': 'not_found'
            }
        
        metadata = self.trace_registry[trace_id]
        
        # ê²€ì¦ ê·œì¹™
        validations = {
            'valid': True,
            'service_count': len(metadata['services']),
            'log_count': metadata['log_count'],
            'duration': (metadata['last_seen'] - metadata['first_seen']).total_seconds(),
            'services': list(metadata['services']),
        }
        
        # ì´ìƒì¹˜ ê²€ì¶œ
        if validations['service_count'] > 10:  # ë„ˆë¬´ ë§ì€ ì„œë¹„ìŠ¤
            validations['warning'] = 'too_many_services'
        
        if validations['duration'] > 3600:  # 1ì‹œê°„ ì´ìƒ
            validations['warning'] = 'too_long_duration'
        
        return validations
```

---

### 3ë‹¨ê³„: ë‹¤ì¤‘ ì„¸ì…˜í™” ì „ëµ

#### 3.1 í•˜ì´ë¸Œë¦¬ë“œ ì„¸ì…˜í™”

**ëª©ì **: Trace ID ê¸°ë°˜ + ì‹œê°„ ê¸°ë°˜ + ì„œë¹„ìŠ¤ ê¸°ë°˜ ì„¸ì…˜í™”

```python
class HybridSessionizer:
    """í•˜ì´ë¸Œë¦¬ë“œ ì„¸ì…˜í™” í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        trace_window_time: int = 600,  # 10ë¶„
        service_window_size: int = 50,
        service_window_time: int = 300,  # 5ë¶„
        sliding_window_size: int = 20,
        sliding_window_time: int = 180,  # 3ë¶„
    ):
        # Trace ID ê¸°ë°˜ ì„¸ì…˜
        self.trace_sessions = defaultdict(list)  # trace_id -> logs
        
        # ì„œë¹„ìŠ¤ë³„ ì„¸ì…˜ (Trace ID ì—†ì„ ë•Œ)
        self.service_sessions = defaultdict(lambda: deque(maxlen=service_window_size))
        
        # Sliding Window ì„¸ì…˜
        self.sliding_windows = defaultdict(lambda: deque(maxlen=sliding_window_size))
        
        # ì‹œê°„ ìœˆë„ìš°
        self.trace_window_time = trace_window_time
        self.service_window_time = service_window_time
        self.sliding_window_time = sliding_window_time
    
    def add_log(
        self,
        parsed_log: Dict,
        trace_id: Optional[str],
        service_name: str,
        timestamp: datetime
    ) -> List[Dict]:
        """ë¡œê·¸ ì¶”ê°€ ë° ì„¸ì…˜ ìƒì„±"""
        sessions = []
        
        # 1. Trace ID ê¸°ë°˜ ì„¸ì…˜í™” (ìµœìš°ì„ )
        if trace_id:
            trace_session = self._add_to_trace_session(
                parsed_log, trace_id, service_name, timestamp
            )
            if trace_session:
                sessions.append(trace_session)
        
        # 2. ì„œë¹„ìŠ¤ë³„ ì„¸ì…˜í™”
        service_session = self._add_to_service_session(
            parsed_log, service_name, timestamp
        )
        if service_session:
            sessions.append(service_session)
        
        # 3. Sliding Window ì„¸ì…˜í™”
        sliding_session = self._add_to_sliding_window(
            parsed_log, service_name, timestamp
        )
        if sliding_session:
            sessions.append(sliding_session)
        
        return sessions
    
    def _add_to_trace_session(
        self,
        parsed_log: Dict,
        trace_id: str,
        service_name: str,
        timestamp: datetime
    ) -> Optional[Dict]:
        """Trace ID ê¸°ë°˜ ì„¸ì…˜ì— ì¶”ê°€"""
        key = f"{trace_id}_{service_name}"
        
        # ì‹œê°„ ìœˆë„ìš° í™•ì¸
        if key in self.trace_sessions:
            first_log = self.trace_sessions[key][0]
            first_time = first_log.get('timestamp')
            
            if first_time and (timestamp - first_time).total_seconds() > self.trace_window_time:
                # ìœˆë„ìš° ì´ˆê³¼ ì‹œ ì„¸ì…˜ ì™„ì„±
                session = self._create_trace_session(self.trace_sessions[key], trace_id, service_name)
                self.trace_sessions[key] = []
                return session
        
        # ë¡œê·¸ ì¶”ê°€
        self.trace_sessions[key].append({
            **parsed_log,
            'trace_id': trace_id,
            'service_name': service_name,
            'timestamp': timestamp
        })
        
        # ì„¸ì…˜ í¬ê¸° í™•ì¸
        if len(self.trace_sessions[key]) >= 100:  # ìµœëŒ€ í¬ê¸°
            session = self._create_trace_session(self.trace_sessions[key], trace_id, service_name)
            self.trace_sessions[key] = []
            return session
        
        return None
    
    def _add_to_service_session(
        self,
        parsed_log: Dict,
        service_name: str,
        timestamp: datetime
    ) -> Optional[Dict]:
        """ì„œë¹„ìŠ¤ë³„ ì„¸ì…˜ì— ì¶”ê°€"""
        buffer = self.service_sessions[service_name]
        
        # ì‹œê°„ ìœˆë„ìš° í™•ì¸
        if buffer:
            first_time = buffer[0].get('timestamp')
            if first_time and (timestamp - first_time).total_seconds() > self.service_window_time:
                # ìœˆë„ìš° ì´ˆê³¼ ì‹œ ì„¸ì…˜ ì™„ì„±
                session = self._create_service_session(list(buffer), service_name)
                buffer.clear()
                return session
        
        # ë¡œê·¸ ì¶”ê°€
        buffer.append({
            **parsed_log,
            'service_name': service_name,
            'timestamp': timestamp
        })
        
        # ì„¸ì…˜ í¬ê¸° í™•ì¸
        if len(buffer) >= 50:  # ìµœëŒ€ í¬ê¸°
            session = self._create_service_session(list(buffer), service_name)
            buffer.clear()
            return session
        
        return None
    
    def _add_to_sliding_window(
        self,
        parsed_log: Dict,
        service_name: str,
        timestamp: datetime
    ) -> Optional[Dict]:
        """Sliding Windowì— ì¶”ê°€"""
        buffer = self.sliding_windows[service_name]
        
        # ì‹œê°„ ìœˆë„ìš° í™•ì¸
        if buffer:
            first_time = buffer[0].get('timestamp')
            if first_time and (timestamp - first_time).total_seconds() > self.sliding_window_time:
                buffer.popleft()
        
        # ë¡œê·¸ ì¶”ê°€
        buffer.append({
            **parsed_log,
            'service_name': service_name,
            'timestamp': timestamp
        })
        
        # ìœˆë„ìš°ê°€ ê°€ë“ ì°¼ì„ ë•Œ ì„¸ì…˜ ìƒì„±
        if len(buffer) >= 20:
            session = self._create_sliding_session(list(buffer), service_name)
            return session
        
        return None
    
    def _create_trace_session(self, logs: List[Dict], trace_id: str, service_name: str) -> Dict:
        """Trace ì„¸ì…˜ ìƒì„±"""
        logs.sort(key=lambda x: x.get('timestamp', datetime.min))
        
        return {
            'session_type': 'trace',
            'session_id': f"{trace_id}_{service_name}_{logs[0].get('timestamp', '').timestamp()}",
            'trace_id': trace_id,
            'service_name': service_name,
            'logs': logs,
            'log_count': len(logs),
            'time_span': (logs[-1].get('timestamp') - logs[0].get('timestamp')).total_seconds() if len(logs) > 1 else 0,
            'has_error': any(log.get('level') == 'ERROR' for log in logs),
            'has_warn': any(log.get('level') == 'WARN' for log in logs),
        }
    
    def _create_service_session(self, logs: List[Dict], service_name: str) -> Dict:
        """ì„œë¹„ìŠ¤ ì„¸ì…˜ ìƒì„±"""
        logs.sort(key=lambda x: x.get('timestamp', datetime.min))
        
        return {
            'session_type': 'service',
            'session_id': f"{service_name}_{logs[0].get('timestamp', '').timestamp()}",
            'service_name': service_name,
            'logs': logs,
            'log_count': len(logs),
            'time_span': (logs[-1].get('timestamp') - logs[0].get('timestamp')).total_seconds() if len(logs) > 1 else 0,
            'has_error': any(log.get('level') == 'ERROR' for log in logs),
            'has_warn': any(log.get('level') == 'WARN' for log in logs),
        }
    
    def _create_sliding_session(self, logs: List[Dict], service_name: str) -> Dict:
        """Sliding Window ì„¸ì…˜ ìƒì„±"""
        logs.sort(key=lambda x: x.get('timestamp', datetime.min))
        
        return {
            'session_type': 'sliding',
            'session_id': f"{service_name}_sliding_{logs[0].get('timestamp', '').timestamp()}",
            'service_name': service_name,
            'logs': logs,
            'log_count': len(logs),
            'time_span': (logs[-1].get('timestamp') - logs[0].get('timestamp')).total_seconds() if len(logs) > 1 else 0,
            'has_error': any(log.get('level') == 'ERROR' for log in logs),
            'has_warn': any(log.get('level') == 'WARN' for log in logs),
        }
```

---

### 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ

#### 4.1 ì‹œê°„ì  íŒ¨í„´ íŠ¹ì§•

```python
class TemporalFeatureExtractor:
    """ì‹œê°„ì  íŒ¨í„´ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def extract_features(self, session: Dict) -> Dict:
        """ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ"""
        logs = session.get('logs', [])
        
        if not logs:
            return {}
        
        timestamps = [log.get('timestamp') for log in logs if log.get('timestamp')]
        timestamps = [ts for ts in timestamps if ts]
        
        if len(timestamps) < 2:
            return {}
        
        # ì‹œê°„ ê°„ê²© ê³„ì‚°
        intervals = []
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i-1]).total_seconds()
            intervals.append(interval)
        
        # í†µê³„ íŠ¹ì§•
        features = {
            'time_span': (timestamps[-1] - timestamps[0]).total_seconds(),
            'mean_interval': np.mean(intervals) if intervals else 0,
            'std_interval': np.std(intervals) if intervals else 0,
            'min_interval': np.min(intervals) if intervals else 0,
            'max_interval': np.max(intervals) if intervals else 0,
            'median_interval': np.median(intervals) if intervals else 0,
            'interval_variance': np.var(intervals) if intervals else 0,
        }
        
        # ì‹œê°„ëŒ€ íŠ¹ì§•
        first_hour = timestamps[0].hour
        features['hour_of_day'] = first_hour
        features['is_business_hours'] = 9 <= first_hour <= 18
        features['is_night'] = first_hour < 6 or first_hour > 22
        
        # ì£¼ê¸°ì„± íŠ¹ì§• (FFT)
        if len(intervals) >= 8:
            fft_values = np.fft.fft(intervals)
            features['fft_dominant_freq'] = np.argmax(np.abs(fft_values[1:len(fft_values)//2])) + 1
            features['fft_power'] = np.sum(np.abs(fft_values)**2)
        
        return features
```

#### 4.2 ì„œë¹„ìŠ¤ ì˜ì¡´ì„± íŠ¹ì§•

```python
class DependencyFeatureExtractor:
    """ì„œë¹„ìŠ¤ ì˜ì¡´ì„± íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def extract_features(self, trace_session: Dict) -> Dict:
        """ì˜ì¡´ì„± íŠ¹ì§• ì¶”ì¶œ"""
        services = trace_session.get('services', {})
        service_order = list(services.keys())
        
        features = {
            'service_count': len(services),
            'service_diversity': len(set(service_order)),
            'has_gateway': 'gateway' in service_order,
            'has_eureka': 'eureka' in service_order,
        }
        
        # ì„œë¹„ìŠ¤ í˜¸ì¶œ ìˆœì„œ íŠ¹ì§•
        if len(service_order) > 1:
            features['service_chain_length'] = len(service_order)
            features['service_chain'] = '->'.join(service_order)
            
            # Gatewayê°€ ì²« ë²ˆì§¸ì¸ì§€
            features['gateway_first'] = service_order[0] == 'gateway'
            
            # ì„œë¹„ìŠ¤ ê¹Šì´ (í˜¸ì¶œ ì²´ì¸ ê¹Šì´)
            features['max_depth'] = self._calculate_depth(services)
        
        # ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ìˆ˜
        service_log_counts = {svc: len(logs) for svc, logs in services.items()}
        features['service_log_distribution'] = service_log_counts
        features['max_service_logs'] = max(service_log_counts.values()) if service_log_counts else 0
        features['min_service_logs'] = min(service_log_counts.values()) if service_log_counts else 0
        
        return features
    
    def _calculate_depth(self, services: Dict) -> int:
        """ì„œë¹„ìŠ¤ í˜¸ì¶œ ê¹Šì´ ê³„ì‚°"""
        # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” í˜¸ì¶œ ê·¸ë˜í”„ ë¶„ì„)
        if 'gateway' in services:
            return len(services) - 1  # Gateway ì œì™¸
        return len(services)
```

#### 4.3 ì—ëŸ¬ ì „íŒŒ íŠ¹ì§•

```python
class ErrorPropagationExtractor:
    """ì—ëŸ¬ ì „íŒŒ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def extract_features(self, trace_session: Dict) -> Dict:
        """ì—ëŸ¬ ì „íŒŒ íŠ¹ì§• ì¶”ì¶œ"""
        services = trace_session.get('services', {})
        service_order = list(services.keys())
        
        features = {
            'error_count': 0,
            'warning_count': 0,
            'error_services': [],
            'warning_services': [],
            'error_propagation': False,
            'error_chain': [],
        }
        
        # ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ ì¶”ì¶œ
        for service in service_order:
            logs = services[service]
            errors = [log for log in logs if log.get('level') == 'ERROR']
            warnings = [log for log in logs if log.get('level') == 'WARN']
            
            if errors:
                features['error_count'] += len(errors)
                features['error_services'].append(service)
                features['error_chain'].append(f"{service}:ERROR")
            
            if warnings:
                features['warning_count'] += len(warnings)
                features['warning_services'].append(service)
                if not errors:  # ì—ëŸ¬ê°€ ì—†ì„ ë•Œë§Œ ê²½ê³  ì²´ì¸ì— ì¶”ê°€
                    features['error_chain'].append(f"{service}:WARN")
        
        # ì—ëŸ¬ ì „íŒŒ í™•ì¸
        if len(features['error_services']) > 1:
            features['error_propagation'] = True
        
        # ì—ëŸ¬ ì²´ì¸ ë¬¸ìì—´
        features['error_chain_str'] = '->'.join(features['error_chain'])
        
        return features
```

---

### 5ë‹¨ê³„: MSA ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©

```python
class AdvancedMSAContextBuilder:
    """ê³ ê¸‰ MSA ì»¨í…ìŠ¤íŠ¸ ë¹Œë”"""
    
    def build_context(self, trace_session: Dict) -> Dict:
        """ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©"""
        services = trace_session.get('services', {})
        
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
        context = {
            'trace_id': trace_session.get('trace_id'),
            'services': services,
            'service_count': len(services),
            'service_order': list(services.keys()),
        }
        
        # ì„œë¹„ìŠ¤ í˜¸ì¶œ ê·¸ë˜í”„ ìƒì„±
        context['call_graph'] = self._build_call_graph(services)
        
        # ì„œë¹„ìŠ¤ ê°„ ì˜ì¡´ì„± ë¶„ì„
        context['dependencies'] = self._analyze_dependencies(services)
        
        # ì „ì²´ ìš”ì²­ íë¦„ ì¶”ì 
        context['request_flow'] = self._trace_request_flow(services)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        context['performance_metrics'] = self._calculate_performance_metrics(services)
        
        # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸
        context['error_context'] = self._build_error_context(services)
        
        return context
    
    def _build_call_graph(self, services: Dict) -> Dict:
        """í˜¸ì¶œ ê·¸ë˜í”„ ìƒì„±"""
        graph = {}
        
        for service, logs in services.items():
            called_services = set()
            
            for log in logs:
                # URLì—ì„œ í˜¸ì¶œëœ ì„œë¹„ìŠ¤ ì¶”ì¶œ
                url = log.get('metadata', {}).get('url', '')
                if url:
                    for target_service in ['research', 'manager', 'code', 'user', 'eureka']:
                        if f'/{target_service}/' in url.lower():
                            called_services.add(target_service)
            
            graph[service] = list(called_services)
        
        return graph
    
    def _analyze_dependencies(self, services: Dict) -> Dict:
        """ì˜ì¡´ì„± ë¶„ì„"""
        dependencies = {}
        
        for service, logs in services.items():
            deps = {
                'depends_on': [],
                'depended_by': [],
                'dependency_count': 0,
            }
            
            # í˜¸ì¶œí•œ ì„œë¹„ìŠ¤ ì°¾ê¸°
            for log in logs:
                url = log.get('metadata', {}).get('url', '')
                if url:
                    for target_service in ['research', 'manager', 'code', 'user']:
                        if f'/{target_service}/' in url.lower() and target_service not in deps['depends_on']:
                            deps['depends_on'].append(target_service)
            
            dependencies[service] = deps
            dependencies[service]['dependency_count'] = len(deps['depends_on'])
        
        # ì—­ë°©í–¥ ì˜ì¡´ì„± ê³„ì‚°
        for service, deps in dependencies.items():
            for other_service, other_deps in dependencies.items():
                if service != other_service and service in other_deps['depends_on']:
                    deps['depended_by'].append(other_service)
        
        return dependencies
    
    def _trace_request_flow(self, services: Dict) -> List[Dict]:
        """ìš”ì²­ íë¦„ ì¶”ì """
        flow = []
        
        # ëª¨ë“  ë¡œê·¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        all_logs = []
        for service, logs in services.items():
            for log in logs:
                all_logs.append({
                    **log,
                    'service': service
                })
        
        all_logs.sort(key=lambda x: x.get('timestamp', datetime.min))
        
        # íë¦„ ìƒì„±
        for log in all_logs:
            flow.append({
                'timestamp': log.get('timestamp'),
                'service': log.get('service'),
                'level': log.get('level'),
                'template': log.get('template'),
                'event_id': log.get('event_id'),
            })
        
        return flow
    
    def _calculate_performance_metrics(self, services: Dict) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        for service, logs in services.items():
            timestamps = [log.get('timestamp') for log in logs if log.get('timestamp')]
            
            if len(timestamps) >= 2:
                duration = (timestamps[-1] - timestamps[0]).total_seconds()
                metrics[service] = {
                    'duration': duration,
                    'log_count': len(logs),
                    'logs_per_second': len(logs) / duration if duration > 0 else 0,
                }
        
        return metrics
    
    def _build_error_context(self, services: Dict) -> Dict:
        """ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©"""
        error_context = {
            'has_error': False,
            'error_services': [],
            'error_messages': [],
            'error_templates': [],
            'first_error_service': None,
            'error_propagation_path': [],
        }
        
        for service, logs in services.items():
            errors = [log for log in logs if log.get('level') == 'ERROR']
            
            if errors:
                error_context['has_error'] = True
                error_context['error_services'].append(service)
                
                for error in errors:
                    error_context['error_messages'].append(error.get('original', ''))
                    error_context['error_templates'].append(error.get('template', ''))
        
        if error_context['error_services']:
            error_context['first_error_service'] = error_context['error_services'][0]
            error_context['error_propagation_path'] = error_context['error_services']
        
        return error_context
```

---

### 6ë‹¨ê³„: ëª¨ë¸ë³„ ìµœì í™” ì¸ì½”ë”©

#### 6.1 LogBERT ì¸ì½”ë”©

```python
class LogBERTEncoder:
    """LogBERT ìµœì í™” ì¸ì½”ë”"""
    
    def __init__(self, vocab_size: int = 20000, max_seq_length: int = 512):
        self.vocab_size = vocab_size
        self.max_seq_length = max_seq_length
        
        # Special Tokens
        self.PAD_TOKEN_ID = 0
        self.CLS_TOKEN_ID = 101
        self.SEP_TOKEN_ID = 102
        self.MASK_TOKEN_ID = 103
        self.UNK_TOKEN_ID = 100
        
        # Event ID -> Token ID ë§¤í•‘
        self.event_to_token = {}
        self.token_to_event = {}
        self.next_token_id = 1
    
    def encode_session(self, session: Dict) -> Dict:
        """LogBERTìš© ì¸ì½”ë”©"""
        logs = session.get('logs', [])
        
        # Event ID ì‹œí€€ìŠ¤ ì¶”ì¶œ
        event_ids = [log.get('event_id', 0) for log in logs]
        
        # Token IDë¡œ ë³€í™˜
        token_ids = []
        for event_id in event_ids:
            if event_id not in self.event_to_token:
                self.event_to_token[event_id] = self.next_token_id
                self.token_to_event[self.next_token_id] = event_id
                self.next_token_id += 1
            
            token_id = self.event_to_token[event_id]
            token_ids.append(token_id)
        
        # Special Tokens ì¶”ê°€
        token_ids = [self.CLS_TOKEN_ID] + token_ids + [self.SEP_TOKEN_ID]
        
        # Padding
        attention_mask = [1] * len(token_ids)
        
        if len(token_ids) < self.max_seq_length:
            padding_length = self.max_seq_length - len(token_ids)
            token_ids = token_ids + [self.PAD_TOKEN_ID] * padding_length
            attention_mask = attention_mask + [0] * padding_length
        else:
            token_ids = token_ids[:self.max_seq_length]
            attention_mask = attention_mask[:self.max_seq_length]
        
        return {
            'token_ids': token_ids,
            'attention_mask': attention_mask,
            'event_sequence': event_ids,
            'session_length': len(logs),
            'padded_length': len(token_ids),
        }
```

#### 6.2 DeepLog ì¸ì½”ë”©

```python
class DeepLogEncoder:
    """DeepLog ìµœì í™” ì¸ì½”ë”"""
    
    def __init__(self, vocab_size: int = 10000, window_size: int = 20):
        self.vocab_size = vocab_size
        self.window_size = window_size
        
        # Event ID -> Index ë§¤í•‘
        self.event_to_index = {}
        self.index_to_event = {}
        self.next_index = 1
    
    def encode_session(self, session: Dict) -> Dict:
        """DeepLogìš© ì¸ì½”ë”©"""
        logs = session.get('logs', [])
        
        # Event ID ì‹œí€€ìŠ¤ ì¶”ì¶œ
        event_ids = [log.get('event_id', 0) for log in logs]
        
        # Indexë¡œ ë³€í™˜
        indices = []
        for event_id in event_ids:
            if event_id not in self.event_to_index:
                self.event_to_index[event_id] = self.next_index
                self.index_to_event[self.next_index] = event_id
                self.next_index += 1
            
            index = self.event_to_index[event_id]
            indices.append(index)
        
        # Sliding Window ìƒì„±
        windows = []
        labels = []
        
        for i in range(len(indices) - self.window_size):
            window = indices[i:i+self.window_size]
            label = indices[i+self.window_size]
            
            windows.append(window)
            labels.append(label)
        
        return {
            'windows': windows,
            'labels': labels,
            'event_sequence': event_ids,
            'vocab_size': len(self.event_to_index),
        }
```

#### 6.3 LogLSTM/LogTCN ì¸ì½”ë”©

```python
class SequenceEncoder:
    """LogLSTM/LogTCN ìµœì í™” ì¸ì½”ë”"""
    
    def __init__(self, vocab_size: int = 10000, max_seq_length: int = 256):
        self.vocab_size = vocab_size
        self.max_seq_length = max_seq_length
        
        # Event ID -> Index ë§¤í•‘
        self.event_to_index = {}
        self.index_to_event = {}
        self.next_index = 1
    
    def encode_session(self, session: Dict) -> Dict:
        """ì‹œí€€ìŠ¤ ëª¨ë¸ìš© ì¸ì½”ë”©"""
        logs = session.get('logs', [])
        
        # Event ID ì‹œí€€ìŠ¤ ì¶”ì¶œ
        event_ids = [log.get('event_id', 0) for log in logs]
        
        # Indexë¡œ ë³€í™˜
        indices = []
        for event_id in event_ids:
            if event_id not in self.event_to_index:
                self.event_to_index[event_id] = self.next_index
                self.index_to_event[self.next_index] = event_id
                self.next_index += 1
            
            index = self.event_to_index[event_id]
            indices.append(index)
        
        # Padding
        if len(indices) < self.max_seq_length:
            padding_length = self.max_seq_length - len(indices)
            indices = indices + [0] * padding_length
            mask = [1] * len(logs) + [0] * padding_length
        else:
            indices = indices[:self.max_seq_length]
            mask = [1] * self.max_seq_length
        
        return {
            'sequence': indices,
            'mask': mask,
            'event_sequence': event_ids[:self.max_seq_length],
            'sequence_length': len(logs),
            'vocab_size': len(self.event_to_index),
        }
```

---

### 7ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§

```python
class DataQualityValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.min_session_length = 5  # ìµœì†Œ ì„¸ì…˜ ê¸¸ì´
        self.max_session_length = 1000  # ìµœëŒ€ ì„¸ì…˜ ê¸¸ì´
        self.min_unique_events = 2  # ìµœì†Œ ê³ ìœ  ì´ë²¤íŠ¸ ìˆ˜
        self.max_duplicate_ratio = 0.9  # ìµœëŒ€ ì¤‘ë³µ ë¹„ìœ¨
    
    def validate_session(self, session: Dict) -> Dict:
        """ì„¸ì…˜ ê²€ì¦"""
        logs = session.get('logs', [])
        
        validations = {
            'valid': True,
            'reasons': [],
            'warnings': [],
        }
        
        # 1. ê¸¸ì´ ê²€ì¦
        if len(logs) < self.min_session_length:
            validations['valid'] = False
            validations['reasons'].append(f'session_too_short: {len(logs)} < {self.min_session_length}')
        
        if len(logs) > self.max_session_length:
            validations['valid'] = False
            validations['reasons'].append(f'session_too_long: {len(logs)} > {self.max_session_length}')
        
        # 2. ê³ ìœ  ì´ë²¤íŠ¸ ìˆ˜ ê²€ì¦
        event_ids = [log.get('event_id', 0) for log in logs]
        unique_events = len(set(event_ids))
        
        if unique_events < self.min_unique_events:
            validations['valid'] = False
            validations['reasons'].append(f'too_few_unique_events: {unique_events} < {self.min_unique_events}')
        
        # 3. ì¤‘ë³µ ë¹„ìœ¨ ê²€ì¦
        if len(event_ids) > 0:
            duplicate_ratio = 1 - (unique_events / len(event_ids))
            if duplicate_ratio > self.max_duplicate_ratio:
                validations['warnings'].append(f'high_duplicate_ratio: {duplicate_ratio:.2f}')
        
        # 4. íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦
        timestamps = [log.get('timestamp') for log in logs if log.get('timestamp')]
        if len(timestamps) < len(logs) * 0.8:  # 80% ì´ìƒ íƒ€ì„ìŠ¤íƒ¬í”„ í•„ìš”
            validations['warnings'].append('missing_timestamps')
        
        # 5. ì‹œê°„ ìˆœì„œ ê²€ì¦
        if len(timestamps) >= 2:
            sorted_timestamps = sorted(timestamps)
            if timestamps != sorted_timestamps:
                validations['warnings'].append('timestamps_not_sorted')
        
        return validations
    
    def filter_sessions(self, sessions: List[Dict]) -> List[Dict]:
        """ì„¸ì…˜ í•„í„°ë§"""
        valid_sessions = []
        invalid_count = 0
        
        for session in sessions:
            validation = self.validate_session(session)
            
            if validation['valid']:
                # ê²½ê³ ê°€ ìˆì–´ë„ í¬í•¨ (ë¡œê¹…ë§Œ)
                if validation['warnings']:
                    logger.debug(f"Session {session.get('session_id')} warnings: {validation['warnings']}")
                
                valid_sessions.append(session)
            else:
                invalid_count += 1
                logger.debug(f"Session {session.get('session_id')} invalid: {validation['reasons']}")
        
        logger.info(f"í•„í„°ë§ ì™„ë£Œ: {len(valid_sessions)}/{len(sessions)} ìœ íš¨ ì„¸ì…˜ (ì œê±°: {invalid_count})")
        
        return valid_sessions
```

---

### 8ë‹¨ê³„: ë°ì´í„° ì¦ê°• (ì„ íƒ)

```python
class DataAugmenter:
    """ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    
    def augment_session(self, session: Dict, methods: List[str] = ['noise', 'shuffle', 'mask']) -> List[Dict]:
        """ì„¸ì…˜ ì¦ê°•"""
        augmented = [session]  # ì›ë³¸ í¬í•¨
        
        logs = session.get('logs', [])
        
        if 'noise' in methods:
            augmented.append(self._add_noise(session))
        
        if 'shuffle' in methods:
            augmented.append(self._shuffle_events(session))
        
        if 'mask' in methods:
            augmented.append(self._mask_events(session))
        
        return augmented
    
    def _add_noise(self, session: Dict) -> Dict:
        """ë…¸ì´ì¦ˆ ì¶”ê°€"""
        logs = session.get('logs', [])
        noisy_logs = logs.copy()
        
        # ì¼ë¶€ ì´ë²¤íŠ¸ë¥¼ ëœë¤í•˜ê²Œ êµì²´ (5% í™•ë¥ )
        for i in range(len(noisy_logs)):
            if random.random() < 0.05:
                # ëœë¤ ì´ë²¤íŠ¸ IDë¡œ êµì²´
                noisy_logs[i] = {
                    **noisy_logs[i],
                    'event_id': random.randint(1, 1000)
                }
        
        return {
            **session,
            'logs': noisy_logs,
            'augmented': True,
            'augmentation_method': 'noise'
        }
    
    def _shuffle_events(self, session: Dict) -> Dict:
        """ì´ë²¤íŠ¸ ìˆœì„œ ì„ê¸° (ì‹œê°„ ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ë¶€ë¶„ì ìœ¼ë¡œ)"""
        logs = session.get('logs', [])
        
        # ì‘ì€ ìœˆë„ìš° ë‚´ì—ì„œë§Œ ì„ê¸°
        window_size = 5
        shuffled_logs = []
        
        for i in range(0, len(logs), window_size):
            window = logs[i:i+window_size]
            random.shuffle(window)
            shuffled_logs.extend(window)
        
        return {
            **session,
            'logs': shuffled_logs,
            'augmented': True,
            'augmentation_method': 'shuffle'
        }
    
    def _mask_events(self, session: Dict) -> Dict:
        """ì¼ë¶€ ì´ë²¤íŠ¸ ë§ˆìŠ¤í‚¹"""
        logs = session.get('logs', [])
        masked_logs = logs.copy()
        
        # 10% ì´ë²¤íŠ¸ ë§ˆìŠ¤í‚¹
        mask_count = int(len(masked_logs) * 0.1)
        mask_indices = random.sample(range(len(masked_logs)), mask_count)
        
        for idx in mask_indices:
            masked_logs[idx] = {
                **masked_logs[idx],
                'event_id': 0,  # MASK í† í°
            }
        
        return {
            **session,
            'logs': masked_logs,
            'augmented': True,
            'augmentation_method': 'mask'
        }
```

---

## ğŸ“Š ìµœì¢… ì¶œë ¥ í˜•ì‹

### LogBERTìš© ë°ì´í„°

```json
{
  "session_id": "gateway_trace_abc123_1234567890",
  "session_type": "trace",
  "token_ids": [101, 1, 2, 3, ..., 102, 0, 0, ...],
  "attention_mask": [1, 1, 1, ..., 1, 0, 0, ...],
  "event_sequence": [1, 5, 1, 12, 3, ...],
  "service_name": "gateway",
  "trace_id": "abc123",
  "has_error": true,
  "has_warn": false,
  "temporal_features": {
    "time_span": 45.2,
    "mean_interval": 2.3,
    "std_interval": 1.5
  },
  "dependency_features": {
    "service_count": 3,
    "service_chain": "gateway->research->manager"
  },
  "error_features": {
    "error_count": 2,
    "error_services": ["manager"],
    "error_propagation": false
  }
}
```

### DeepLogìš© ë°ì´í„°

```json
{
  "session_id": "gateway_service_1234567890",
  "session_type": "service",
  "windows": [[1, 2, 3, ...], [2, 3, 4, ...], ...],
  "labels": [4, 5, 6, ...],
  "event_sequence": [1, 2, 3, 4, 5, 6, ...],
  "service_name": "gateway",
  "vocab_size": 500
}
```

### LogLSTM/LogTCNìš© ë°ì´í„°

```json
{
  "session_id": "gateway_sliding_1234567890",
  "session_type": "sliding",
  "sequence": [1, 2, 3, 4, 5, ..., 0, 0, ...],
  "mask": [1, 1, 1, ..., 0, 0, ...],
  "event_sequence": [1, 2, 3, 4, 5, ...],
  "service_name": "gateway",
  "sequence_length": 20,
  "vocab_size": 500
}
```

---

## ğŸ”§ í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
class AdvancedPreprocessingPipeline:
    """ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cleaner = AdvancedLogCleaner()
        self.parser = AdvancedLogParser(config.get('drain3_config_path'))
        self.trace_extractor = AdvancedTraceExtractor()
        self.trace_validator = TraceValidator()
        self.sessionizer = HybridSessionizer(
            trace_window_time=config.get('trace_window_time', 600),
            service_window_size=config.get('service_window_size', 50),
            sliding_window_size=config.get('sliding_window_size', 20),
        )
        
        # íŠ¹ì§• ì¶”ì¶œê¸°
        self.temporal_extractor = TemporalFeatureExtractor()
        self.dependency_extractor = DependencyFeatureExtractor()
        self.error_extractor = ErrorPropagationExtractor()
        
        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë”
        self.context_builder = AdvancedMSAContextBuilder()
        
        # ì¸ì½”ë”
        self.logbert_encoder = LogBERTEncoder(
            vocab_size=config.get('logbert_vocab_size', 20000),
            max_seq_length=config.get('logbert_max_seq_length', 512)
        )
        self.deeplog_encoder = DeepLogEncoder(
            vocab_size=config.get('deeplog_vocab_size', 10000),
            window_size=config.get('deeplog_window_size', 20)
        )
        self.sequence_encoder = SequenceEncoder(
            vocab_size=config.get('sequence_vocab_size', 10000),
            max_seq_length=config.get('sequence_max_seq_length', 256)
        )
        
        # ê²€ì¦ ë° ì¦ê°•
        self.validator = DataQualityValidator()
        self.augmenter = DataAugmenter() if config.get('enable_augmentation', False) else None
    
    def process_logs(self, log_files: List[Path]) -> Dict:
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        all_sessions = {
            'logbert': [],
            'deeplog': [],
            'lstm': [],
            'tcn': [],
        }
        
        # 1. ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬
        for log_file in log_files:
            service_name = self._extract_service_name(log_file)
            
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # 1-1. ë¡œê·¸ ì •ë¦¬
                    cleaned = self.cleaner.clean_log_line(line)
                    if not cleaned:
                        continue
                    
                    # 1-2. ë¡œê·¸ íŒŒì‹±
                    parsed = self.parser.parse_log(cleaned, service_name)
                    if not parsed:
                        continue
                    
                    # 2. Trace ID ì¶”ì¶œ
                    trace_result = self.trace_extractor.extract_trace_id(cleaned, service_name)
                    trace_id = trace_result['trace_id'] if trace_result else None
                    
                    # 3. Trace ID ê²€ì¦ ë° ë“±ë¡
                    if trace_id:
                        self.trace_validator.register_trace(
                            trace_id, service_name, parsed['timestamp']
                        )
                    
                    # 4. ì„¸ì…˜í™”
                    sessions = self.sessionizer.add_log(
                        parsed, trace_id, service_name, parsed['timestamp']
                    )
                    
                    # 5. ì„¸ì…˜ ì²˜ë¦¬
                    for session in sessions:
                        # 5-1. íŠ¹ì§• ì¶”ì¶œ
                        session['temporal_features'] = self.temporal_extractor.extract_features(session)
                        session['dependency_features'] = self.dependency_extractor.extract_features(session)
                        session['error_features'] = self.error_extractor.extract_features(session)
                        
                        # 5-2. MSA ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©
                        if session.get('session_type') == 'trace':
                            session['msa_context'] = self.context_builder.build_context(session)
                        
                        # 5-3. ëª¨ë¸ë³„ ì¸ì½”ë”©
                        logbert_data = self.logbert_encoder.encode_session(session)
                        deeplog_data = self.deeplog_encoder.encode_session(session)
                        sequence_data = self.sequence_encoder.encode_session(session)
                        
                        # 5-4. ë°ì´í„° ê²€ì¦
                        validation = self.validator.validate_session(session)
                        if not validation['valid']:
                            continue
                        
                        # 5-5. ë°ì´í„° ì €ì¥
                        all_sessions['logbert'].append({
                            **session,
                            **logbert_data
                        })
                        all_sessions['deeplog'].append({
                            **session,
                            **deeplog_data
                        })
                        all_sessions['lstm'].append({
                            **session,
                            **sequence_data
                        })
                        all_sessions['tcn'].append({
                            **session,
                            **sequence_data
                        })
                        
                        # 5-6. ë°ì´í„° ì¦ê°• (ì„ íƒ)
                        if self.augmenter and self.config.get('enable_augmentation'):
                            augmented = self.augmenter.augment_session(session)
                            # ì¦ê°•ëœ ë°ì´í„°ë„ ì¶”ê°€ (ê°„ë‹¨íˆ ìƒëµ)
        
        # 6. ìµœì¢… í•„í„°ë§
        all_sessions['logbert'] = self.validator.filter_sessions(all_sessions['logbert'])
        all_sessions['deeplog'] = self.validator.filter_sessions(all_sessions['deeplog'])
        all_sessions['lstm'] = self.validator.filter_sessions(all_sessions['lstm'])
        all_sessions['tcn'] = self.validator.filter_sessions(all_sessions['tcn'])
        
        return all_sessions
    
    def _extract_service_name(self, log_file: Path) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ"""
        filename = log_file.stem.lower()
        
        for service in ['gateway', 'eureka', 'user', 'research', 'manager', 'code']:
            if service in filename:
                return service
        
        return 'unknown'
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì „ì²˜ë¦¬ í’ˆì§ˆ í™•ì¸

- [ ] ëª¨ë“  ë¡œê·¸ê°€ ì •í™•í•˜ê²Œ íŒŒì‹±ë˜ì—ˆëŠ”ê°€?
- [ ] Trace IDê°€ ì •í™•í•˜ê²Œ ì¶”ì¶œë˜ê³  ì—°ê²°ë˜ì—ˆëŠ”ê°€?
- [ ] ì„¸ì…˜ì´ ì ì ˆí•œ í¬ê¸°ì™€ ì‹œê°„ ë²”ìœ„ë¥¼ ê°€ì§€ëŠ”ê°€?
- [ ] ì‹œê°„ì  íŠ¹ì§•ì´ ì •í™•í•˜ê²Œ ì¶”ì¶œë˜ì—ˆëŠ”ê°€?
- [ ] ì„œë¹„ìŠ¤ ì˜ì¡´ì„±ì´ ì •í™•í•˜ê²Œ ë¶„ì„ë˜ì—ˆëŠ”ê°€?
- [ ] ì—ëŸ¬ ì „íŒŒê°€ ì •í™•í•˜ê²Œ ì¶”ì ë˜ì—ˆëŠ”ê°€?
- [ ] ëª¨ë¸ë³„ ì¸ì½”ë”©ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ê°€?
- [ ] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ì´ í†µê³¼ë˜ì—ˆëŠ”ê°€?

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ìµœê³  ì •í™•ë„ì˜ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
