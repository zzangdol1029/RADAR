#!/usr/bin/env python3
"""
DAGMM ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ì¬êµ¬ì„± ì˜¤ë¥˜ ë¶„ì„
- ì—ë„ˆì§€ ì ìˆ˜ ë¶„í¬
- ì´ìƒ íƒì§€ ì„±ëŠ¥ (Precision, Recall, F1)
- ROC-AUC ê³¡ì„ 
- ì‹œê°í™”
"""

import os
import sys
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from sklearn.metrics import (
    precision_recall_fscore_support,
    roc_auc_score,
    roc_curve,
    confusion_matrix,
    classification_report,
    precision_recall_curve,
    average_precision_score,
)
from torch.utils.data import DataLoader, TensorDataset
import argparse

sys.path.insert(0, str(Path(__file__).parent))
from model_dagmm import DAGMM, create_dagmm_model

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class DAGMMEvaluator:
    """DAGMM ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model: DAGMM, device: torch.device = None):
        self.model = model
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()
    
    def compute_scores(self, dataloader: DataLoader) -> Dict[str, np.ndarray]:
        """
        ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ì—ë„ˆì§€ ì ìˆ˜ ë° ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
        """
        energies = []
        rec_errors = []
        
        with torch.no_grad():
            for batch in dataloader:
                if isinstance(batch, (list, tuple)):
                    data = batch[0]
                else:
                    data = batch
                data = data.to(self.device)
                
                z, x_hat, z_c, gamma, x_flat = self.model(data)
                energy = self.model.compute_energy(z_c)
                
                # ì¬êµ¬ì„± ì˜¤ë¥˜
                rec_error = torch.mean((x_flat - x_hat) ** 2, dim=1)
                
                energies.extend(energy.cpu().numpy())
                rec_errors.extend(rec_error.cpu().numpy())
        
        return {
            'energy': np.array(energies),
            'reconstruction_error': np.array(rec_errors),
        }
    
    def evaluate_with_labels(
        self,
        scores: Dict[str, np.ndarray],
        labels: np.ndarray,
        threshold_percentile: float = 95,
    ) -> Dict[str, float]:
        """
        ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš° ì„±ëŠ¥ í‰ê°€
        
        Args:
            scores: ì—ë„ˆì§€ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
            labels: ì‹¤ì œ ì´ìƒ ë ˆì´ë¸” (0: ì •ìƒ, 1: ì´ìƒ)
            threshold_percentile: ì´ìƒ íŒì • ì„ê³„ê°’ ë°±ë¶„ìœ„ìˆ˜
        """
        energy = scores['energy']
        
        # ì„ê³„ê°’ ì„¤ì • (ì •ìƒ ë°ì´í„°ì˜ ìƒìœ„ percentile)
        threshold = np.percentile(energy, threshold_percentile)
        
        # ì˜ˆì¸¡
        predictions = (energy > threshold).astype(int)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='binary', zero_division=0
        )
        
        try:
            auc = roc_auc_score(labels, energy)
            ap = average_precision_score(labels, energy)
        except:
            auc = 0.0
            ap = 0.0
        
        return {
            'threshold': threshold,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'average_precision': ap,
            'predictions': predictions,
        }
    
    def evaluate_unsupervised(
        self,
        scores: Dict[str, np.ndarray],
        threshold_percentile: float = 95,
    ) -> Dict[str, any]:
        """
        ë¹„ì§€ë„ í‰ê°€ (ë ˆì´ë¸” ì—†ì´)
        """
        energy = scores['energy']
        rec_error = scores['reconstruction_error']
        
        threshold = np.percentile(energy, threshold_percentile)
        anomaly_count = np.sum(energy > threshold)
        
        return {
            'energy_mean': np.mean(energy),
            'energy_std': np.std(energy),
            'energy_min': np.min(energy),
            'energy_max': np.max(energy),
            'energy_median': np.median(energy),
            'rec_error_mean': np.mean(rec_error),
            'rec_error_std': np.std(rec_error),
            'threshold': threshold,
            'anomaly_count': anomaly_count,
            'anomaly_ratio': anomaly_count / len(energy),
        }
    
    def find_optimal_threshold(
        self,
        scores: Dict[str, np.ndarray],
        labels: np.ndarray,
    ) -> Tuple[float, Dict[str, float]]:
        """
        ìµœì  ì„ê³„ê°’ íƒìƒ‰ (F1 ê¸°ì¤€)
        """
        energy = scores['energy']
        best_f1 = 0
        best_threshold = 0
        best_metrics = {}
        
        for percentile in range(80, 100):
            threshold = np.percentile(energy, percentile)
            predictions = (energy > threshold).astype(int)
            
            precision, recall, f1, _ = precision_recall_fscore_support(
                labels, predictions, average='binary', zero_division=0
            )
            
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
                best_metrics = {
                    'percentile': percentile,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                }
        
        return best_threshold, best_metrics
    
    def plot_energy_distribution(
        self,
        scores: Dict[str, np.ndarray],
        labels: Optional[np.ndarray] = None,
        save_path: str = None,
    ):
        """ì—ë„ˆì§€ ì ìˆ˜ ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        energy = scores['energy']
        
        # íˆìŠ¤í† ê·¸ë¨
        if labels is not None:
            axes[0].hist(energy[labels == 0], bins=50, alpha=0.7, label='ì •ìƒ', color='blue')
            axes[0].hist(energy[labels == 1], bins=50, alpha=0.7, label='ì´ìƒ', color='red')
            axes[0].legend()
        else:
            axes[0].hist(energy, bins=50, alpha=0.7, color='blue')
        
        axes[0].set_xlabel('ì—ë„ˆì§€ ì ìˆ˜')
        axes[0].set_ylabel('ë¹ˆë„')
        axes[0].set_title('ì—ë„ˆì§€ ì ìˆ˜ ë¶„í¬')
        
        # ë°•ìŠ¤í”Œë¡¯
        if labels is not None:
            data = [energy[labels == 0], energy[labels == 1]]
            bp = axes[1].boxplot(data, labels=['ì •ìƒ', 'ì´ìƒ'], patch_artist=True)
            bp['boxes'][0].set_facecolor('blue')
            bp['boxes'][1].set_facecolor('red')
        else:
            axes[1].boxplot(energy, patch_artist=True)
        
        axes[1].set_ylabel('ì—ë„ˆì§€ ì ìˆ˜')
        axes[1].set_title('ì—ë„ˆì§€ ì ìˆ˜ ë°•ìŠ¤í”Œë¡¯')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ê·¸ë˜í”„ ì €ì¥: {save_path}")
        
        plt.show()
    
    def plot_roc_curve(
        self,
        scores: Dict[str, np.ndarray],
        labels: np.ndarray,
        save_path: str = None,
    ):
        """ROC ê³¡ì„  ì‹œê°í™”"""
        energy = scores['energy']
        
        fpr, tpr, thresholds = roc_curve(labels, energy)
        auc = roc_auc_score(labels, energy)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.4f})')
        plt.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('DAGMM ROC ê³¡ì„ ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ROC ê³¡ì„  ì €ì¥: {save_path}")
        
        plt.show()
    
    def plot_precision_recall_curve(
        self,
        scores: Dict[str, np.ndarray],
        labels: np.ndarray,
        save_path: str = None,
    ):
        """Precision-Recall ê³¡ì„ """
        energy = scores['energy']
        
        precision, recall, thresholds = precision_recall_curve(labels, energy)
        ap = average_precision_score(labels, energy)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, 'b-', linewidth=2, label=f'PR (AP = {ap:.4f})')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('DAGMM Precision-Recall ê³¡ì„ ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        
        plt.show()
    
    def generate_report(
        self,
        scores: Dict[str, np.ndarray],
        labels: Optional[np.ndarray] = None,
        output_dir: str = None,
    ) -> str:
        """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("DAGMM ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸")
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        report.append("")
        
        # ë¹„ì§€ë„ í†µê³„
        unsup_metrics = self.evaluate_unsupervised(scores)
        
        report.append("ğŸ“Š ì—ë„ˆì§€ ì ìˆ˜ í†µê³„")
        report.append("-" * 40)
        report.append(f"  í‰ê· : {unsup_metrics['energy_mean']:.4f}")
        report.append(f"  í‘œì¤€í¸ì°¨: {unsup_metrics['energy_std']:.4f}")
        report.append(f"  ìµœì†Œ: {unsup_metrics['energy_min']:.4f}")
        report.append(f"  ìµœëŒ€: {unsup_metrics['energy_max']:.4f}")
        report.append(f"  ì¤‘ì•™ê°’: {unsup_metrics['energy_median']:.4f}")
        report.append("")
        
        report.append("ğŸ“Š ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„")
        report.append("-" * 40)
        report.append(f"  í‰ê· : {unsup_metrics['rec_error_mean']:.4f}")
        report.append(f"  í‘œì¤€í¸ì°¨: {unsup_metrics['rec_error_std']:.4f}")
        report.append("")
        
        report.append("ğŸ” ì´ìƒ íƒì§€ ê²°ê³¼ (ìƒìœ„ 5% ê¸°ì¤€)")
        report.append("-" * 40)
        report.append(f"  ì„ê³„ê°’: {unsup_metrics['threshold']:.4f}")
        report.append(f"  ì´ìƒ ìƒ˜í”Œ ìˆ˜: {unsup_metrics['anomaly_count']}")
        report.append(f"  ì´ìƒ ë¹„ìœ¨: {unsup_metrics['anomaly_ratio']*100:.2f}%")
        report.append("")
        
        # ì§€ë„ í‰ê°€ (ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš°)
        if labels is not None:
            report.append("ğŸ“ˆ ë¶„ë¥˜ ì„±ëŠ¥ (ë ˆì´ë¸” ê¸°ë°˜)")
            report.append("-" * 40)
            
            metrics = self.evaluate_with_labels(scores, labels)
            report.append(f"  Precision: {metrics['precision']:.4f}")
            report.append(f"  Recall: {metrics['recall']:.4f}")
            report.append(f"  F1 Score: {metrics['f1']:.4f}")
            report.append(f"  ROC-AUC: {metrics['auc']:.4f}")
            report.append(f"  Average Precision: {metrics['average_precision']:.4f}")
            report.append("")
            
            # ìµœì  ì„ê³„ê°’
            opt_threshold, opt_metrics = self.find_optimal_threshold(scores, labels)
            report.append("ğŸ¯ ìµœì  ì„ê³„ê°’ (F1 ê¸°ì¤€)")
            report.append("-" * 40)
            report.append(f"  ë°±ë¶„ìœ„ìˆ˜: {opt_metrics['percentile']}%")
            report.append(f"  ìµœì  F1: {opt_metrics['f1']:.4f}")
        
        report.append("")
        report.append("=" * 60)
        
        report_text = "\n".join(report)
        print(report_text)
        
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            report_path = os.path.join(output_dir, 'dagmm_evaluation_report.txt')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"\në¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        return report_text


def load_model(checkpoint_path: str) -> DAGMM:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint.get('config', {})
    
    model = create_dagmm_model(config.get('model', {}))
    model.load_state_dict(checkpoint['model'])
    
    return model


def main():
    parser = argparse.ArgumentParser(description='DAGMM ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
    parser.add_argument('--checkpoint', type=str, required=True, help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--data-dir', type=str, required=True, help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-dir', type=str, default='evaluation_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--batch-size', type=int, default=64, help='ë°°ì¹˜ í¬ê¸°')
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¡œë“œ
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = load_model(args.checkpoint)
    evaluator = DAGMMEvaluator(model)
    
    # ë°ì´í„° ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ)
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë¡œì§ í•„ìš”
    
    print("\ní‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == '__main__':
    main()
