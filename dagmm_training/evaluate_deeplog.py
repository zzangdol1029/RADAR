#!/usr/bin/env python3
"""
DeepLog ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ì •í™•ë„, Top-k ì •í™•ë„
- Precision, Recall, F1
- í˜¼ë™ í–‰ë ¬
- ì´ìƒ íƒì§€ ì„±ëŠ¥
- ì‹œê°í™”
"""

import os
import sys
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from collections import Counter
from sklearn.metrics import (
    precision_recall_fscore_support,
    accuracy_score,
    confusion_matrix,
    classification_report,
    top_k_accuracy_score,
)
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import argparse

sys.path.insert(0, str(Path(__file__).parent))
from model_deeplog import DeepLog, create_deeplog_model

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class DeepLogEvaluator:
    """DeepLog ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model: DeepLog, device: torch.device = None):
        self.model = model
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()
    
    def predict(self, dataloader: DataLoader) -> Dict[str, np.ndarray]:
        """
        ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
        """
        all_probs = []
        all_preds = []
        all_labels = []
        all_logits = []
        
        with torch.no_grad():
            for batch in dataloader:
                sequences = batch[0].to(self.device)
                labels = batch[1].cpu().numpy()
                
                outputs = self.model(sequences)
                logits = outputs['logits']
                probs = F.softmax(logits, dim=-1)
                preds = logits.argmax(dim=-1).cpu().numpy()
                
                all_logits.append(logits.cpu().numpy())
                all_probs.append(probs.cpu().numpy())
                all_preds.extend(preds)
                all_labels.extend(labels)
        
        return {
            'logits': np.vstack(all_logits),
            'probabilities': np.vstack(all_probs),
            'predictions': np.array(all_preds),
            'labels': np.array(all_labels),
        }
    
    def compute_accuracy_metrics(
        self,
        results: Dict[str, np.ndarray],
        k_values: List[int] = [1, 3, 5, 10],
    ) -> Dict[str, float]:
        """
        ì •í™•ë„ ë° Top-k ì •í™•ë„ ê³„ì‚°
        """
        labels = results['labels']
        preds = results['predictions']
        probs = results['probabilities']
        
        metrics = {
            'accuracy': accuracy_score(labels, preds),
        }
        
        # Top-k ì •í™•ë„
        for k in k_values:
            if k <= probs.shape[1]:
                try:
                    top_k_acc = top_k_accuracy_score(labels, probs, k=k)
                    metrics[f'top_{k}_accuracy'] = top_k_acc
                except:
                    # ìˆ˜ë™ ê³„ì‚°
                    top_k_preds = np.argsort(probs, axis=1)[:, -k:]
                    correct = [1 if label in top_k_pred else 0 
                              for label, top_k_pred in zip(labels, top_k_preds)]
                    metrics[f'top_{k}_accuracy'] = np.mean(correct)
        
        return metrics
    
    def compute_classification_metrics(
        self,
        results: Dict[str, np.ndarray],
        average: str = 'weighted',
    ) -> Dict[str, float]:
        """
        ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚° (Precision, Recall, F1)
        """
        labels = results['labels']
        preds = results['predictions']
        
        precision, recall, f1, support = precision_recall_fscore_support(
            labels, preds, average=average, zero_division=0
        )
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
        }
    
    def compute_anomaly_detection_metrics(
        self,
        results: Dict[str, np.ndarray],
        anomaly_labels: np.ndarray,
        k: int = 5,
    ) -> Dict[str, float]:
        """
        ì´ìƒ íƒì§€ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        ì˜ˆì¸¡ì´ ìƒìœ„ kê°œì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ì´ìƒìœ¼ë¡œ íŒì •
        """
        labels = results['labels']
        probs = results['probabilities']
        
        # Top-k ì˜ˆì¸¡
        top_k_preds = np.argsort(probs, axis=1)[:, -k:]
        
        # ì´ìƒ íŒì •: ì‹¤ì œ ë ˆì´ë¸”ì´ top-kì— ì—†ìœ¼ë©´ ì´ìƒ
        predicted_anomaly = np.array([
            0 if label in top_k else 1
            for label, top_k in zip(labels, top_k_preds)
        ])
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        precision, recall, f1, _ = precision_recall_fscore_support(
            anomaly_labels, predicted_anomaly, average='binary', zero_division=0
        )
        
        accuracy = accuracy_score(anomaly_labels, predicted_anomaly)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(anomaly_labels, predicted_anomaly)
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'true_positive': int(tp),
            'true_negative': int(tn),
            'false_positive': int(fp),
            'false_negative': int(fn),
            'predicted_anomaly': predicted_anomaly,
        }
    
    def analyze_prediction_confidence(
        self,
        results: Dict[str, np.ndarray],
    ) -> Dict[str, any]:
        """
        ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„
        """
        probs = results['probabilities']
        labels = results['labels']
        preds = results['predictions']
        
        # ì˜ˆì¸¡ í™•ë¥ 
        pred_probs = np.max(probs, axis=1)
        
        # ì •ë‹µ í™•ë¥ 
        true_probs = probs[np.arange(len(labels)), labels]
        
        # ì •ë‹µ/ì˜¤ë‹µë³„ ì‹ ë¢°ë„
        correct_mask = preds == labels
        
        return {
            'mean_confidence': np.mean(pred_probs),
            'std_confidence': np.std(pred_probs),
            'mean_true_prob': np.mean(true_probs),
            'correct_confidence': np.mean(pred_probs[correct_mask]) if correct_mask.any() else 0,
            'wrong_confidence': np.mean(pred_probs[~correct_mask]) if (~correct_mask).any() else 0,
        }
    
    def plot_confusion_matrix(
        self,
        results: Dict[str, np.ndarray],
        top_n: int = 20,
        save_path: str = None,
    ):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (ìƒìœ„ Nê°œ í´ë˜ìŠ¤)"""
        labels = results['labels']
        preds = results['predictions']
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ ì„ íƒ
        label_counts = Counter(labels)
        top_classes = [c for c, _ in label_counts.most_common(top_n)]
        
        # í•„í„°ë§
        mask = np.isin(labels, top_classes) & np.isin(preds, top_classes)
        filtered_labels = labels[mask]
        filtered_preds = preds[mask]
        
        if len(filtered_labels) == 0:
            print("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        cm = confusion_matrix(filtered_labels, filtered_preds, labels=top_classes)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=top_classes, yticklabels=top_classes)
        plt.xlabel('ì˜ˆì¸¡')
        plt.ylabel('ì‹¤ì œ')
        plt.title(f'í˜¼ë™ í–‰ë ¬ (ìƒìœ„ {top_n}ê°œ í´ë˜ìŠ¤)')
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"í˜¼ë™ í–‰ë ¬ ì €ì¥: {save_path}")
        
        plt.show()
    
    def plot_confidence_distribution(
        self,
        results: Dict[str, np.ndarray],
        save_path: str = None,
    ):
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬ ì‹œê°í™”"""
        probs = results['probabilities']
        labels = results['labels']
        preds = results['predictions']
        
        pred_probs = np.max(probs, axis=1)
        correct_mask = preds == labels
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # ì „ì²´ ì‹ ë¢°ë„ ë¶„í¬
        axes[0].hist(pred_probs, bins=50, alpha=0.7, color='blue')
        axes[0].axvline(np.mean(pred_probs), color='red', linestyle='--', label=f'í‰ê· : {np.mean(pred_probs):.3f}')
        axes[0].set_xlabel('ì˜ˆì¸¡ ì‹ ë¢°ë„')
        axes[0].set_ylabel('ë¹ˆë„')
        axes[0].set_title('ì „ì²´ ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬')
        axes[0].legend()
        
        # ì •ë‹µ/ì˜¤ë‹µë³„ ë¶„í¬
        if correct_mask.any():
            axes[1].hist(pred_probs[correct_mask], bins=50, alpha=0.7, label='ì •ë‹µ', color='green')
        if (~correct_mask).any():
            axes[1].hist(pred_probs[~correct_mask], bins=50, alpha=0.7, label='ì˜¤ë‹µ', color='red')
        axes[1].set_xlabel('ì˜ˆì¸¡ ì‹ ë¢°ë„')
        axes[1].set_ylabel('ë¹ˆë„')
        axes[1].set_title('ì •ë‹µ/ì˜¤ë‹µë³„ ì‹ ë¢°ë„ ë¶„í¬')
        axes[1].legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        
        plt.show()
    
    def plot_top_k_accuracy(
        self,
        results: Dict[str, np.ndarray],
        k_values: List[int] = [1, 2, 3, 5, 10, 15, 20],
        save_path: str = None,
    ):
        """Top-k ì •í™•ë„ ê·¸ë˜í”„"""
        labels = results['labels']
        probs = results['probabilities']
        
        accuracies = []
        valid_k = []
        
        for k in k_values:
            if k <= probs.shape[1]:
                top_k_preds = np.argsort(probs, axis=1)[:, -k:]
                correct = [1 if label in top_k_pred else 0 
                          for label, top_k_pred in zip(labels, top_k_preds)]
                accuracies.append(np.mean(correct) * 100)
                valid_k.append(k)
        
        plt.figure(figsize=(10, 6))
        plt.plot(valid_k, accuracies, 'b-o', linewidth=2, markersize=8)
        plt.xlabel('k')
        plt.ylabel('Top-k ì •í™•ë„ (%)')
        plt.title('DeepLog Top-k ì •í™•ë„')
        plt.grid(True, alpha=0.3)
        plt.xticks(valid_k)
        
        for k, acc in zip(valid_k, accuracies):
            plt.annotate(f'{acc:.1f}%', (k, acc), textcoords="offset points", 
                        xytext=(0, 10), ha='center')
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        
        plt.show()
    
    def generate_report(
        self,
        results: Dict[str, np.ndarray],
        anomaly_labels: Optional[np.ndarray] = None,
        output_dir: str = None,
    ) -> str:
        """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("DeepLog ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸")
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        report.append("")
        
        # ê¸°ë³¸ í†µê³„
        report.append("ğŸ“Š ë°ì´í„° í†µê³„")
        report.append("-" * 40)
        report.append(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(results['labels']):,}")
        report.append(f"  í´ë˜ìŠ¤ ìˆ˜: {results['probabilities'].shape[1]}")
        report.append("")
        
        # ì •í™•ë„ ë©”íŠ¸ë¦­
        acc_metrics = self.compute_accuracy_metrics(results)
        report.append("ğŸ“ˆ ì •í™•ë„ ë©”íŠ¸ë¦­")
        report.append("-" * 40)
        report.append(f"  Top-1 ì •í™•ë„: {acc_metrics['accuracy']*100:.2f}%")
        for k in [3, 5, 10]:
            key = f'top_{k}_accuracy'
            if key in acc_metrics:
                report.append(f"  Top-{k} ì •í™•ë„: {acc_metrics[key]*100:.2f}%")
        report.append("")
        
        # ë¶„ë¥˜ ë©”íŠ¸ë¦­
        cls_metrics = self.compute_classification_metrics(results)
        report.append("ğŸ“ˆ ë¶„ë¥˜ ë©”íŠ¸ë¦­ (ê°€ì¤‘ í‰ê· )")
        report.append("-" * 40)
        report.append(f"  Precision: {cls_metrics['precision']:.4f}")
        report.append(f"  Recall: {cls_metrics['recall']:.4f}")
        report.append(f"  F1 Score: {cls_metrics['f1']:.4f}")
        report.append("")
        
        # ì‹ ë¢°ë„ ë¶„ì„
        conf_metrics = self.analyze_prediction_confidence(results)
        report.append("ğŸ” ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„")
        report.append("-" * 40)
        report.append(f"  í‰ê·  ì‹ ë¢°ë„: {conf_metrics['mean_confidence']:.4f}")
        report.append(f"  ì •ë‹µ ì˜ˆì¸¡ ì‹œ ì‹ ë¢°ë„: {conf_metrics['correct_confidence']:.4f}")
        report.append(f"  ì˜¤ë‹µ ì˜ˆì¸¡ ì‹œ ì‹ ë¢°ë„: {conf_metrics['wrong_confidence']:.4f}")
        report.append("")
        
        # ì´ìƒ íƒì§€ (ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš°)
        if anomaly_labels is not None:
            report.append("ğŸš¨ ì´ìƒ íƒì§€ ì„±ëŠ¥ (Top-5 ê¸°ì¤€)")
            report.append("-" * 40)
            
            for k in [5, 10]:
                anom_metrics = self.compute_anomaly_detection_metrics(results, anomaly_labels, k=k)
                report.append(f"  [Top-{k}]")
                report.append(f"    ì •í™•ë„: {anom_metrics['accuracy']*100:.2f}%")
                report.append(f"    Precision: {anom_metrics['precision']:.4f}")
                report.append(f"    Recall: {anom_metrics['recall']:.4f}")
                report.append(f"    F1: {anom_metrics['f1']:.4f}")
            report.append("")
        
        report.append("=" * 60)
        
        report_text = "\n".join(report)
        print(report_text)
        
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            report_path = os.path.join(output_dir, 'deeplog_evaluation_report.txt')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"\në¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        return report_text


def load_model(checkpoint_path: str) -> DeepLog:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint.get('config', {})
    
    model = create_deeplog_model(config.get('model', {}))
    model.load_state_dict(checkpoint['model'])
    
    return model


def main():
    parser = argparse.ArgumentParser(description='DeepLog ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
    parser.add_argument('--checkpoint', type=str, required=True, help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--data-dir', type=str, required=True, help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-dir', type=str, default='evaluation_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--batch-size', type=int, default=64, help='ë°°ì¹˜ í¬ê¸°')
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¡œë“œ
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = load_model(args.checkpoint)
    evaluator = DeepLogEvaluator(model)
    
    # ë°ì´í„° ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ)
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë¡œì§ í•„ìš”
    
    print("\ní‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == '__main__':
    main()
