#!/usr/bin/env python3
"""
CPU í™˜ê²½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ë©”ëª¨ë¦¬ íš¨ìœ¨ ë²„ì „
Intel 7ì„¸ëŒ€, 16GB RAM ìµœì í™”
"""

import os
import sys
import json
import time
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
import ijson  # ìŠ¤íŠ¸ë¦¬ë° JSON íŒŒì„œ

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from model_dagmm import DAGMM, DAGMMLoss
from model_deeplog import DeepLog
from torch.utils.data import DataLoader, TensorDataset

print("=" * 60)
print("DAGMM & DeepLog CPU í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ë²„ì „)")
print("=" * 60)
print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"CPU ì‚¬ìš©")
print()

# ========== ì„¤ì • ==========
DATA_FILE = r"C:\Users\ssoo2\Downloads\logFile\logFile\preprocessed_logs_2025-05-01.json"
WINDOW_SIZE = 10
BATCH_SIZE = 32  # CPUìš© ì‘ì€ ë°°ì¹˜
TEST_EPOCHS = 3  # í…ŒìŠ¤íŠ¸ìš© ì ì€ ì—í­
MAX_SESSIONS = 5000  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 5000ê°œë§Œ ì‚¬ìš©

# ========== 1. ë°ì´í„° ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë°) ==========
print("=" * 60)
print("1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)")
print("=" * 60)

start_time = time.time()
print(f"íŒŒì¼: {DATA_FILE}")
print(f"ìµœëŒ€ {MAX_SESSIONS:,}ê°œ ì„¸ì…˜ë§Œ ë¡œë“œí•©ë‹ˆë‹¤...")

sessions = []
try:
    with open(DATA_FILE, 'rb') as f:
        parser = ijson.items(f, 'item')
        for i, session in enumerate(parser):
            if i >= MAX_SESSIONS:
                break
            # event_sequenceë§Œ ì¶”ì¶œ
            event_seq = session.get('event_sequence', [])
            if event_seq:
                sessions.append({'event_sequence': event_seq})
            
            if (i + 1) % 1000 == 0:
                print(f"  ë¡œë“œ ì¤‘... {i+1:,}ê°œ")
    
    load_time = time.time() - start_time
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(sessions):,}ê°œ ì„¸ì…˜ ({load_time:.1f}ì´ˆ)")
    
except ImportError:
    print("ijson íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜í•©ë‹ˆë‹¤...")
    import subprocess
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ijson'])
    print("ijson ì„¤ì¹˜ ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(0)
except Exception as e:
    print(f"ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ì¼ë°˜ JSON ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
    
    # ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ ì¼ë¶€ë§Œ ë¡œë“œ
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        # íŒŒì¼ì˜ ì²˜ìŒ ë¶€ë¶„ë§Œ ì½ê¸°
        content = f.read(50_000_000)  # 50MBë§Œ ì½ê¸°
        # JSON ë°°ì—´ì—ì„œ ì™„ì „í•œ ê°ì²´ë“¤ë§Œ ì¶”ì¶œ
        content = content.rsplit('}, {', 1)[0] + '}]'
        if not content.startswith('['):
            content = '[' + content
        
        all_sessions = json.loads(content)[:MAX_SESSIONS]
        sessions = [{'event_sequence': s.get('event_sequence', [])} for s in all_sessions if s.get('event_sequence')]
    
    load_time = time.time() - start_time
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(sessions):,}ê°œ ì„¸ì…˜ ({load_time:.1f}ì´ˆ)")

# ========== 2. ë°ì´í„° ì „ì²˜ë¦¬ ==========
print()
print("=" * 60)
print("2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 60)

# DAGMMìš© ë°ì´í„°
dagmm_samples = []
all_event_ids = set()

# DeepLogìš© ë°ì´í„°
deeplog_sequences = []
deeplog_labels = []

for session in sessions:
    seq = session.get('event_sequence', [])
    if not seq:
        continue
    
    all_event_ids.update(seq)
    
    # DAGMM: ìœˆë„ìš° ìƒì„±
    if len(seq) < WINDOW_SIZE:
        padded = seq + [0] * (WINDOW_SIZE - len(seq))
        dagmm_samples.append(padded)
    else:
        for i in range(len(seq) - WINDOW_SIZE + 1):
            dagmm_samples.append(seq[i:i + WINDOW_SIZE])
    
    # DeepLog: ì…ë ¥ + ë ˆì´ë¸” ìƒì„±
    if len(seq) > WINDOW_SIZE:
        for i in range(len(seq) - WINDOW_SIZE):
            deeplog_sequences.append(seq[i:i + WINDOW_SIZE])
            deeplog_labels.append(seq[i + WINDOW_SIZE])

del sessions  # ë©”ëª¨ë¦¬ í•´ì œ

# ì´ë²¤íŠ¸ ID ì¬ë§¤í•‘
event_id_map = {eid: idx + 1 for idx, eid in enumerate(sorted(all_event_ids))}
event_id_map[0] = 0
num_classes = len(event_id_map)

# ë°ì´í„° ì¬ë§¤í•‘
dagmm_samples = [[event_id_map.get(x, 0) for x in seq] for seq in dagmm_samples[:20000]]  # 2ë§Œê°œë¡œ ì œí•œ
deeplog_sequences = [[event_id_map.get(x, 0) for x in seq] for seq in deeplog_sequences[:20000]]
deeplog_labels = [event_id_map.get(x, 0) for x in deeplog_labels[:20000]]

print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
print(f"  - DAGMM ìƒ˜í”Œ: {len(dagmm_samples):,}ê°œ")
print(f"  - DeepLog ìƒ˜í”Œ: {len(deeplog_sequences):,}ê°œ")
print(f"  - ì´ë²¤íŠ¸ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")

# ========== 3. ì‹œê°„ ì¶”ì • ==========
print()
print("=" * 60)
print("3ë‹¨ê³„: í•™ìŠµ ì‹œê°„ ì¶”ì •")
print("=" * 60)

# ì‘ì€ ìƒ˜í”Œë¡œ ì†ë„ í…ŒìŠ¤íŠ¸
print("ì†ë„ í…ŒìŠ¤íŠ¸ ì¤‘...")

# DAGMM ì†ë„ í…ŒìŠ¤íŠ¸
dagmm_model = DAGMM(num_classes=num_classes, window_size=WINDOW_SIZE)
sample_size = min(100, len(dagmm_samples))
dagmm_test_data = torch.LongTensor(dagmm_samples[:sample_size])

start = time.time()
for _ in range(5):
    with torch.no_grad():
        _ = dagmm_model(dagmm_test_data)
dagmm_batch_time = (time.time() - start) / 5

# DeepLog ì†ë„ í…ŒìŠ¤íŠ¸
deeplog_model = DeepLog(num_classes=num_classes)
sample_size = min(100, len(deeplog_sequences)) if deeplog_sequences else 100
deeplog_test_data = torch.LongTensor(deeplog_sequences[:sample_size] if deeplog_sequences else [[0]*WINDOW_SIZE]*100)

start = time.time()
for _ in range(5):
    with torch.no_grad():
        _ = deeplog_model(deeplog_test_data)
deeplog_batch_time = (time.time() - start) / 5

# ì¶”ì • ê³„ì‚°
dagmm_batches = len(dagmm_samples) // BATCH_SIZE + 1
deeplog_batches = len(deeplog_sequences) // BATCH_SIZE + 1 if deeplog_sequences else 0

dagmm_epoch_time = dagmm_batches * dagmm_batch_time * 3
deeplog_epoch_time = deeplog_batches * deeplog_batch_time * 3

dagmm_total_time = dagmm_epoch_time * TEST_EPOCHS
deeplog_total_time = deeplog_epoch_time * TEST_EPOCHS

print()
print("ğŸ“Š ì˜ˆìƒ í•™ìŠµ ì‹œê°„:")
print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  â”‚ DAGMM ({TEST_EPOCHS} epochs, {len(dagmm_samples):,} ìƒ˜í”Œ)")
print(f"  â”‚  - ë°°ì¹˜ ìˆ˜: {dagmm_batches:,}")
print(f"  â”‚  - ì´ ì˜ˆìƒ: ì•½ {dagmm_total_time/60:.1f}ë¶„")
print(f"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  â”‚ DeepLog ({TEST_EPOCHS} epochs, {len(deeplog_sequences):,} ìƒ˜í”Œ)")
print(f"  â”‚  - ë°°ì¹˜ ìˆ˜: {deeplog_batches:,}")
print(f"  â”‚  - ì´ ì˜ˆìƒ: ì•½ {deeplog_total_time/60:.1f}ë¶„")
print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  ì´ ì˜ˆìƒ ì‹œê°„: ì•½ {(dagmm_total_time + deeplog_total_time)/60:.1f}ë¶„")
print()

# ========== 4. DAGMM í•™ìŠµ ==========
print("=" * 60)
print("4ë‹¨ê³„: DAGMM í•™ìŠµ ì‹œì‘")
print("=" * 60)

# ë°ì´í„° ë¶„í• 
n = len(dagmm_samples)
idx = np.random.permutation(n)
split = int(n * 0.2)
train_dagmm = [dagmm_samples[i] for i in idx[split:]]
test_dagmm = [dagmm_samples[i] for i in idx[:split]]

train_loader = DataLoader(TensorDataset(torch.LongTensor(train_dagmm)), batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(TensorDataset(torch.LongTensor(test_dagmm)), batch_size=BATCH_SIZE)

print(f"í•™ìŠµ: {len(train_dagmm):,}ê°œ, í…ŒìŠ¤íŠ¸: {len(test_dagmm):,}ê°œ")

dagmm_model = DAGMM(num_classes=num_classes, window_size=WINDOW_SIZE)
criterion = DAGMMLoss()
optimizer = torch.optim.Adam(dagmm_model.parameters(), lr=0.001)

dagmm_start = time.time()
best_loss = float('inf')

for epoch in range(1, TEST_EPOCHS + 1):
    dagmm_model.train()
    total_loss = 0
    
    for batch_idx, (data,) in enumerate(train_loader):
        z, x_hat, z_c, gamma, x_flat = dagmm_model(data)
        phi, mu, sigma = dagmm_model.compute_gmm_params(z_c, gamma)
        
        if batch_idx == 0 and epoch == 1:
            dagmm_model.phi = phi
            dagmm_model.mu = mu
            dagmm_model.sigma = sigma
        else:
            m = 0.9
            dagmm_model.phi = m * dagmm_model.phi.detach() + (1-m) * phi
            dagmm_model.mu = m * dagmm_model.mu.detach() + (1-m) * mu
            dagmm_model.sigma = m * dagmm_model.sigma.detach() + (1-m) * sigma
        
        energy = dagmm_model.compute_energy(z_c, phi, mu, sigma)
        loss, _, _, _ = criterion(x_flat, x_hat, energy, sigma)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(dagmm_model.parameters(), 5.0)
        optimizer.step()
        
        total_loss += loss.item()
        
        if (batch_idx + 1) % 200 == 0:
            print(f"  Epoch {epoch}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}")
    
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch}/{TEST_EPOCHS} - Loss: {avg_loss:.4f}")
    
    if avg_loss < best_loss:
        best_loss = avg_loss

dagmm_time = time.time() - dagmm_start
print(f"\nâœ… DAGMM ì™„ë£Œ! ì‹œê°„: {dagmm_time/60:.1f}ë¶„, ìµœì € Loss: {best_loss:.4f}")

# ========== 5. DeepLog í•™ìŠµ ==========
print()
print("=" * 60)
print("5ë‹¨ê³„: DeepLog í•™ìŠµ ì‹œì‘")
print("=" * 60)

if not deeplog_sequences:
    print("âš ï¸ DeepLog í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    n = len(deeplog_sequences)
    idx = np.random.permutation(n)
    split = int(n * 0.2)
    
    train_seq = [deeplog_sequences[i] for i in idx[split:]]
    train_lbl = [deeplog_labels[i] for i in idx[split:]]
    test_seq = [deeplog_sequences[i] for i in idx[:split]]
    test_lbl = [deeplog_labels[i] for i in idx[:split]]
    
    train_loader = DataLoader(TensorDataset(torch.LongTensor(train_seq), torch.LongTensor(train_lbl)), batch_size=BATCH_SIZE, shuffle=True)
    
    print(f"í•™ìŠµ: {len(train_seq):,}ê°œ, í…ŒìŠ¤íŠ¸: {len(test_seq):,}ê°œ")
    
    deeplog_model = DeepLog(num_classes=num_classes)
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(deeplog_model.parameters(), lr=0.001)
    
    deeplog_start = time.time()
    best_acc = 0
    
    for epoch in range(1, TEST_EPOCHS + 1):
        deeplog_model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (seq, lbl) in enumerate(train_loader):
            outputs = deeplog_model(seq)
            logits = outputs['logits']
            loss = criterion(logits, lbl)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = logits.max(1)
            total += lbl.size(0)
            correct += predicted.eq(lbl).sum().item()
            
            if (batch_idx + 1) % 200 == 0:
                print(f"  Epoch {epoch}, Batch {batch_idx+1}/{len(train_loader)}")
        
        acc = 100. * correct / total
        print(f"Epoch {epoch}/{TEST_EPOCHS} - Loss: {total_loss/len(train_loader):.4f}, Acc: {acc:.2f}%")
        
        if acc > best_acc:
            best_acc = acc
    
    deeplog_time = time.time() - deeplog_start
    print(f"\nâœ… DeepLog ì™„ë£Œ! ì‹œê°„: {deeplog_time/60:.1f}ë¶„, ìµœê³  Acc: {best_acc:.2f}%")

# ========== ì™„ë£Œ ==========
print()
print("=" * 60)
print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 60)
total_time = time.time() - start_time
print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
print(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
