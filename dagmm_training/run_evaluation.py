#!/usr/bin/env python3
"""
DAGMM & DeepLog í†µí•© í…ŒìŠ¤íŠ¸ ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
CPU í™˜ê²½ ìµœì í™” (Intel 7ì„¸ëŒ€, 16GB RAM)
"""

import os
import sys
import json
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
from torch.utils.data import DataLoader, TensorDataset
import argparse

sys.path.insert(0, str(Path(__file__).parent))

from model_dagmm import DAGMM, DAGMMLoss
from model_deeplog import DeepLog
from evaluate_dagmm import DAGMMEvaluator
from evaluate_deeplog import DeepLogEvaluator

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


def load_data_streaming(file_path: str, max_sessions: int = 5000):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ"""
    try:
        import ijson
        sessions = []
        with open(file_path, 'rb') as f:
            parser = ijson.items(f, 'item')
            for i, session in enumerate(parser):
                if i >= max_sessions:
                    break
                event_seq = session.get('event_sequence', [])
                if event_seq:
                    sessions.append({'event_sequence': event_seq})
                if (i + 1) % 1000 == 0:
                    print(f"  ë¡œë“œ ì¤‘... {i+1:,}ê°œ")
        return sessions
    except ImportError:
        print("ijson íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install ijson")
        return []


def prepare_data(sessions, window_size=10, max_samples=20000):
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    dagmm_samples = []
    deeplog_sequences = []
    deeplog_labels = []
    all_event_ids = set()
    
    for session in sessions:
        seq = session.get('event_sequence', [])
        if not seq:
            continue
        
        all_event_ids.update(seq)
        
        # DAGMM
        if len(seq) < window_size:
            padded = seq + [0] * (window_size - len(seq))
            dagmm_samples.append(padded)
        else:
            for i in range(len(seq) - window_size + 1):
                dagmm_samples.append(seq[i:i + window_size])
        
        # DeepLog
        if len(seq) > window_size:
            for i in range(len(seq) - window_size):
                deeplog_sequences.append(seq[i:i + window_size])
                deeplog_labels.append(seq[i + window_size])
    
    # ì´ë²¤íŠ¸ ID ë§¤í•‘
    event_id_map = {eid: idx + 1 for idx, eid in enumerate(sorted(all_event_ids))}
    event_id_map[0] = 0
    num_classes = len(event_id_map)
    
    # ì œí•œ ë° ë§¤í•‘
    dagmm_samples = [[event_id_map.get(x, 0) for x in seq] for seq in dagmm_samples[:max_samples]]
    deeplog_sequences = [[event_id_map.get(x, 0) for x in seq] for seq in deeplog_sequences[:max_samples]]
    deeplog_labels = [event_id_map.get(x, 0) for x in deeplog_labels[:max_samples]]
    
    return dagmm_samples, deeplog_sequences, deeplog_labels, num_classes, event_id_map


def train_and_evaluate_dagmm(
    train_samples, test_samples, num_classes, window_size, 
    epochs=3, batch_size=32, output_dir='results'
):
    """DAGMM í•™ìŠµ ë° í‰ê°€"""
    print("\n" + "=" * 60)
    print("DAGMM í•™ìŠµ ë° í‰ê°€")
    print("=" * 60)
    
    # DataLoader
    train_loader = DataLoader(TensorDataset(torch.LongTensor(train_samples)), 
                             batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(TensorDataset(torch.LongTensor(test_samples)), 
                            batch_size=batch_size)
    
    # ëª¨ë¸
    model = DAGMM(num_classes=num_classes, window_size=window_size)
    criterion = DAGMMLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_samples):,}, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_samples):,}")
    
    # í•™ìŠµ
    start_time = time.time()
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        
        for batch_idx, (data,) in enumerate(train_loader):
            z, x_hat, z_c, gamma, x_flat = model(data)
            phi, mu, sigma = model.compute_gmm_params(z_c, gamma)
            
            if batch_idx == 0 and epoch == 1:
                model.phi = phi
                model.mu = mu
                model.sigma = sigma
            else:
                m = 0.9
                model.phi = m * model.phi.detach() + (1-m) * phi
                model.mu = m * model.mu.detach() + (1-m) * mu
                model.sigma = m * model.sigma.detach() + (1-m) * sigma
            
            energy = model.compute_energy(z_c, phi, mu, sigma)
            loss, _, _, _ = criterion(x_flat, x_hat, energy, sigma)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        print(f"  Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}")
    
    train_time = time.time() - start_time
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {train_time/60:.1f}ë¶„")
    
    # í‰ê°€
    print("\nğŸ“Š DAGMM ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    evaluator = DAGMMEvaluator(model)
    scores = evaluator.compute_scores(test_loader)
    metrics = evaluator.evaluate_unsupervised(scores)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    
    # ì—ë„ˆì§€ ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist(scores['energy'], bins=50, alpha=0.7, color='blue')
    plt.axvline(metrics['threshold'], color='red', linestyle='--', label=f"ì„ê³„ê°’ (95%): {metrics['threshold']:.2f}")
    plt.xlabel('ì—ë„ˆì§€ ì ìˆ˜')
    plt.ylabel('ë¹ˆë„')
    plt.title('DAGMM ì—ë„ˆì§€ ì ìˆ˜ ë¶„í¬')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.hist(scores['reconstruction_error'], bins=50, alpha=0.7, color='green')
    plt.xlabel('ì¬êµ¬ì„± ì˜¤ë¥˜')
    plt.ylabel('ë¹ˆë„')
    plt.title('DAGMM ì¬êµ¬ì„± ì˜¤ë¥˜ ë¶„í¬')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'dagmm_distribution.png'), dpi=150)
    print(f"  ê·¸ë˜í”„ ì €ì¥: {output_dir}/dagmm_distribution.png")
    plt.close()
    
    # ë¦¬í¬íŠ¸
    evaluator.generate_report(scores, output_dir=output_dir)
    
    # ëª¨ë¸ ì €ì¥
    torch.save({
        'model': model.state_dict(),
        'config': {'num_classes': num_classes, 'window_size': window_size},
        'metrics': metrics,
    }, os.path.join(output_dir, 'dagmm_model.pt'))
    print(f"  ëª¨ë¸ ì €ì¥: {output_dir}/dagmm_model.pt")
    
    return model, metrics


def train_and_evaluate_deeplog(
    train_sequences, train_labels, test_sequences, test_labels,
    num_classes, epochs=3, batch_size=32, output_dir='results'
):
    """DeepLog í•™ìŠµ ë° í‰ê°€"""
    print("\n" + "=" * 60)
    print("DeepLog í•™ìŠµ ë° í‰ê°€")
    print("=" * 60)
    
    if len(train_sequences) == 0:
        print("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ window_sizeë³´ë‹¤ ì§§ìŒ)")
        return None, {}
    
    # DataLoader
    train_loader = DataLoader(
        TensorDataset(torch.LongTensor(train_sequences), torch.LongTensor(train_labels)),
        batch_size=batch_size, shuffle=True
    )
    test_loader = DataLoader(
        TensorDataset(torch.LongTensor(test_sequences), torch.LongTensor(test_labels)),
        batch_size=batch_size
    )
    
    # ëª¨ë¸
    model = DeepLog(num_classes=num_classes)
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_sequences):,}, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_sequences):,}")
    
    # í•™ìŠµ
    start_time = time.time()
    best_acc = 0
    
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for seq, lbl in train_loader:
            outputs = model(seq)
            logits = outputs['logits']
            loss = criterion(logits, lbl)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = logits.max(1)
            total += lbl.size(0)
            correct += predicted.eq(lbl).sum().item()
        
        acc = 100. * correct / total
        print(f"  Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f}, Acc: {acc:.2f}%")
        
        if acc > best_acc:
            best_acc = acc
    
    train_time = time.time() - start_time
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {train_time/60:.1f}ë¶„, ìµœê³  ì •í™•ë„: {best_acc:.2f}%")
    
    # í‰ê°€
    print("\nğŸ“Š DeepLog ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    evaluator = DeepLogEvaluator(model)
    results = evaluator.predict(test_loader)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    acc_metrics = evaluator.compute_accuracy_metrics(results)
    cls_metrics = evaluator.compute_classification_metrics(results)
    conf_metrics = evaluator.analyze_prediction_confidence(results)
    
    # ì‹œê°í™”
    os.makedirs(output_dir, exist_ok=True)
    
    # Top-k ì •í™•ë„
    k_values = [1, 2, 3, 5, 10]
    accuracies = []
    valid_k = []
    
    for k in k_values:
        if k <= results['probabilities'].shape[1]:
            top_k_preds = np.argsort(results['probabilities'], axis=1)[:, -k:]
            correct = [1 if label in top_k else 0 
                      for label, top_k in zip(results['labels'], top_k_preds)]
            accuracies.append(np.mean(correct) * 100)
            valid_k.append(k)
    
    if valid_k:
        plt.figure(figsize=(10, 5))
        plt.bar(range(len(valid_k)), accuracies, color='steelblue')
        plt.xticks(range(len(valid_k)), [f'Top-{k}' for k in valid_k])
        plt.ylabel('ì •í™•ë„ (%)')
        plt.title('DeepLog Top-k ì •í™•ë„')
        for i, acc in enumerate(accuracies):
            plt.text(i, acc + 1, f'{acc:.1f}%', ha='center')
        plt.savefig(os.path.join(output_dir, 'deeplog_topk_accuracy.png'), dpi=150)
        print(f"  ê·¸ë˜í”„ ì €ì¥: {output_dir}/deeplog_topk_accuracy.png")
        plt.close()
    
    # ì‹ ë¢°ë„ ë¶„í¬
    pred_probs = np.max(results['probabilities'], axis=1)
    correct_mask = results['predictions'] == results['labels']
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist(pred_probs, bins=50, alpha=0.7, color='blue')
    plt.axvline(np.mean(pred_probs), color='red', linestyle='--', label=f'í‰ê· : {np.mean(pred_probs):.3f}')
    plt.xlabel('ì˜ˆì¸¡ ì‹ ë¢°ë„')
    plt.ylabel('ë¹ˆë„')
    plt.title('ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    if correct_mask.any():
        plt.hist(pred_probs[correct_mask], bins=30, alpha=0.7, label='ì •ë‹µ', color='green')
    if (~correct_mask).any():
        plt.hist(pred_probs[~correct_mask], bins=30, alpha=0.7, label='ì˜¤ë‹µ', color='red')
    plt.xlabel('ì˜ˆì¸¡ ì‹ ë¢°ë„')
    plt.ylabel('ë¹ˆë„')
    plt.title('ì •ë‹µ/ì˜¤ë‹µë³„ ì‹ ë¢°ë„')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'deeplog_confidence.png'), dpi=150)
    print(f"  ê·¸ë˜í”„ ì €ì¥: {output_dir}/deeplog_confidence.png")
    plt.close()
    
    # ë¦¬í¬íŠ¸
    evaluator.generate_report(results, output_dir=output_dir)
    
    # ëª¨ë¸ ì €ì¥
    torch.save({
        'model': model.state_dict(),
        'config': {'num_classes': num_classes},
        'metrics': {**acc_metrics, **cls_metrics},
    }, os.path.join(output_dir, 'deeplog_model.pt'))
    print(f"  ëª¨ë¸ ì €ì¥: {output_dir}/deeplog_model.pt")
    
    return model, {**acc_metrics, **cls_metrics, **conf_metrics}


def main():
    parser = argparse.ArgumentParser(description='DAGMM & DeepLog í†µí•© í…ŒìŠ¤íŠ¸ ë° í‰ê°€')
    parser.add_argument('--data-file', type=str, 
                       default=r'C:\Users\ssoo2\Downloads\logFile\logFile\preprocessed_logs_2025-05-01.json',
                       help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', type=str, default='evaluation_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--max-sessions', type=int, default=5000, help='ìµœëŒ€ ì„¸ì…˜ ìˆ˜')
    parser.add_argument('--window-size', type=int, default=5, help='ìœˆë„ìš° í¬ê¸°')
    parser.add_argument('--epochs', type=int, default=3, help='í•™ìŠµ ì—í­')
    parser.add_argument('--batch-size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    args = parser.parse_args()
    
    print("=" * 60)
    print("DAGMM & DeepLog í†µí•© í…ŒìŠ¤íŠ¸ ë° í‰ê°€")
    print("=" * 60)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ë°ì´í„° íŒŒì¼: {args.data_file}")
    print(f"ìœˆë„ìš° í¬ê¸°: {args.window_size}")
    print()
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    sessions = load_data_streaming(args.data_file, args.max_sessions)
    print(f"ë¡œë“œ ì™„ë£Œ: {len(sessions):,}ê°œ ì„¸ì…˜")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    dagmm_samples, deeplog_sequences, deeplog_labels, num_classes, _ = prepare_data(
        sessions, window_size=args.window_size
    )
    print(f"DAGMM ìƒ˜í”Œ: {len(dagmm_samples):,}ê°œ")
    print(f"DeepLog ìƒ˜í”Œ: {len(deeplog_sequences):,}ê°œ")
    print(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    
    # ë°ì´í„° ë¶„í• 
    n_dagmm = len(dagmm_samples)
    n_deeplog = len(deeplog_sequences)
    
    idx_dagmm = np.random.permutation(n_dagmm)
    split_dagmm = int(n_dagmm * 0.2)
    train_dagmm = [dagmm_samples[i] for i in idx_dagmm[split_dagmm:]]
    test_dagmm = [dagmm_samples[i] for i in idx_dagmm[:split_dagmm]]
    
    if n_deeplog > 0:
        idx_deeplog = np.random.permutation(n_deeplog)
        split_deeplog = int(n_deeplog * 0.2)
        train_seq = [deeplog_sequences[i] for i in idx_deeplog[split_deeplog:]]
        train_lbl = [deeplog_labels[i] for i in idx_deeplog[split_deeplog:]]
        test_seq = [deeplog_sequences[i] for i in idx_deeplog[:split_deeplog]]
        test_lbl = [deeplog_labels[i] for i in idx_deeplog[:split_deeplog]]
    else:
        train_seq, train_lbl = [], []
        test_seq, test_lbl = [], []
    
    # DAGMM í•™ìŠµ ë° í‰ê°€
    dagmm_model, dagmm_metrics = train_and_evaluate_dagmm(
        train_dagmm, test_dagmm, num_classes, args.window_size,
        epochs=args.epochs, batch_size=args.batch_size, output_dir=args.output_dir
    )
    
    # DeepLog í•™ìŠµ ë° í‰ê°€
    deeplog_model, deeplog_metrics = train_and_evaluate_deeplog(
        train_seq, train_lbl, test_seq, test_lbl,
        num_classes, epochs=args.epochs, batch_size=args.batch_size, output_dir=args.output_dir
    )
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    print("\nğŸ“Š DAGMM ê²°ê³¼:")
    print(f"  - ì—ë„ˆì§€ ì ìˆ˜ í‰ê· : {dagmm_metrics.get('energy_mean', 0):.4f}")
    print(f"  - ì´ìƒ ë¹„ìœ¨ (ìƒìœ„ 5%): {dagmm_metrics.get('anomaly_ratio', 0)*100:.2f}%")
    
    if deeplog_metrics:
        print("\nğŸ“Š DeepLog ê²°ê³¼:")
        print(f"  - Top-1 ì •í™•ë„: {deeplog_metrics.get('accuracy', 0)*100:.2f}%")
        print(f"  - F1 Score: {deeplog_metrics.get('f1', 0):.4f}")
    
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")
    print(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == '__main__':
    main()
