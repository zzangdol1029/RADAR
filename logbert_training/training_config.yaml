# LogBERT 학습 설정 파일

# 모델 설정
model:
  vocab_size: 10000          # 어휘 크기 (Event ID + Special Tokens)
  hidden_size: 768           # 은닉층 크기
  num_hidden_layers: 12      # Transformer 레이어 수
  num_attention_heads: 12    # 어텐션 헤드 수
  intermediate_size: 3072     # Feed-forward 네트워크 중간 크기
  max_position_embeddings: 512  # 최대 시퀀스 길이
  hidden_dropout_prob: 0.1   # 은닉층 드롭아웃 확률
  attention_probs_dropout_prob: 0.1  # 어텐션 드롭아웃 확률

# 학습 설정
training:
  batch_size: 32             # 배치 크기
  learning_rate: 0.00002     # 학습률 (2e-5를 숫자로 명시)
  weight_decay: 0.01         # 가중치 감쇠 (L2 정규화)
  num_epochs: 10              # 총 에폭 수
  total_steps: 100000        # 총 스텝 수 (스케줄러용)
  min_lr: 0.000001           # 최소 학습률 (1e-6를 숫자로 명시)
  max_grad_norm: 1.0         # Gradient Clipping 최대값
  mask_prob: 0.15            # MLM 마스킹 확률
  log_interval: 100          # 로그 출력 간격 (스텝)
  save_interval: 1000        # 체크포인트 저장 간격 (스텝)
  num_workers: 4             # 데이터 로딩 워커 수

# 데이터 설정
data:
  preprocessed_dir: "../preprocessing/output"  # 전처리된 데이터 디렉토리
  max_seq_length: 512        # 최대 시퀀스 길이

# 출력 설정
output_dir: "checkpoints"    # 체크포인트 저장 디렉토리

