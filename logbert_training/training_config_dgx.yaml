# LogBERT 학습 설정 파일 - DGX Station 최적화 버전
# DGX Station: Tesla V100-DGXS-32GB 4개, 256GB+ 메모리
# GPU: Tesla V100 32GB (각 GPU당 32GB 메모리)

# 모델 설정
model:
  vocab_size: 10000          # 어휘 크기 (Event ID + Special Tokens)
  hidden_size: 768           # 은닉층 크기
  num_hidden_layers: 12      # Transformer 레이어 수
  num_attention_heads: 12    # 어텐션 헤드 수
  intermediate_size: 3072     # Feed-forward 네트워크 중간 크기
  max_position_embeddings: 512  # 최대 시퀀스 길이
  hidden_dropout_prob: 0.1   # 은닉층 드롭아웃 확률
  attention_probs_dropout_prob: 0.1  # 어텐션 드롭아웃 확률

# 학습 설정 (V100 32GB × 4개 멀티 GPU 최적화)
training:
  batch_size: 128            # 배치 크기 (멀티 GPU: 4개 GPU × 32 = 128)
                              # 각 GPU당 32 배치 (예상 메모리 ~12-15GB/GPU)
                              # 총 효과적 배치 크기: 128 (안정적)
                              # 메모리 부족 시 96 (각 GPU당 24)로 더 줄일 수 있음
  use_multi_gpu: true         # 멀티 GPU 사용 (4개 GPU 모두 활용)
  learning_rate: 0.00002     # 학습률 (2e-5를 숫자로 명시)
  weight_decay: 0.01         # 가중치 감쇠 (L2 정규화)
  num_epochs: 10             # 총 에폭 수
  total_steps: 100000        # 총 스텝 수 (스케줄러용)
  min_lr: 0.000001           # 최소 학습률 (1e-6를 숫자로 명시)
  max_grad_norm: 1.0         # Gradient Clipping 최대값
  mask_prob: 0.15            # MLM 마스킹 확률
  log_interval: 100          # 로그 출력 간격 (스텝)
  save_interval: 1000        # 체크포인트 저장 간격 (스텝)
  num_workers: 8             # 데이터 로딩 워커 수 (DGX: 4 -> 8로 증가)

# 데이터 설정
data:
  preprocessed_dir: "../preprocessing/output"  # 전처리된 데이터 디렉토리
  max_seq_length: 512        # 최대 시퀀스 길이

# 출력 설정
output_dir: "checkpoints"    # 체크포인트 저장 디렉토리

