# 점진적 학습 시간 계산 (각 단계마다 10% 데이터 사용)

## 현재 방식

각 단계마다 **전체 데이터의 10%만 사용** (다른 10%씩):
- 10% 단계: 0~10% 데이터
- 20% 단계: 10~20% 데이터
- 30% 단계: 20~30% 데이터
- ...
- 100% 단계: 90~100% 데이터

## 학습 시간 계산

### 기본 설정 (배치 32, 에폭 5)

**각 단계마다:**
- 데이터: 약 43,085개 샘플 (전체의 10%)
- 배치 크기: 32
- 배치 수: 43,085 / 32 = **1,347개**
- 배치당 시간: 약 1.3-1.5초
- 에폭당 시간: 1,347 × 1.4초 ≈ **0.5시간**
- 5 에폭: **2.5시간**

**전체 10단계:**
- 총 시간: 2.5시간 × 10단계 = **25시간** (약 1일)

### 배치 크기별 시간 비교

| 배치 크기 | 배치 수 | 에폭당 시간 | 5 에폭 시간 | 10단계 총 시간 |
|----------|---------|------------|-----------|--------------|
| 16 | 2,694개 | 1.0시간 | 5.0시간 | **50시간** (2.1일) |
| **32** | **1,347개** | **0.5시간** | **2.5시간** | **25시간** (1일) |
| **48** | **898개** | **0.33시간** | **1.7시간** | **17시간** (0.7일) |
| **64** | **674개** | **0.25시간** | **1.25시간** | **12.5시간** (0.5일) |

## 배치 크기 증가 가능 여부

### 메모리 사용량 분석

**10% 데이터에서 16GB 사용 (배치 8 기준, 실제 측정)**

배치 크기와 메모리 사용량:
- 배치 8: 16GB
- 배치 16: 약 20-25GB (예상)
- 배치 32: 약 30-35GB (예상)
- 배치 48: 약 40-45GB (예상)
- 배치 64: 약 50-55GB (예상, 위험)

### M4 Pro 48GB 기준 권장 배치 크기

| 배치 크기 | 예상 메모리 | 안전성 | 권장 |
|----------|-----------|--------|------|
| 32 | 30-35GB | ✅ 안전 | **권장 (기본값)** |
| 48 | 40-45GB | ⚠️ 주의 | **공격적 설정** |
| 64 | 50-55GB | ❌ 위험 | 비권장 |

## 최적 설정 추천

### 설정 1: 빠른 학습 (배치 48)

```bash
python train_transfer.py \
  --progressive \
  --fixed-batch-size 48 \
  --max-memory-mb 45000
```

**예상 시간:**
- 각 단계: 1.7시간
- 전체 10단계: **17시간** (약 0.7일)
- 메모리: 40-45GB

### 설정 2: 균형잡힌 설정 (배치 32, 기본값)

```bash
python train_transfer.py --progressive
```

**예상 시간:**
- 각 단계: 2.5시간
- 전체 10단계: **25시간** (약 1일)
- 메모리: 30-35GB

### 설정 3: 안전한 설정 (배치 24)

```bash
python train_transfer.py \
  --progressive \
  --fixed-batch-size 24 \
  --max-memory-mb 40000
```

**예상 시간:**
- 각 단계: 3.3시간
- 전체 10단계: **33시간** (약 1.4일)
- 메모리: 25-30GB

## 시간 단축 방법

### 방법 1: 배치 크기 증가 (가장 효과적)

```bash
# 배치 32 → 48
python train_transfer.py \
  --progressive \
  --fixed-batch-size 48
```

**시간 절약**: 25시간 → **17시간** (32% 단축)

### 방법 2: 에폭 수 줄이기

```bash
# 에폭 5 → 3
python train_transfer.py \
  --progressive \
  --epochs-per-stage 3
```

**시간 절약**: 25시간 → **15시간** (40% 단축)

### 방법 3: 배치 + 에폭 조합 (가장 빠름)

```bash
python train_transfer.py \
  --progressive \
  --fixed-batch-size 48 \
  --epochs-per-stage 3
```

**시간 절약**: 25시간 → **10시간** (60% 단축)

## 단계별 상세 시간

### 배치 32, 에폭 5 기준

| 단계 | 데이터 범위 | 배치 수 | 단계 시간 | 누적 시간 |
|------|------------|---------|----------|----------|
| 1 | 0~10% | 1,347개 | 2.5시간 | 2.5시간 |
| 2 | 10~20% | 1,347개 | 2.5시간 | 5.0시간 |
| 3 | 20~30% | 1,347개 | 2.5시간 | 7.5시간 |
| 4 | 30~40% | 1,347개 | 2.5시간 | 10.0시간 |
| 5 | 40~50% | 1,347개 | 2.5시간 | 12.5시간 |
| 6 | 50~60% | 1,347개 | 2.5시간 | 15.0시간 |
| 7 | 60~70% | 1,347개 | 2.5시간 | 17.5시간 |
| 8 | 70~80% | 1,347개 | 2.5시간 | 20.0시간 |
| 9 | 80~90% | 1,347개 | 2.5시간 | 22.5시간 |
| 10 | 90~100% | 1,347개 | 2.5시간 | **25.0시간** |

### 배치 48, 에폭 5 기준

| 단계 | 배치 수 | 단계 시간 | 누적 시간 |
|------|---------|----------|----------|
| 1~10 | 898개 | 1.7시간 | **17.0시간** |

### 배치 48, 에폭 3 기준

| 단계 | 배치 수 | 단계 시간 | 누적 시간 |
|------|---------|----------|----------|
| 1~10 | 898개 | 1.0시간 | **10.0시간** |

## 결론

### 학습 시간 요약

**기본 설정 (배치 32, 에폭 5):**
- 전체 10단계: **25시간** (약 1일)

**최적 설정 (배치 48, 에폭 3):**
- 전체 10단계: **10시간** (약 0.4일)

### 배치 크기 권장

**M4 Pro 48GB 기준:**
- ✅ **배치 32**: 안전, 기본값 (25시간)
- ✅ **배치 48**: 공격적, 빠름 (17시간)
- ⚠️ 배치 64: 위험, 메모리 부족 가능

**추천: 배치 48 사용** (메모리 여유 있음, 32% 시간 단축)

