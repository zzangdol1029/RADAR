# 학습 곡선 시각화 가이드

## 📊 학습 곡선이란?

학습 곡선(Training Curve)은 모델 학습 과정에서 Loss, Accuracy 등의 지표가 시간에 따라 어떻게 변화하는지를 시각화한 그래프입니다.

### 학습 곡선의 중요성

1. **학습 진행 상황 모니터링**
   - Loss가 제대로 감소하는지 확인
   - 학습이 수렴하는지 확인
   - 과적합 발생 여부 확인

2. **문제 진단**
   - Loss가 감소하지 않으면 학습률 조정 필요
   - Loss가 불안정하면 배치 크기 조정 필요
   - Loss가 너무 낮으면 과적합 가능성

3. **성능 평가**
   - 최종 성능 예측
   - 추가 학습 필요 여부 판단

---

## 📈 학습 곡선의 종류

### 1. Step별 Loss 곡선

**의미:**
- 각 학습 스텝에서의 Loss 값
- 학습의 세부적인 진행 상황 확인

**특징:**
- 노이즈가 많음 (배치마다 다름)
- 전체적인 추세 파악 가능
- 이상치 감지 가능

**해석:**
- ✅ 감소 추세: 정상 학습
- ❌ 증가 추세: 문제 발생 (학습률 너무 높음 등)
- ⚠️ 불안정: 배치 크기 조정 필요

### 2. 이동 평균 Loss 곡선

**의미:**
- 여러 스텝의 평균 Loss
- 노이즈를 제거한 부드러운 곡선

**특징:**
- 전체적인 추세 파악 용이
- 학습 속도 확인 가능

**해석:**
- ✅ 부드럽게 감소: 안정적인 학습
- ⚠️ 급격히 감소: 학습률이 높을 수 있음
- ❌ 평평: 학습이 멈춤

### 3. 에폭별 평균 Loss

**의미:**
- 각 에폭이 끝날 때의 평균 Loss
- 에폭 간 성능 비교

**특징:**
- 큰 그림 파악 용이
- 에폭별 개선 정도 확인

**해석:**
- ✅ 지속적 감소: 좋은 학습
- ⚠️ 감소 멈춤: 수렴 또는 과적합
- ❌ 증가: 과적합 발생

### 4. 학습률 스케줄

**의미:**
- 시간에 따른 학습률 변화
- 스케줄러 동작 확인

**특징:**
- CosineAnnealingLR: 코사인 곡선으로 감소
- StepLR: 단계적으로 감소

**해석:**
- ✅ 부드러운 감소: 정상 동작
- ❌ 급격한 변화: 스케줄러 설정 확인 필요

---

## 🎯 이상적인 학습 곡선

### 정상적인 학습 곡선

```
Loss
  |
  |  ╱╲╱╲╱╲╱╲╱╲
  | ╱        ╲
  |╱          ╲
  |             ╲
  |              ╲___
  |___________________
     Step/Epoch
```

**특징:**
- 초기에 빠르게 감소
- 점점 완만하게 감소
- 수렴 (더 이상 크게 감소하지 않음)

### 이상적인 Loss 감소 패턴

**에폭 1-3:**
- 빠른 감소 (50-70% 감소)
- Loss: 5.0 → 1.5

**에폭 4-7:**
- 중간 속도 감소 (30-50% 감소)
- Loss: 1.5 → 0.5

**에폭 8-10:**
- 느린 감소 (10-30% 감소)
- Loss: 0.5 → 0.2

---

## ⚠️ 문제가 있는 학습 곡선

### 1. Loss가 감소하지 않음

```
Loss
  |
  |___________________
  |                   |
  |___________________
     Step/Epoch
```

**원인:**
- 학습률이 너무 낮음
- 모델 용량 부족
- 데이터 문제

**해결:**
- 학습률 증가
- 모델 크기 증가
- 데이터 확인

### 2. Loss가 불안정함

```
Loss
  |
  |  ╱╲╱╲╱╲╱╲╱╲╱╲
  | ╱  ╲╱  ╲╱  ╲╱
  |╱    ╲    ╲    ╲
  |___________________
     Step/Epoch
```

**원인:**
- 학습률이 너무 높음
- 배치 크기가 너무 작음

**해결:**
- 학습률 감소
- 배치 크기 증가

### 3. 과적합 (Overfitting)

```
Loss
  |
  |  ╱╲
  | ╱  ╲
  |╱    ╲___
  |          ╲___
  |              ╲___
  |___________________
     Step/Epoch
```

**특징:**
- 학습 Loss는 계속 감소
- 검증 Loss는 증가 또는 정체

**해결:**
- 드롭아웃 증가
- 정규화 강화
- 조기 종료 (Early Stopping)

---

## 📊 현재 모델의 학습 곡선 분석

### 예상되는 학습 곡선

**Loss 0.2029 달성 과정:**

```
에폭 1: Loss 5.6560 → 1.5-2.0 (약 70% 감소)
에폭 2: Loss 1.5-2.0 → 1.0-1.5 (약 30% 감소)
에폭 3-5: Loss 1.0-1.5 → 0.5-1.0 (약 40% 감소)
에폭 6-10: Loss 0.5-1.0 → 0.2-0.5 (약 60% 감소)
```

**특징:**
- ✅ 지속적인 감소
- ✅ 안정적인 학습
- ✅ 우수한 최종 성능

---

## 🔧 학습 곡선 시각화 방법

### 방법 1: Python 스크립트 사용

```bash
# 학습 곡선 시각화 스크립트 실행
python3 plot_training_curve.py --log logs/training_20260105_090738.log --output plots/
```

**출력:**
- `training_curves.png`: 6개의 서브플롯
  1. Step별 Loss (전체)
  2. Step별 Loss (스무딩)
  3. 에폭별 평균 Loss
  4. 학습률 변화
  5. Loss 감소율
  6. 에폭별 Loss 분포

### 방법 2: TensorBoard 사용 (향후)

```bash
# TensorBoard 로그 생성 (향후 구현)
tensorboard --logdir=logs/tensorboard/
```

### 방법 3: 수동 분석

```python
import re
import matplotlib.pyplot as plt

# 로그 파일에서 Loss 추출
losses = []
with open('logs/training_20260105_090738.log', 'r') as f:
    for line in f:
        if 'loss=' in line and 'avg=' in line:
            # Loss 값 추출
            match = re.search(r'avg=([\d.]+)', line)
            if match:
                losses.append(float(match.group(1)))

# 그래프 그리기
plt.plot(losses)
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```

---

## 📈 학습 곡선 해석 가이드

### Loss 감소율 분석

**초기 10% 스텝:**
- 목표: 30-50% 감소
- 예: 5.0 → 2.5-3.5

**중기 50% 스텝:**
- 목표: 60-80% 감소
- 예: 5.0 → 1.0-2.0

**후기 90% 스텝:**
- 목표: 85-95% 감소
- 예: 5.0 → 0.25-0.75

### 에폭별 개선 정도

**정상적인 패턴:**
- 에폭 1-2: 큰 개선 (50-70% 감소)
- 에폭 3-5: 중간 개선 (30-50% 감소)
- 에폭 6-10: 작은 개선 (10-30% 감소)

**이상적인 패턴:**
- 모든 에폭에서 큰 개선: 학습률 높일 수 있음
- 모든 에폭에서 작은 개선: 학습률 낮출 수 있음
- 중간에 개선 멈춤: 과적합 가능성

---

## 💡 실용적인 팁

### 1. 로그 스케일 사용

Loss가 크게 변할 때는 로그 스케일로 보면 더 명확합니다.

```python
plt.yscale('log')
```

### 2. 이동 평균 사용

노이즈가 많을 때는 이동 평균으로 부드럽게 만듭니다.

```python
window = 100
smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')
```

### 3. 여러 메트릭 동시 확인

Loss만 보지 말고 학습률, Gradient Norm 등도 함께 확인합니다.

### 4. 에폭별 비교

에폭별로 나누어 비교하면 패턴을 더 잘 파악할 수 있습니다.

---

## 🎯 현재 모델 평가

### 예상 학습 곡선

**Loss 0.2029 달성:**
- ✅ 매우 우수한 학습 곡선 예상
- ✅ 지속적인 감소
- ✅ 안정적인 수렴

**다음 단계:**
1. 실제 학습 곡선 확인 (`plot_training_curve.py` 실행)
2. 에폭별 개선 정도 분석
3. 최적 에폭 수 결정 (향후 학습 시)

---

## 📝 요약

### 학습 곡선의 핵심

1. **Loss 감소 추세 확인**
   - 지속적으로 감소하는가?
   - 불안정한가?

2. **수렴 여부 확인**
   - 더 이상 감소하지 않는가?
   - 과적합이 발생했는가?

3. **에폭별 개선 확인**
   - 각 에폭에서 얼마나 개선되는가?
   - 언제 학습을 멈춰야 하는가?

### 현재 모델

**Loss 0.2029:**
- ✅ 매우 우수한 최종 성능
- ✅ 학습 곡선 확인으로 학습 과정 분석 가능
- ✅ 향후 학습 시 참고 자료로 활용

**학습 곡선을 확인하여 학습 과정을 분석해보세요!** 📊


