#!/usr/bin/env python3
"""
LogBERT ì „ì´ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Pre-trained BERT ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ M4 Proì—ì„œë„ í•™ìŠµ ê°€ëŠ¥
"""

import os
import json
import yaml
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging
from logging.handlers import RotatingFileHandler
from tqdm import tqdm
import numpy as np
from datetime import datetime
import random
import gc
import psutil
import sys

from transformers import BertForMaskedLM, BertConfig
from dataset import LogBERTDataset, create_dataloader, collate_fn

# ë¡œê±° ì„¤ì • (ê¸°ë³¸)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_memory_usage() -> Dict[str, float]:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return {
        'rss': mem_info.rss / 1024 / 1024,  # Resident Set Size (ì‹¤ì œ ë©”ëª¨ë¦¬)
        'vms': mem_info.vms / 1024 / 1024,  # Virtual Memory Size
        'percent': process.memory_percent(),  # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ë¹„ìœ¨
    }


def get_cpu_usage() -> float:
    """í˜„ì¬ CPU ì‚¬ìš©ë¥  ë°˜í™˜ (%)"""
    return psutil.cpu_percent(interval=0.1)


def log_resource_usage(logger_instance: logging.Logger, prefix: str = ""):
    """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
    mem = get_memory_usage()
    cpu = get_cpu_usage()
    msg = (
        f"{prefix}ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ - "
        f"ë©”ëª¨ë¦¬: {mem['rss']:.1f}MB (ì‹œìŠ¤í…œ {mem['percent']:.1f}%), "
        f"CPU: {cpu:.1f}%"
    )
    logger_instance.info(msg)


def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    # Pythonì˜ ë©”ëª¨ë¦¬ ì •ë¦¬
    if hasattr(gc, 'collect'):
        gc.collect()


def setup_file_logger(log_dir: Path, log_name: str = 'training') -> logging.Logger:
    """
    íŒŒì¼ ë¡œê±° ì„¤ì •
    
    Args:
        log_dir: ë¡œê·¸ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        log_name: ë¡œê·¸ íŒŒì¼ëª…
    
    Returns:
        ì„¤ì •ëœ ë¡œê±°
    """
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f'{log_name}_{timestamp}.log'
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„±
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(
        logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    )
    
    # ë¡œê±°ì— í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_logger = logging.getLogger(f'{__name__}.{log_name}')
    file_logger.setLevel(logging.INFO)
    file_logger.addHandler(file_handler)
    file_logger.propagate = False  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€
    
    logger.info(f"ë¡œê·¸ íŒŒì¼ ìƒì„±: {log_file}")
    return file_logger


class TransferLogBERT(nn.Module):
    """
    ì „ì´ í•™ìŠµìš© LogBERT ëª¨ë¸
    Pre-trained BERTë¥¼ ë¡œë“œí•˜ì—¬ íŒŒì¸íŠœë‹
    """
    
    def __init__(
        self,
        pretrained_model_name: str = 'bert-base-uncased',
        vocab_size: int = 10000,
    ):
        """
        Args:
            pretrained_model_name: Hugging Face ëª¨ë¸ëª…
            vocab_size: ë¡œê·¸ ë°ì´í„°ì˜ ì–´íœ˜ í¬ê¸°
        """
        super(TransferLogBERT, self).__init__()
        
        # Pre-trained BERT ë¡œë“œ
        logger.info(f"Pre-trained BERT ë¡œë“œ ì¤‘: {pretrained_model_name}")
        self.bert = BertForMaskedLM.from_pretrained(pretrained_model_name)
        
        # ì–´íœ˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì„ë² ë”© ë ˆì´ì–´ ì¬ì´ˆê¸°í™”
        if vocab_size != self.bert.config.vocab_size:
            logger.info(f"ì–´íœ˜ í¬ê¸° ì¡°ì •: {self.bert.config.vocab_size} -> {vocab_size}")
            # ì„ë² ë”© ë ˆì´ì–´ ì¬ìƒì„±
            old_embeddings = self.bert.bert.embeddings.word_embeddings
            new_embeddings = nn.Embedding(vocab_size, old_embeddings.embedding_dim)
            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³µì‚¬ (ê°€ëŠ¥í•œ ë²”ìœ„ë§Œ)
            min_size = min(vocab_size, old_embeddings.num_embeddings)
            new_embeddings.weight.data[:min_size] = old_embeddings.weight.data[:min_size]
            self.bert.bert.embeddings.word_embeddings = new_embeddings
            self.bert.config.vocab_size = vocab_size
            # MLM í—¤ë“œë„ ì¬ìƒì„±
            self.bert.cls.predictions.decoder = nn.Linear(
                self.bert.config.hidden_size,
                vocab_size
            )
            self.bert.cls.predictions.bias = nn.Parameter(torch.zeros(vocab_size))
        
        logger.info("ì „ì´ í•™ìŠµ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor]:
        """Forward pass"""
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
        )
        
        return {
            'loss': outputs.loss,
            'logits': outputs.logits,
        }
    
    def predict_anomaly_score(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """ì´ìƒ ì ìˆ˜ ê³„ì‚°"""
        self.eval()
        with torch.no_grad():
            outputs = self.forward(input_ids, attention_mask)
            logits = outputs['logits']
            
            # Softmaxë¡œ í™•ë¥  ê³„ì‚°
            probs = torch.softmax(logits, dim=-1)
            
            # ì‹¤ì œ í† í°ì˜ í™•ë¥  ì¶”ì¶œ
            batch_size, seq_len = input_ids.shape
            token_probs = probs[torch.arange(batch_size).unsqueeze(1), 
                                torch.arange(seq_len).unsqueeze(0), 
                                input_ids]
            
            # ìŒì˜ ë¡œê·¸ í™•ë¥ 
            anomaly_scores = -torch.log(token_probs + 1e-10)
            
            # íŒ¨ë”© ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ ì„¤ì •
            if attention_mask is not None:
                anomaly_scores = anomaly_scores * attention_mask.float()
            
            # ì‹œí€€ìŠ¤ë³„ í‰ê·  ì´ìƒ ì ìˆ˜
            if attention_mask is not None:
                seq_scores = anomaly_scores.sum(dim=1) / attention_mask.sum(dim=1).float()
            else:
                seq_scores = anomaly_scores.mean(dim=1)
            
            return seq_scores


class TransferTrainer:
    """ì „ì´ í•™ìŠµìš© íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: Dict[str, Any], load_checkpoint: Optional[str] = None, file_logger: Optional[logging.Logger] = None):
        self.config = config
        self.device = torch.device('cpu')  # M4 ProëŠ” CPUë§Œ
        self.file_logger = file_logger or logger  # íŒŒì¼ ë¡œê±° ë˜ëŠ” ê¸°ë³¸ ë¡œê±°
        
        logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self.file_logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = Path(config.get('output_dir', 'checkpoints_transfer'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ì „ì´ í•™ìŠµ)
        self.model = TransferLogBERT(
            pretrained_model_name=config.get('pretrained_model', 'bert-base-uncased'),
            vocab_size=config['model']['vocab_size'],
        )
        self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € (ì‘ì€ í•™ìŠµë¥ ë¡œ íŒŒì¸íŠœë‹)
        learning_rate = float(config['training']['learning_rate'])
        weight_decay = float(config['training'].get('weight_decay', 0.01))
        
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        total_steps = int(config['training'].get('total_steps', 10000))
        min_lr = float(config['training'].get('min_lr', 1e-6))
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=min_lr,
        )
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.best_loss = float('inf')
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
        self.training_metrics = {
            'epochs': [],
            'losses': [],
            'learning_rates': [],
            'steps': [],
        }
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
        self.checkpoint_dir = self.output_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì ì§„ì  í•™ìŠµìš©)
        if load_checkpoint:
            self.load_checkpoint(load_checkpoint)
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.global_step = checkpoint.get('global_step', 0)
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
        logger.info(f"  - Global Step: {self.global_step}")
        logger.info(f"  - Best Loss: {self.best_loss:.4f}")
    
    def train_epoch(self, dataloader, epoch: int) -> float:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        total_batches = len(dataloader)
        
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch}/{self.config['training']['num_epochs']}",
            total=total_batches,
            unit="batch",
            leave=True,
            ncols=100
        )
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )
            
            loss = outputs['loss']
            
            self.optimizer.zero_grad()
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config['training'].get('max_grad_norm', 1.0)
            )
            
            self.optimizer.step()
            self.scheduler.step()
            
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            current_lr = self.optimizer.param_groups[0]['lr']
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg': f'{total_loss / num_batches:.4f}',
                'lr': f'{current_lr:.2e}',
            })
            
            if self.global_step % self.config['training'].get('log_interval', 50) == 0:
                log_msg = (
                    f"[Step {self.global_step}] "
                    f"Loss={loss.item():.4f}, "
                    f"Avg Loss={total_loss/num_batches:.4f}, "
                    f"LR={current_lr:.2e}"
                )
                logger.info(log_msg)
                self.file_logger.info(log_msg)
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def save_checkpoint(self, name: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoint_dir / f'{name}.pt'
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_loss': self.best_loss,
            'config': self.config,
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def train(self, train_dataloader, num_epochs: int, stage_name: str = ""):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        stage_prefix = f"[{stage_name}] " if stage_name else ""
        
        # í•™ìŠµ ì‹œì‘ ë¡œê·¸
        start_msg = f"{stage_prefix}LogBERT ì „ì´ í•™ìŠµ ì‹œì‘ (Pre-trained BERT íŒŒì¸íŠœë‹)"
        logger.info("=" * 80)
        logger.info(start_msg)
        logger.info("=" * 80)
        logger.info(f"ì´ ì—í­: {num_epochs}")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.config['training']['batch_size']}")
        logger.info(f"í•™ìŠµë¥ : {self.config['training']['learning_rate']}")
        logger.info("=" * 80)
        
        self.file_logger.info("=" * 80)
        self.file_logger.info(start_msg)
        self.file_logger.info("=" * 80)
        self.file_logger.info(f"ì´ ì—í­: {num_epochs}")
        self.file_logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.config['training']['batch_size']}")
        self.file_logger.info(f"í•™ìŠµë¥ : {self.config['training']['learning_rate']}")
        self.file_logger.info("=" * 80)
        
        for epoch in range(1, num_epochs + 1):
            epoch_start_msg = f"{stage_prefix}ì—í­ {epoch}/{num_epochs} ì‹œì‘"
            logger.info(f"\n{epoch_start_msg}")
            self.file_logger.info(f"\n{epoch_start_msg}")
            
            avg_loss = self.train_epoch(train_dataloader, epoch)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            self.training_metrics['epochs'].append(epoch)
            self.training_metrics['losses'].append(float(avg_loss))
            self.training_metrics['learning_rates'].append(float(current_lr))
            self.training_metrics['steps'].append(self.global_step)
            
            epoch_end_msg = f"{stage_prefix}ì—í­ {epoch} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}, LR: {current_lr:.2e}"
            logger.info(epoch_end_msg)
            self.file_logger.info(epoch_end_msg)
            
            if avg_loss < self.best_loss:
                improvement = self.best_loss - avg_loss
                self.best_loss = avg_loss
                checkpoint_name = f'best_model_{stage_name}' if stage_name else 'best_model'
                self.save_checkpoint(checkpoint_name)
                best_msg = f"{stage_prefix}âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (Loss: {avg_loss:.4f}, ê°œì„ : {improvement:.4f})"
                logger.info(best_msg)
                self.file_logger.info(best_msg)
            
            epoch_name = f'epoch_{epoch}_{stage_name}' if stage_name else f'epoch_{epoch}'
            self.save_checkpoint(epoch_name)
        
        # í•™ìŠµ ì™„ë£Œ ë¡œê·¸
        logger.info("=" * 80)
        logger.info(f"{stage_prefix}ì „ì´ í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ìµœê³  Loss: {self.best_loss:.4f}")
        logger.info("=" * 80)
        
        self.file_logger.info("=" * 80)
        self.file_logger.info(f"{stage_prefix}ì „ì´ í•™ìŠµ ì™„ë£Œ!")
        self.file_logger.info(f"ìµœê³  Loss: {self.best_loss:.4f}")
        self.file_logger.info("=" * 80)
        
        return self.best_loss
    
    def save_metrics(self, stage_name: str = ""):
        """í•™ìŠµ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        metrics_file = self.output_dir / f'training_metrics_{stage_name}.json' if stage_name else self.output_dir / 'training_metrics.json'
        
        metrics_data = {
            'stage': stage_name,
            'best_loss': float(self.best_loss),
            'global_step': self.global_step,
            'metrics': self.training_metrics,
            'config': self.config,
        }
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_file}")
        self.file_logger.info(f"í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_file}")


def load_config() -> Dict[str, Any]:
    """ì „ì´ í•™ìŠµìš© ì„¤ì •"""
    return {
        'pretrained_model': 'bert-base-uncased',  # ë˜ëŠ” 'distilbert-base-uncased' (ë” ì‘ìŒ)
        'model': {
            'vocab_size': 10000,
        },
        'training': {
            'batch_size': 8,  # ë°œì—´ ê°ì†Œë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ (ê¸°ë³¸ê°’)
            'learning_rate': 0.00001,  # ì‘ì€ í•™ìŠµë¥  (íŒŒì¸íŠœë‹)
            'weight_decay': 0.01,
            'num_epochs': 5,  # ë” ë§ì€ ì—í­ (48GB ë©”ëª¨ë¦¬ë¡œ ê°€ëŠ¥)
            'total_steps': 10000,  # ë” ë§ì€ ìŠ¤í…
            'min_lr': 0.000001,
            'max_grad_norm': 1.0,
            'mask_prob': 0.15,
            'log_interval': 50,
            'save_interval': 500,
            'num_workers': 0,
        },
        'data': {
            'preprocessed_dir': '../preprocessing/output',
            'max_seq_length': 512,  # ë” ê¸´ ì‹œí€€ìŠ¤ (48GB ë©”ëª¨ë¦¬ë¡œ ê°€ëŠ¥)
            'sample_ratio': 0.1,  # 10% ë°ì´í„° (48GB ë©”ëª¨ë¦¬ë¡œ ë” ë§ì€ ë°ì´í„° ê°€ëŠ¥)
            'max_files': 50,  # ë” ë§ì€ íŒŒì¼ (48GB ë©”ëª¨ë¦¬ë¡œ ê°€ëŠ¥)
        },
        'output_dir': 'checkpoints_transfer',
    }


def get_data_files(preprocessed_dir: str, max_files: int = 10) -> list:
    """ë°ì´í„° íŒŒì¼ ëª©ë¡"""
    data_dir = Path(preprocessed_dir)
    files = sorted(data_dir.glob("preprocessed_logs_*.json"))
    
    if len(files) > max_files:
        files = random.sample(files, max_files)
    
    logger.info(f"ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼: {len(files)}ê°œ")
    return [str(f) for f in files]


def train_progressive(
    config: Dict[str, Any],
    data_files: List[str],
    start_ratio: float = 0.1,
    step_size: float = 0.1,
    max_ratio: float = 1.0,
    epochs_per_stage: int = 2,
    auto_batch_size: bool = True,
    max_memory_mb: Optional[float] = None,
    min_batch_size: int = 1,
    fixed_batch_size: Optional[int] = None,
):
    """
    ì ì§„ì  í•™ìŠµ: 10%ë¶€í„° ì‹œì‘í•´ì„œ ë‹¨ê³„ì ìœ¼ë¡œ ë°ì´í„° ì¦ê°€
    
    Args:
        config: í•™ìŠµ ì„¤ì •
        data_files: ë°ì´í„° íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        start_ratio: ì‹œì‘ ë¹„ìœ¨ (0.1 = 10%)
        step_size: ê° ë‹¨ê³„ ì¦ê°€ëŸ‰ (0.1 = 10%)
        max_ratio: ìµœëŒ€ ë¹„ìœ¨ (1.0 = 100%)
        epochs_per_stage: ê° ë‹¨ê³„ë‹¹ ì—í­ ìˆ˜
        auto_batch_size: ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        max_memory_mb: ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (MB, Noneì´ë©´ ì œí•œ ì—†ìŒ)
        min_batch_size: ìë™ ì¡°ì • ì‹œ ìµœì†Œ ë°°ì¹˜ í¬ê¸°
        fixed_batch_size: ê³ ì • ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ìë™ ì¡°ì •)
    """
    from train_test import SampledLogBERTDataset
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(config.get('output_dir', 'checkpoints_transfer'))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì „ì²´ ì ì§„ì  í•™ìŠµ ë¡œê·¸ íŒŒì¼ ì„¤ì •
    progressive_logger = setup_file_logger(output_dir, 'progressive_training')
    
    # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
    initial_mem = get_memory_usage()
    initial_cpu = get_cpu_usage()
    
    logger.info("=" * 80)
    logger.info("ì ì§„ì  í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"ì‹œì‘ ë¹„ìœ¨: {start_ratio*100:.0f}%")
    logger.info(f"ë‹¨ê³„ í¬ê¸°: {step_size*100:.0f}%")
    logger.info(f"ìµœëŒ€ ë¹„ìœ¨: {max_ratio*100:.0f}%")
    logger.info(f"ë‹¨ê³„ë‹¹ ì—í­: {epochs_per_stage}")
    logger.info(f"ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {auto_batch_size}")
    if max_memory_mb:
        logger.info(f"ìµœëŒ€ ë©”ëª¨ë¦¬ ì œí•œ: {max_memory_mb:.0f}MB")
    logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_mem['rss']:.1f}MB ({initial_mem['percent']:.1f}%)")
    logger.info(f"ì´ˆê¸° CPU: {initial_cpu:.1f}%")
    logger.info("=" * 80)
    
    progressive_logger.info("=" * 80)
    progressive_logger.info("ì ì§„ì  í•™ìŠµ ì‹œì‘")
    progressive_logger.info("=" * 80)
    progressive_logger.info(f"ì‹œì‘ ë¹„ìœ¨: {start_ratio*100:.0f}%")
    progressive_logger.info(f"ë‹¨ê³„ í¬ê¸°: {step_size*100:.0f}%")
    progressive_logger.info(f"ìµœëŒ€ ë¹„ìœ¨: {max_ratio*100:.0f}%")
    progressive_logger.info(f"ë‹¨ê³„ë‹¹ ì—í­: {epochs_per_stage}")
    progressive_logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_mem['rss']:.1f}MB")
    progressive_logger.info("=" * 80)
    
    # ê²°ê³¼ ì €ì¥
    results = []
    previous_checkpoint = None
    base_batch_size = config['training']['batch_size']
    
    # ëˆ„ì  ë°ì´í„° ì¸ë±ìŠ¤ ì €ì¥ (ì ì§„ì  í•™ìŠµì„ ìœ„í•´)
    cumulative_indices = None
    
    # ê° ë‹¨ê³„ë³„ í•™ìŠµ
    current_ratio = start_ratio
    stage_num = 1
    
    while current_ratio <= max_ratio:
        stage_name = f"stage_{stage_num}_{int(current_ratio*100)}pct"
        
        # ì´ì „ ë‹¨ê³„ ì •ë¦¬
        if stage_num > 1:
            logger.info("\nì´ì „ ë‹¨ê³„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
            cleanup_memory()
            log_resource_usage(logger, "ì •ë¦¬ í›„ ")
            log_resource_usage(progressive_logger, "ì •ë¦¬ í›„ ")
        
        logger.info("\n" + "=" * 80)
        logger.info(f"ë‹¨ê³„ {stage_num}: {current_ratio*100:.0f}% ë°ì´í„°ë¡œ í•™ìŠµ")
        logger.info("=" * 80)
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        if fixed_batch_size is not None:
            # ê³ ì • ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            config['training']['batch_size'] = fixed_batch_size
            logger.info(f"ë°°ì¹˜ í¬ê¸° ê³ ì •: {fixed_batch_size}")
            progressive_logger.info(f"ë°°ì¹˜ í¬ê¸°: {fixed_batch_size} (ê³ ì •)")
        elif auto_batch_size:
            # ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¼ ìë™ ì¡°ì •
            # í•˜ì§€ë§Œ ìµœì†Œ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ìœ ì§€ (ì„±ëŠ¥ì„ ìœ„í•´)
            adjusted_batch_size = max(min_batch_size, int(base_batch_size * current_ratio))
            config['training']['batch_size'] = adjusted_batch_size
            logger.info(f"ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •: {base_batch_size} â†’ {adjusted_batch_size} (ë¹„ìœ¨: {current_ratio*100:.0f}%, ìµœì†Œ: {min_batch_size})")
            progressive_logger.info(f"ë°°ì¹˜ í¬ê¸°: {adjusted_batch_size}")
        # else: auto_batch_sizeê°€ Falseë©´ base_batch_size ì‚¬ìš©
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        mem_before = get_memory_usage()
        log_resource_usage(logger, f"[ë‹¨ê³„ {stage_num}] ì‹œì‘ ì „ ")
        log_resource_usage(progressive_logger, f"[ë‹¨ê³„ {stage_num}] ì‹œì‘ ì „ ")
        
        # ë°ì´í„°ì…‹ ìƒì„± (ê° ë‹¨ê³„ë§ˆë‹¤ ìƒˆë¡œìš´ 10% ë°ì´í„°ë§Œ ì‚¬ìš©)
        # 10% ë‹¨ê³„: 0~10% ë°ì´í„°
        # 20% ë‹¨ê³„: 10~20% ë°ì´í„° (ìƒˆë¡œìš´ 10%)
        # 30% ë‹¨ê³„: 20~30% ë°ì´í„° (ìƒˆë¡œìš´ 10%)
        # ...
        # 100% ë‹¨ê³„: 90~100% ë°ì´í„° (ìƒˆë¡œìš´ 10%)
        # ê° ë‹¨ê³„ë§ˆë‹¤ í•­ìƒ ì „ì²´ ë°ì´í„°ì˜ 10%ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì¼ì •)
        
        prev_ratio = current_ratio - step_size if stage_num > 1 else 0.0
        
        if stage_num == 1:
            # ì²« ë‹¨ê³„: 0~10% ë°ì´í„° ì‚¬ìš©
            logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì¤‘... (ë¹„ìœ¨: 0~{current_ratio*100:.0f}%, {step_size*100:.0f}% ì‚¬ìš©)")
        else:
            # ì´í›„ ë‹¨ê³„: ìƒˆë¡œìš´ 10% ë°ì´í„°ë§Œ ì‚¬ìš©
            logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì¤‘... (ë¹„ìœ¨: {prev_ratio*100:.0f}~{current_ratio*100:.0f}%, ìƒˆë¡œìš´ {step_size*100:.0f}% ì‚¬ìš©)")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ (í•œ ë²ˆë§Œ, ì²« ë‹¨ê³„ì—ì„œ)
        if stage_num == 1:
            # ì²« ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì €ì¥
            full_dataset = SampledLogBERTDataset(
                data_files=data_files,
                max_seq_length=config['data']['max_seq_length'],
                mask_prob=config['training'].get('mask_prob', 0.15),
                vocab_size=config['model']['vocab_size'],
                sample_ratio=1.0,  # ì „ì²´ ë°ì´í„° ë¡œë“œ
                max_files=config['data'].get('max_files', 10),
            )
            # ì „ì²´ ì„¸ì…˜ ì €ì¥ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì¬ì‚¬ìš©)
            cumulative_indices = full_dataset.sessions.copy()
            total_size = len(cumulative_indices)
        
        # í˜„ì¬ ë‹¨ê³„ì— ì‚¬ìš©í•  ë°ì´í„° ë²”ìœ„ ê³„ì‚°
        start_idx = int(total_size * prev_ratio)
        end_idx = int(total_size * current_ratio)
        
        # ìƒˆë¡œìš´ 10% ë°ì´í„°ë§Œ ì‚¬ìš©
        dataset = SampledLogBERTDataset(
            data_files=data_files,
            max_seq_length=config['data']['max_seq_length'],
            mask_prob=config['training'].get('mask_prob', 0.15),
            vocab_size=config['model']['vocab_size'],
            sample_ratio=1.0,
            max_files=config['data'].get('max_files', 10),
        )
        # í•´ë‹¹ ë²”ìœ„ì˜ ë°ì´í„°ë§Œ ì‚¬ìš© (í•­ìƒ 10%)
        dataset.sessions = cumulative_indices[start_idx:end_idx]
        
        logger.info(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset):,}ê°œ ìƒ˜í”Œ (ì „ì²´ì˜ {step_size*100:.0f}%, ë²”ìœ„: {prev_ratio*100:.0f}~{current_ratio*100:.0f}%)")
        
        # ë°ì´í„°ì…‹ ìƒì„± í›„ ë©”ëª¨ë¦¬ í™•ì¸
        mem_after_dataset = get_memory_usage()
        dataset_mem = mem_after_dataset['rss'] - mem_before['rss']
        logger.info(f"ë°ì´í„°ì…‹ ë©”ëª¨ë¦¬ ì‚¬ìš©: +{dataset_mem:.1f}MB")
        
        # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        current_batch_size = config['training']['batch_size']
        if max_memory_mb and mem_after_dataset['rss'] > max_memory_mb:
            logger.warning(
                f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: "
                f"{mem_after_dataset['rss']:.1f}MB > {max_memory_mb:.0f}MB"
            )
            
            # ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • (ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ)
            if fixed_batch_size is None:  # ê³ ì • ë°°ì¹˜ í¬ê¸°ê°€ ì•„ë‹ ë•Œë§Œ ìë™ ì¡°ì •
                # ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (ë°ì´í„° ë¹„ìœ¨ì— ë¹„ë¡€)
                estimated_memory_ratio = mem_after_dataset['rss'] / max_memory_mb
                # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
                new_batch_size = max(min_batch_size, int(current_batch_size / estimated_memory_ratio))
                
                if new_batch_size < current_batch_size:
                    config['training']['batch_size'] = new_batch_size
                    logger.warning(
                        f"âš ï¸ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤: {current_batch_size} â†’ {new_batch_size} "
                        f"(ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€, ìµœì†Œ: {min_batch_size})"
                    )
                    progressive_logger.warning(
                        f"ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •: {current_batch_size} â†’ {new_batch_size} "
                        f"(ë©”ëª¨ë¦¬: {mem_after_dataset['rss']:.1f}MB > {max_memory_mb:.0f}MB)"
                    )
                else:
                    logger.error(
                        f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤. "
                        f"ìµœì†Œ ë°°ì¹˜ í¬ê¸°({min_batch_size})ë¡œë„ ë©”ëª¨ë¦¬ ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤. "
                        f"ë°ì´í„° ë¹„ìœ¨ì„ ì¤„ì´ê±°ë‚˜ ë©”ëª¨ë¦¬ ì œí•œì„ ë†’ì´ì„¸ìš”."
                    )
                    progressive_logger.error(
                        f"ë©”ëª¨ë¦¬ ì´ˆê³¼: {mem_after_dataset['rss']:.1f}MB > {max_memory_mb:.0f}MB, "
                        f"ë°°ì¹˜ í¬ê¸° ì¡°ì • ë¶ˆê°€ (ì´ë¯¸ ìµœì†Œê°’: {min_batch_size})"
                    )
            else:
                logger.warning(
                    f"âš ï¸ ê³ ì • ë°°ì¹˜ í¬ê¸°({fixed_batch_size})ë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. "
                    f"ë©”ëª¨ë¦¬ ì´ˆê³¼ë¥¼ ë°©ì§€í•˜ë ¤ë©´ --fixed-batch-sizeë¥¼ ì œê±°í•˜ê±°ë‚˜ ë” ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”."
                )
                progressive_logger.warning(
                    f"ë©”ëª¨ë¦¬ ì´ˆê³¼: {mem_after_dataset['rss']:.1f}MB > {max_memory_mb:.0f}MB, "
                    f"ê³ ì • ë°°ì¹˜ í¬ê¸°ë¡œ ì¸í•´ ìë™ ì¡°ì • ë¶ˆê°€"
                )
        
        # 100% ë°ì´í„°ì¼ ë•Œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì²´í¬ ë° ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if current_ratio >= 0.9 and max_memory_mb:  # 90% ì´ìƒì¼ ë•Œ
            # ì˜ˆìƒ í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë°°ì¹˜ í¬ê¸°ì— ë¹„ë¡€)
            estimated_training_memory = mem_after_dataset['rss'] * (1.5 + current_batch_size / 16)
            
            if estimated_training_memory > max_memory_mb * 0.9:  # 90% ì´ìƒ ì‚¬ìš© ì˜ˆìƒ
                if fixed_batch_size is None:  # ê³ ì • ë°°ì¹˜ í¬ê¸°ê°€ ì•„ë‹ ë•Œë§Œ
                    # ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •
                    safe_batch_size = max(min_batch_size, int(current_batch_size * 0.75))
                    if safe_batch_size < current_batch_size:
                        config['training']['batch_size'] = safe_batch_size
                        logger.info(
                            f"ğŸ’¡ 100% ë°ì´í„° ë‹¨ê³„ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •: {current_batch_size} â†’ {safe_batch_size} "
                            f"(ì˜ˆìƒ ë©”ëª¨ë¦¬: {estimated_training_memory:.1f}MB)"
                        )
                        progressive_logger.info(
                            f"100% ë°ì´í„° ë‹¨ê³„ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {current_batch_size} â†’ {safe_batch_size}"
                        )
        
        # DataLoader
        dataloader = create_dataloader(
            dataset,
            batch_size=config['training']['batch_size'],
            shuffle=True,
            num_workers=0,
            pin_memory=False,
        )
        
        # ë‹¨ê³„ë³„ í´ë” ìƒì„±
        stage_dir = output_dir / stage_name
        stage_dir.mkdir(parents=True, exist_ok=True)
        stage_checkpoint_dir = stage_dir / 'checkpoints'
        stage_logs_dir = stage_dir / 'logs'
        stage_logs_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¨ê³„ë³„ ë¡œê·¸ íŒŒì¼ ì„¤ì • (ë‹¨ê³„ë³„ í´ë”ì— ì €ì¥)
        stage_logger = setup_file_logger(stage_logs_dir, f'stage_{stage_num}_{int(current_ratio*100)}pct')
        
        # ë‹¨ê³„ë³„ ì„¤ì • ì—…ë°ì´íŠ¸ (ì¶œë ¥ ë””ë ‰í† ë¦¬ ë³€ê²½)
        stage_config = config.copy()
        stage_config['output_dir'] = str(stage_dir)
        
        # í•™ìŠµê¸° ìƒì„± (ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ)
        if previous_checkpoint:
            load_msg = f"ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ: {previous_checkpoint}"
            logger.info(load_msg)
            progressive_logger.info(load_msg)
            stage_logger.info(load_msg)
            trainer = TransferTrainer(stage_config, load_checkpoint=previous_checkpoint, file_logger=stage_logger)
        else:
            trainer = TransferTrainer(stage_config, file_logger=stage_logger)
        
        # í•™ìŠµ ì „ ë©”ëª¨ë¦¬ í™•ì¸
        mem_before_train = get_memory_usage()
        log_resource_usage(logger, f"[ë‹¨ê³„ {stage_num}] í•™ìŠµ ì „ ")
        
        # í•™ìŠµ
        best_loss = trainer.train(
            train_dataloader=dataloader,
            num_epochs=epochs_per_stage,
            stage_name=stage_name,
        )
        
        # í•™ìŠµ í›„ ë©”ëª¨ë¦¬ í™•ì¸
        mem_after_train = get_memory_usage()
        train_mem = mem_after_train['rss'] - mem_before_train['rss']
        log_resource_usage(logger, f"[ë‹¨ê³„ {stage_num}] í•™ìŠµ í›„ ")
        log_resource_usage(progressive_logger, f"[ë‹¨ê³„ {stage_num}] í•™ìŠµ í›„ ")
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
        trainer.save_metrics(stage_name)
        
        # ê²°ê³¼ ì €ì¥ (ë‹¨ê³„ë³„ í´ë”ì— ì €ì¥)
        checkpoint_path = trainer.checkpoint_dir / 'best_model.pt'
        metrics_path = stage_dir / 'training_metrics.json'
        
        # ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ ë³€ê²½ (best_modelë¡œ í†µì¼)
        if (trainer.checkpoint_dir / f'best_model_{stage_name}.pt').exists():
            import shutil
            shutil.move(
                str(trainer.checkpoint_dir / f'best_model_{stage_name}.pt'),
                str(checkpoint_path)
            )
        
        results.append({
            'stage': stage_num,
            'ratio': current_ratio,
            'data_size': len(dataset),
            'best_loss': best_loss,
            'stage_dir': str(stage_dir),
            'checkpoint': str(checkpoint_path),
            'metrics_file': str(metrics_path),
            'log_file': str(stage_logger.handlers[0].baseFilename) if stage_logger.handlers else None,
            'memory_usage': {
                'before_dataset': mem_before['rss'],
                'after_dataset': mem_after_dataset['rss'],
                'after_training': mem_after_train['rss'],
                'dataset_memory': dataset_mem,
                'training_memory': train_mem,
            },
            'batch_size': config['training']['batch_size'],
        })
        
        # ë°ì´í„°ì…‹ê³¼ DataLoader ì •ë¦¬
        del dataset
        del dataloader
        del trainer
        cleanup_memory()
        
        mem_after_cleanup = get_memory_usage()
        logger.info(f"ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {mem_after_cleanup['rss']:.1f}MB (ì ˆì•½: {mem_after_train['rss'] - mem_after_cleanup['rss']:.1f}MB)")
        
        stage_summary = (
            f"\në‹¨ê³„ {stage_num} ì™„ë£Œ:\n"
            f"  - ë°ì´í„° ë¹„ìœ¨: {current_ratio*100:.0f}%\n"
            f"  - ë°ì´í„° í¬ê¸°: {len(dataset)}ê°œ\n"
            f"  - ìµœê³  Loss: {best_loss:.4f}\n"
            f"  - ë‹¨ê³„ í´ë”: {stage_dir}\n"
            f"  - ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}\n"
            f"  - ë©”íŠ¸ë¦­ íŒŒì¼: {metrics_path}\n"
            f"  - ë¡œê·¸ íŒŒì¼: {stage_logger.handlers[0].baseFilename if stage_logger.handlers else 'N/A'}"
        )
        
        logger.info(stage_summary)
        progressive_logger.info(stage_summary)
        
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        previous_checkpoint = str(checkpoint_path)
        
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì €ì¥
        previous_checkpoint = str(checkpoint_path)
        
        # ë‹¤ìŒ ë‹¨ê³„ë¡œ
        current_ratio += step_size
        stage_num += 1
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        logger.info("\ní˜„ì¬ê¹Œì§€ ê²°ê³¼ ìš”ì•½:")
        for r in results:
            logger.info(
                f"  ë‹¨ê³„ {r['stage']}: {r['ratio']*100:.0f}% "
                f"(ë°ì´í„°: {r['data_size']}, Loss: {r['best_loss']:.4f})"
            )
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    results_file = output_dir / 'progressive_training_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    final_summary = (
        "\n" + "=" * 80 + "\n"
        "ì ì§„ì  í•™ìŠµ ì™„ë£Œ!\n"
        "=" * 80 + "\n"
        f"ê²°ê³¼ ì €ì¥: {results_file}\n"
        "\nìµœì¢… ê²°ê³¼:\n"
    )
    
    for r in results:
        final_summary += (
            f"  {r['ratio']*100:.0f}%: Loss={r['best_loss']:.4f}, "
            f"ë°ì´í„°={r['data_size']}ê°œ\n"
        )
    
    logger.info(final_summary)
    progressive_logger.info(final_summary)
    
    # ìµœì¢… ìš”ì•½ì„ íŒŒì¼ë¡œë„ ì €ì¥
    summary_file = output_dir / 'progressive_training_summary.txt'
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(final_summary)
        f.write(f"\nìƒì„¸ ë¡œê·¸:\n")
        f.write(f"  - ì „ì²´ ë¡œê·¸: {progressive_logger.handlers[0].baseFilename if progressive_logger.handlers else 'N/A'}\n")
        for r in results:
            if r.get('log_file'):
                f.write(f"  - ë‹¨ê³„ {r['stage']} ({r['ratio']*100:.0f}%): {r['log_file']}\n")
    
    logger.info(f"ìš”ì•½ íŒŒì¼ ì €ì¥: {summary_file}")
    progressive_logger.info(f"ìš”ì•½ íŒŒì¼ ì €ì¥: {summary_file}")
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LogBERT ì „ì´ í•™ìŠµ (M4 Pro ìµœì í™”)')
    parser.add_argument('--pretrained', type=str, default='bert-base-uncased',
                       help='Pre-trained ëª¨ë¸ëª… (bert-base-uncased ë˜ëŠ” distilbert-base-uncased)')
    parser.add_argument('--sample-ratio', type=float, default=0.1,
                       help='ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨ (ì ì§„ì  í•™ìŠµ ë¹„í™œì„±í™” ì‹œ ì‚¬ìš©, ê¸°ë³¸: 0.1 = 10%%, M4 Pro 48GB ê¸°ì¤€)')
    parser.add_argument('--max-files', type=int, default=50,
                       help='ìµœëŒ€ íŒŒì¼ ìˆ˜ (ê¸°ë³¸: 50, M4 Pro 48GB ê¸°ì¤€)')
    parser.add_argument('--progressive', action='store_true',
                       help='ì ì§„ì  í•™ìŠµ í™œì„±í™” (10%%ë¶€í„° ì‹œì‘)')
    parser.add_argument('--start-ratio', type=float, default=0.05,
                       help='ì ì§„ì  í•™ìŠµ ì‹œì‘ ë¹„ìœ¨ (ê¸°ë³¸: 0.05 = 5%%)')
    parser.add_argument('--step-size', type=float, default=0.05,
                       help='ì ì§„ì  í•™ìŠµ ë‹¨ê³„ í¬ê¸° (ê¸°ë³¸: 0.05 = 5%%, 5%ì”© 10ë²ˆìœ¼ë¡œ 50%ê¹Œì§€ í•™ìŠµ)')
    parser.add_argument('--max-ratio', type=float, default=0.5,
                       help='ì ì§„ì  í•™ìŠµ ìµœëŒ€ ë¹„ìœ¨ (ê¸°ë³¸: 0.5 = 50%%, ë°œì—´ ê°ì†Œë¥¼ ìœ„í•´ 50%ë¡œ ì„¤ì •)')
    parser.add_argument('--epochs-per-stage', type=int, default=5,
                       help='ì ì§„ì  í•™ìŠµ ê° ë‹¨ê³„ë‹¹ ì—í­ ìˆ˜ (ê¸°ë³¸: 5, M4 Pro 48GB ê¸°ì¤€)')
    parser.add_argument('--no-auto-batch-size', action='store_true',
                       help='ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • ë¹„í™œì„±í™”')
    parser.add_argument('--max-memory-mb', type=float, default=45000,
                       help='ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (MB, ê¸°ë³¸: 45000MB = 45GB, M4 Pro 48GB ê¸°ì¤€, 10% ë°ì´í„°ì—ì„œ 16GB ì‚¬ìš© ì‹œ ë” ê³µê²©ì  ì„¤ì • ê°€ëŠ¥)')
    parser.add_argument('--min-batch-size', type=int, default=8,
                       help='ìë™ ì¡°ì • ì‹œ ìµœì†Œ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 8, ë°œì—´ ê°ì†Œë¥¼ ìœ„í•´ ì¤„ì„)')
    parser.add_argument('--fixed-batch-size', type=int, default=8,
                       help='ê³ ì • ë°°ì¹˜ í¬ê¸° (ìë™ ì¡°ì • ë¬´ì‹œ, ê¸°ë³¸: 8, ë°œì—´ ê°ì†Œë¥¼ ìœ„í•´ ì¤„ì„)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    config['pretrained_model'] = args.pretrained
    config['data']['max_files'] = args.max_files
    
    # ë°ì´í„° íŒŒì¼
    data_files = get_data_files(
        config['data']['preprocessed_dir'],
        max_files=config['data']['max_files']
    )
    
    if len(data_files) == 0:
        logger.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì ì§„ì  í•™ìŠµ ëª¨ë“œ
    if args.progressive:
        logger.info("ì ì§„ì  í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
        results = train_progressive(
            config=config,
            data_files=data_files,
            start_ratio=args.start_ratio,
            step_size=args.step_size,
            max_ratio=args.max_ratio,
            epochs_per_stage=args.epochs_per_stage,
            auto_batch_size=not args.no_auto_batch_size,
            max_memory_mb=args.max_memory_mb,
            min_batch_size=args.min_batch_size,
            fixed_batch_size=args.fixed_batch_size,
        )
        logger.info("ì ì§„ì  í•™ìŠµ ì™„ë£Œ!")
    else:
        # ê¸°ì¡´ ë°©ì‹ (ë‹¨ì¼ ë¹„ìœ¨)
        logger.info("ì¼ë°˜ í•™ìŠµ ëª¨ë“œ")
        config['data']['sample_ratio'] = args.sample_ratio
        
        # ë°ì´í„°ì…‹ ìƒì„±
        logger.info("ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        from train_test import SampledLogBERTDataset
        
        dataset = SampledLogBERTDataset(
            data_files=data_files,
            max_seq_length=config['data']['max_seq_length'],
            mask_prob=config['training'].get('mask_prob', 0.15),
            vocab_size=config['model']['vocab_size'],
            sample_ratio=config['data'].get('sample_ratio', 0.05),
            max_files=config['data'].get('max_files', 10),
        )
        
        # DataLoader
        dataloader = create_dataloader(
            dataset,
            batch_size=config['training']['batch_size'],
            shuffle=True,
            num_workers=0,
            pin_memory=False,
        )
        
        # í•™ìŠµê¸°
        trainer = TransferTrainer(config)
        
        # í•™ìŠµ ì‹œì‘
        trainer.train(
            train_dataloader=dataloader,
            num_epochs=config['training']['num_epochs'],
        )


if __name__ == '__main__':
    main()

