# 현재 학습 설정 상세 설명

## 📊 한 에폭당 학습 데이터량

### 데이터셋 크기

로그에서 확인된 정보:
- **총 배치 수**: 19,252개
- **배치 크기**: 128 (멀티 GPU: 4개 GPU × 32)
- **데이터 파일**: 286개 (preprocessed_logs_2025-02-24.json ~ preprocessed_logs_2025-12-08.json)

### 한 에폭당 학습하는 데이터량

```
총 세션 수 = 배치 수 × 배치 크기
           = 19,252 × 128
           = 2,464,256개 세션
```

**한 에폭당 약 246만 개의 세션을 학습합니다.**

### 데이터 구성

- **세션**: 로그 이벤트 시퀀스 (최대 512개 토큰)
- **시퀀스 길이**: 최대 512 토큰
- **데이터 기간**: 2025-02-24 ~ 2025-12-08 (약 10개월)

## 🎯 학습 방식: MLM (Masked Language Modeling)

### 학습 방식 개요

**MLM (Masked Language Modeling)** - BERT의 핵심 학습 방식

### 학습 과정

#### 1단계: 데이터 준비
```
전처리된 로그 세션
  ↓
Event ID 시퀀스로 변환
  ↓
토큰 ID로 인코딩
```

#### 2단계: 마스킹 (Masking)
```
원본 시퀀스: [101, 1234, 5678, 9012, 3456, 102]
              ↓
마스킹 적용: [101, [MASK], 5678, [MASK], 3456, 102]
```

**마스킹 전략:**
- **15%의 토큰을 마스킹** (`mask_prob: 0.15`)
- 마스킹된 토큰 중:
  - **80%**: [MASK] 토큰으로 교체
  - **10%**: 랜덤 토큰으로 교체
  - **10%**: 원래 토큰 유지

#### 3단계: 예측 (Prediction)
```
입력: [101, [MASK], 5678, [MASK], 3456, 102]
  ↓
모델이 예측: [101, 1234, 5678, 9012, 3456, 102]
  ↓
정답과 비교: [101, 1234, 5678, 9012, 3456, 102]
```

#### 4단계: Loss 계산
- 마스킹된 위치의 예측만 평가
- Cross Entropy Loss 사용
- 정답 토큰을 맞추는 확률을 최대화

### 학습 목표

**정상적인 로그 패턴을 학습하여:**
1. 마스킹된 토큰을 정확히 예측
2. 로그 시퀀스의 문맥을 이해
3. 이상 패턴과 정상 패턴을 구분

## 📈 학습 설정 상세

### 모델 구조

```yaml
model:
  vocab_size: 10000          # 어휘 크기 (Event ID + Special Tokens)
  hidden_size: 768           # 은닉층 크기
  num_hidden_layers: 12      # Transformer 레이어 수
  num_attention_heads: 12    # 어텐션 헤드 수
  max_position_embeddings: 512  # 최대 시퀀스 길이
```

**모델 크기**: BERT-base 수준 (약 110M 파라미터)

### 학습 파라미터

```yaml
training:
  batch_size: 128            # 총 배치 크기 (각 GPU당 32)
  learning_rate: 0.00002     # 학습률 (2e-5)
  num_epochs: 10             # 총 에폭 수
  mask_prob: 0.15            # 마스킹 확률 (15%)
  use_multi_gpu: true        # 4개 GPU 사용
```

### 학습 알고리즘

- **옵티마이저**: AdamW
- **학습률 스케줄러**: CosineAnnealingLR
- **Gradient Clipping**: 1.0
- **Weight Decay**: 0.01 (L2 정규화)

## 🔄 학습 프로세스 상세

### 한 배치 처리 과정

```
1. 데이터 로드
   - 128개 세션 로드
   - 각 세션: 최대 512 토큰

2. 마스킹 생성
   - 각 세션의 15% 토큰 마스킹
   - 랜덤하게 위치 선택

3. Forward Pass
   - BERT 모델로 마스킹된 토큰 예측
   - 각 위치에서 vocab_size(10000)개 토큰 중 선택

4. Loss 계산
   - 마스킹된 위치만 평가
   - Cross Entropy Loss

5. Backward Pass
   - Gradient 계산
   - Gradient Clipping 적용
   - 가중치 업데이트
```

### 한 에폭 처리 과정

```
에폭 시작
  ↓
19,252개 배치 순차 처리
  ↓
각 배치마다:
  - 128개 세션 학습
  - Loss 계산 및 역전파
  - 가중치 업데이트
  ↓
에폭 완료
  ↓
체크포인트 저장
```

## 📊 데이터 사용량 계산

### 한 에폭당

| 항목 | 값 |
|------|-----|
| 총 배치 수 | 19,252개 |
| 배치 크기 | 128 |
| **총 세션 수** | **2,464,256개** |
| 평균 시퀀스 길이 | ~100-200 토큰 (가정) |
| 총 토큰 수 | 약 2억 4천만 ~ 4억 9천만 토큰 |

### 10 에폭 총합

| 항목 | 값 |
|------|-----|
| 총 에폭 | 10 |
| 총 배치 수 | 192,520개 |
| **총 세션 수** | **24,642,560개** (약 2,464만 개) |
| 총 학습 스텝 | 192,520 스텝 |

## 🎓 학습 방식의 특징

### 1. 비지도 학습 (Unsupervised Learning)

- **레이블 불필요**: 정상/이상 레이블 없이 학습
- **Self-Supervised**: 데이터 자체에서 학습 신호 생성
- **마스킹된 토큰 예측**이 학습 목표

### 2. 문맥 이해 (Context Understanding)

- **양방향 문맥**: BERT의 양방향 어텐션으로 앞뒤 문맥 모두 활용
- **시퀀스 패턴**: 로그 이벤트의 순서와 패턴 학습
- **의미 학습**: Event ID 간의 관계와 의미 학습

### 3. 이상 탐지 준비

학습 후:
- 정상 패턴: 낮은 Loss (예측 쉬움)
- 이상 패턴: 높은 Loss (예측 어려움)
- 이상 점수 = 예측 확률의 음의 로그

## 💡 학습 예시

### 예시 1: 정상 로그 패턴

```
원본: [gateway → eureka → manager → database]
마스킹: [gateway → [MASK] → manager → [MASK]]
예측: [gateway → eureka → manager → database] ✅
Loss: 낮음 (예측 쉬움)
```

### 예시 2: 이상 로그 패턴

```
원본: [gateway → [MASK] → [MASK] → database]
마스킹: [gateway → [MASK] → [MASK] → database]
예측: [gateway → unknown → unknown → database] ❌
Loss: 높음 (예측 어려움)
```

## 📝 요약

### 한 에폭당 학습 데이터

- **세션 수**: 약 246만 개
- **배치 수**: 19,252개
- **배치 크기**: 128 (각 GPU당 32)

### 학습 방식

- **방식**: MLM (Masked Language Modeling)
- **타입**: 비지도 학습 (Self-Supervised)
- **목표**: 마스킹된 토큰 예측을 통한 로그 패턴 학습
- **모델**: BERT-base (12 layers, 768 hidden)

### 학습 효과

- 정상 로그 패턴 학습
- 로그 시퀀스 문맥 이해
- 이상 탐지 준비

**현재 학습이 정상적으로 진행 중이며, Loss가 감소하고 있어 학습이 잘 되고 있습니다!** ✅




